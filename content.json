{"pages":[{"title":"","text":"window.PlotlyConfig = {MathJaxConfig: 'local'}; /** * plotly.js v2.12.1 * Copyright 2012-2022, Plotly, Inc. * All rights reserved. * Licensed under the MIT license */ !function(t){if(\"object\"==typeof exports&&\"undefined\"!=typeof module)module.exports=t();else if(\"function\"==typeof define&&define.amd)define([],t);else{(\"undefined\"!=typeof window?window:\"undefined\"!=typeof global?global:\"undefined\"!=typeof self?self:this).Plotly=t()}}((function(){return function t(e,r,n){function i(o,s){if(!r[o]){if(!e[o]){var l=\"function\"==typeof require&&require;if(!s&&l)return l(o,!0);if(a)return a(o,!0);var c=new Error(\"Cannot find module '\"+o+\"'\");throw c.code=\"MODULE_NOT_FOUND\",c}var u=r[o]={exports:{}};e[o][0].call(u.exports,(function(t){return i(e[o][1][t]||t)}),u,u.exports,t,e,r,n)}return r[o].exports}for(var a=\"function\"==typeof require&&require,o=0;oe.target.column){var r=R(e,t);return t.y1-r}if(e.target.column>t.target.column)return R(t,e)-e.y1}return t.circular&&!e.circular?\"top\"==t.circularLinkType?-1:1:e.circular&&!t.circular?\"top\"==e.circularLinkType?1:-1:t.circular&&e.circular?t.circularLinkType===e.circularLinkType&&\"top\"==t.circularLinkType?t.target.column===e.target.column?t.target.y1-e.target.y1:e.target.column-t.target.column:t.circularLinkType===e.circularLinkType&&\"bottom\"==t.circularLinkType?t.target.column===e.target.column?e.target.y1-t.target.y1:t.target.column-e.target.column:\"top\"==t.circularLinkType?-1:1:void 0}));var s=i.y0;a.forEach((function(t){t.y0=s+t.width/2,s+=t.width})),a.forEach((function(t,e){if(\"bottom\"==t.circularLinkType){for(var r=e+1,n=0;r1&&n.sort((function(t,e){if(!t.circular&&!e.circular){if(t.source.column==e.source.column)return t.y0-e.y0;if(!V(t,e))return t.y0-e.y0;if(e.source.column0);else if(0==o&&1==a)s=i.y1-i.y0,i.y0=T/2-s/2,i.y1=T/2+s/2;else if(o==n-1&&1==a)s=i.y1-i.y0,i.y0=T/2-s/2,i.y1=T/2+s/2;else{var l=e.mean(i.sourceLinks,m),c=e.mean(i.targetLinks,d),u=((l&&c?(l+c)/2:l||c)-p(i))*t;i.y0+=u,i.y1+=u}}))}))}function y(){c.forEach((function(e){var r,n,i,o=a,s=e.length;for(e.sort(f),i=0;i0&&(r.y0+=n,r.y1+=n),o=r.y1+t;if((n=o-t-T)>0)for(o=r.y0-=n,r.y1-=n,i=s-2;i>=0;--i)(n=(r=e[i]).y1+t-o)>0&&(r.y0-=n,r.y1-=n),o=r.y0}))}}function V(t){t.nodes.forEach((function(t){t.sourceLinks.sort(u),t.targetLinks.sort(c)})),t.nodes.forEach((function(t){var e=t.y0,r=e,n=t.y1,i=n;t.sourceLinks.forEach((function(t){t.circular?(t.y0=n-t.width/2,n-=t.width):(t.y0=e+t.width/2,e+=t.width)})),t.targetLinks.forEach((function(t){t.circular?(t.y1=i-t.width/2,i-=t.width):(t.y1=r+t.width/2,r+=t.width)}))}))}return z.nodeId=function(t){return arguments.length?(M=\"function\"==typeof t?t:s(t),z):M},z.nodeAlign=function(t){return arguments.length?(E=\"function\"==typeof t?t:s(t),z):E},z.nodeWidth=function(t){return arguments.length?(A=+t,z):A},z.nodePadding=function(e){return arguments.length?(t=+e,z):t},z.nodes=function(t){return arguments.length?(L=\"function\"==typeof t?t:s(t),z):L},z.links=function(t){return arguments.length?(C=\"function\"==typeof t?t:s(t),z):C},z.size=function(t){return arguments.length?(i=a=0,b=+t[0],T=+t[1],z):[b-i,T-a]},z.extent=function(t){return arguments.length?(i=+t[0][0],b=+t[1][0],a=+t[0][1],T=+t[1][1],z):[[i,a],[b,T]]},z.iterations=function(t){return arguments.length?(P=+t,z):P},z.circularLinkGap=function(t){return arguments.length?(I=+t,z):I},z.nodePaddingRatio=function(t){return arguments.length?(n=+t,z):n},z.sortNodes=function(t){return arguments.length?(O=t,z):O},z.update=function(t){return w(t,M),V(t),t.links.forEach((function(t){t.circular&&(t.circularLinkType=t.y0+t.y10)for(a=e.y0-=r,e.y1-=r,i=o-2;i>=0;--i)(r=(e=t[i]).y1+b-a)>0&&(e.y0-=r,e.y1-=r),a=e.y0}))}}function P(t){t.nodes.forEach((function(t){t.sourceLinks.sort(l),t.targetLinks.sort(s)})),t.nodes.forEach((function(t){var e=t.y0,r=e;t.sourceLinks.forEach((function(t){t.y0=e+t.width/2,e+=t.width})),t.targetLinks.forEach((function(t){t.y1=r+t.width/2,r+=t.width}))}))}return M.update=function(t){return P(t),t},M.nodeId=function(t){return arguments.length?(_=\"function\"==typeof t?t:o(t),M):_},M.nodeAlign=function(t){return arguments.length?(w=\"function\"==typeof t?t:o(t),M):w},M.nodeWidth=function(t){return arguments.length?(x=+t,M):x},M.nodePadding=function(t){return arguments.length?(b=+t,M):b},M.nodes=function(t){return arguments.length?(T=\"function\"==typeof t?t:o(t),M):T},M.links=function(t){return arguments.length?(k=\"function\"==typeof t?t:o(t),M):k},M.size=function(e){return arguments.length?(t=n=0,i=+e[0],y=+e[1],M):[i-t,y-n]},M.extent=function(e){return arguments.length?(t=+e[0][0],i=+e[1][0],n=+e[0][1],y=+e[1][1],M):[[t,n],[i,y]]},M.iterations=function(t){return arguments.length?(A=+t,M):A},M},t.sankeyCenter=function(t){return t.targetLinks.length?t.depth:t.sourceLinks.length?e.min(t.sourceLinks,i)-1:0},t.sankeyLeft=function(t){return t.depth},t.sankeyRight=function(t,e){return e-1-t.height},t.sankeyJustify=a,t.sankeyLinkHorizontal=function(){return n.linkHorizontal().source(y).target(x)},Object.defineProperty(t,\"__esModule\",{value:!0})}))},{\"d3-array\":107,\"d3-collection\":108,\"d3-shape\":119}],58:[function(t,e,r){(function(){var t={version:\"3.8.0\"},r=[].slice,n=function(t){return r.call(t)},i=self.document;function a(t){return t&&(t.ownerDocument||t.document||t).documentElement}function o(t){return t&&(t.ownerDocument&&t.ownerDocument.defaultView||t.document&&t||t.defaultView)}if(i)try{n(i.documentElement.childNodes)[0].nodeType}catch(t){n=function(t){for(var e=t.length,r=new Array(e);e--;)r[e]=t[e];return r}}if(Date.now||(Date.now=function(){return+new Date}),i)try{i.createElement(\"DIV\").style.setProperty(\"opacity\",0,\"\")}catch(t){var s=this.Element.prototype,l=s.setAttribute,c=s.setAttributeNS,u=this.CSSStyleDeclaration.prototype,f=u.setProperty;s.setAttribute=function(t,e){l.call(this,t,e+\"\")},s.setAttributeNS=function(t,e,r){c.call(this,t,e,r+\"\")},u.setProperty=function(t,e,r){f.call(this,t,e+\"\",r)}}function h(t,e){return te?1:t>=e?0:NaN}function p(t){return null===t?NaN:+t}function d(t){return!isNaN(t)}function m(t){return{left:function(e,r,n,i){for(arguments.length1;t(e[a],r)0?i=a:n=a+1}return n}}}t.ascending=h,t.descending=function(t,e){return et?1:e>=t?0:NaN},t.min=function(t,e){var r,n,i=-1,a=t.length;if(1===arguments.length){for(;++i=n){r=n;break}for(;++in&&(r=n)}else{for(;++i=n){r=n;break}for(;++in&&(r=n)}return r},t.max=function(t,e){var r,n,i=-1,a=t.length;if(1===arguments.length){for(;++i=n){r=n;break}for(;++ir&&(r=n)}else{for(;++i=n){r=n;break}for(;++ir&&(r=n)}return r},t.extent=function(t,e){var r,n,i,a=-1,o=t.length;if(1===arguments.length){for(;++a=n){r=i=n;break}for(;++an&&(r=n),i","link":"/html/Coal_POA_SOA_spectrum_comparsion+new.html"}],"posts":[{"title":"安装笔记: 在Jupyter中添加R Kernel","text":"Jupyter(即原来的Ipython Notebook Project)基于Web端的实时交互，拥有着极佳的用户体验。同时，Jupyter体系可直接使用其他语言，用户呢能够在同一界面下调用R内核或Python内核(共支持49种)，功能十分强大。在Mac OS下尝试在Jupyter中添加R语言内核，遇到了一些问题，在此记录个人安装过程和诸问题的解决方法。 STEP1 基本软件准备 Python和IPython.作为初学者，我直接安装了Python的科学计算发行版Anaconda，其包含了Python2.7并内置很多有用的Package(numpy, SciPy, Matplotlib)，Jupyter(Ipython Notebook)以及Spyder(个人感觉类似于R studio的集成开发环境) R 3.2.1(World-Famous Astronaut)，官网下载安装即可。 STEP2 有关库的安装主要工作是为R语言添加IRkernal这一新的Package, 以实现Jupyter对R语言的支持。主要的步骤如下(依照IRkernal的README文档)： R console123456789## 安装开发工具库install.packages(\"devtools\") ## 调用RCurl，可通过github下载有关文件install.packages('RCurl')library(devtools)install_github('armstrtw/rzmq')install_github(\"takluyver/IRdisplay\")install_github(\"takluyver/IRkernel\")IRkernel::installspec() 在安装rzmq时出现问题，导致后续步骤无法进行。参考网络资源，改用brew工具进行安装 Terminal1$ brew install zmq 下载成功但未能安装，利用brew doctor进行诊断 结果显示，显示/usr/local/lib/pkgconfig is not writable. 首先考虑的是MacPort可能存在干扰，将其暂时移除 1$ sudo mv /opt/local /macports pkgconfig进行重新安装关联 12345$ sudo chown -R `whoami` /usr/local/lib/pkgconfig$ brew uninstall pkgconfig $ brew install pkgconfig $ brew unlink pkg-config &amp;&amp; brew link pkg-config $ brew link zmq 回到R Console123install_github(\"takluyver/IRdisplay\")install_github(\"takluyver/IRkernel\")IRkernel::installspec() 最后一个步骤再次出现问题： 123Error in IRkernel::installspec(): Jupyter or IPython 3.0 has to be installed but could neither run “jupyter” nor “ipython”, “ipython2” or “ipython3”.(Note that “ipython2” is just IPython for Python 2, but still may be IPython 3.0) 安装包的位置出现问题，解决方法如下： 1print(system.file(\"kernelspec\", package = \"IRkernel\")) 复制该命令的输出结果，我的结果是：/Library/Frameworks/R.framework/Versions/3.2/Resources/library/IRkernel/kernelspec) 再回到Terminal(够折腾的)1ipython kernelspec install --replace --name ir --user /Library/Frameworks/R.framework/Versions/3.2/Resources/library/IRkernel/kernelspec 指令中的路径改为个人的输出结果即可。 STEP3 检验Terminal中键入ipython notebook或jupyter notebook或在Anaconda图形界面下就可以打开Jupyter的网页，如果出现下图的选项，就说明顺利完成了! 使用中出现的其他问题使用时突然出现，R kernel 无法连接的问题 jupyter-client has to be installed but “jupyter kernelspec –version” exited with code 127. 此种情况，执行下述命令，可修复 12## 换成自己的用户名路径 sudo ln -s /Users/HYF/anaconda/bin/jupyter /usr/bin/jupyter 参考 Native R kernel for Jupyter Multi-Language IPython (Jupyter) setup","link":"/2015/08/16/1.%5BCode%5D%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0-%E5%9C%A8Jupyter%E4%B8%AD%E6%B7%BB%E5%8A%A0R-Kernel/"},{"title":"FLEXPART installation notes","text":"FLEXPART is a Lagrangian particle dispersion model (LPDM) developed by Norwegian Institute of Air Research, Norway. It allows researchers to simulate the long-range transportation, diffusion, dry/wet deposition processes of atmospheric spcecies from their sources. It also can be utilized for backward calculation based on the observation of receptor to anaysis source-receptor relationships. This model is coded following the Fortran 95 standard, and can be freely download from the page here. Flexpart 8.x/9.x is easy for compilation following the offical reference. I noticed that netCDF-format output (which would make the post-processing easier compared to the original binary output files) has been merged in the newer veision. Therefore, I tried to compile FLEXPART 10.0 beta version in the Linux system, while lots of issues appeared. My installing steps are listed as follows Fig.1 Application example: CO age composition of the plume arrived at the receptor source 1. Installing required packagesCompile flag “-fuse-linker-plugin” option is enabled in the makefile of flexaprt. The error gfortran: error: -fuse-linker-plugin is not supported in this configuration indicated that this option may not work in the current system. The website Explain Linux Commands shows that: Enables the use of a linker plugin during link-time optimization. This option relies on the linker plugin support in linker that is available in gold or in GNU ld 2.21 or newer. Thus, I need to re-compile binutils to upgrade the GNU ld (my current version is 2.17 which is attched in Cent OS 5.9). Later on, I have to re-compile all the required packages in theo order: binutils $\\rightarrow$ gcc $\\rightarrow$ netcdf-c/fortran $\\rightarrow$ jasper $\\rightarrow$ grib_api. (PS: since there are many softwares have been compiled in the current environment, I had to edit a new ~/.bashrc_forFlex as an switch to select the suitable system configurations ) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#Install binutils mkdir /disk2/hyf/lib/buildcd /disk2/hyf/lib/buildwget https://ftp.gnu.org/gnu/binutils/binutils-2.27.tar.gztar -xvf binutils-2.27.tar.gzcd binutils-2.27./configure prefix=/home/hli2/libs/binutilsmakemake installcd ../vi ~/.bashrc_forFLexexport DIR=/disk2/hyf/libexport PATH=$DIR/binutils-2.25/bin:$PATHsource ~/.bashrc_forFLex#Install gcc#source code in https://ftp.gnu.org/gnu/gcc/#Referenc in https://gcc.gnu.org/wiki/InstallingGCCcd gcc-7.3.0./contrib/download_prerequisitescd ..mkdir objdircd objdir../gcc-6.1.0/configure --prefix=$DIR/gcc-6.1.0 --enable-languages=c,c++,fortran,go makemake installvi ~/.bashrc_forFlexexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$DIR/gcc-6.1.0/lib/../lib64export PATH=/disk2/hyf/lib/gcc-6.1.0/bin:$PATHexport CC=gcc#export CFLAGS='-gdwarf-2 -gstrict-dwarf -02 -fPIC'export CFLAGS='-gdwarf-2 -gstrict-dwarf'export CXX=g++export FC=gfortranexport FCFLAGS=-m64export F77=gfortranexport FFLAGS=-m64source ~/.bashrc_forFlex#Install netCDFwget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-4.6.1.tar.gztar -zxvf netcdf-4.6.1.tar.gzcd netcdf-4.6.1/wget 'http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess;hb=HEAD' -O config.guesswget 'http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub;hb=HEAD' -O config.sub./configure --prefix=/disk2/hyf/netcdf-4.6.1 CPPFLAGS=\"-I/disk2/hyf/lib/hdf5/include -I/diks2/hyf/lib/grib2/include -O3\" LDFLAGS=\"-L/disk2/hyf/lib/hdf5/lib -L/diks2/hyf/lib/grib2/lib\" --enable-shared --enable-netcdf-4 --disable-dap --disable-doxygenmake -jmake checkmake installvi~/.bashrc_forFlexexport PATH=\"$DIR/netcdf-4.6.1/bin:$PATH\"export NETCDF=$DIR/netcdf-4.6.1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NETCDF/libexport CFLAGS=\"-L$HDF5/lib -I$HDF5/include -L$NETCDF/lib -I$NETCDF/include\"export CXXFLAGS=\"-L$HDF5/lib -I$HDF5/include -L$NETCDF/lib -I$NETCDF/include\"xport FCFLAGS=\"-L$HDF5/lib -I$HDF5/include -L$NETCDF/lib -I$NETCDF/include\"export CPPFLAGS=\"-I${HDF5}/include -I${NETCDF}/include\"export LDFLAGS=\"-L${HDF5}/lib -L${NETCDF}/lib\"source ~/.bashrc_forFlex#Install netCDF-Fortranwget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-fortran-4.4.4.tar.gztar -zxvf netcdf-fortran-4.4.4.tar.gz./configure --prefix=${NETCDF} --enable-shared --disable-doxygenmake -jmake checkmake install#Install jasper./configure --prefix=$DIR/jaspermakemake installvi~/.bashrc_forFlexexport JASPER=$DIR/jasperexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JASPER/libsource ~/.bashrc_forFlex#Install grib_api#download page https://confluence.ecmwf.int/display/GRIB/Releases./configure --prefix=$DIR/grib_api --with-jasper=$DIR/jasper --disable-shared # --enable-sharedvmakemake install#Noted that if disable-shared is not added into the compile flag, the error \"/disk2/hyf/lib/jasper/lib/libjasper.a: error adding symbols: Bad value\" would appear. vi ~/.bashrc_forFlexexport PATH=$GRIB_API_BIN:$PATHexport GRIB_API=$DIR/grib_api/export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$GRIB_API/libexport GRIB_API_BIN=$GRIB_API/binexport GRIB_API_LIB=$GRIB_API/libexport GRIB_API_INCLUDE=$GRIB_API/includesource ~/.bashrc_forFlex 2. Installing FLEXPART123456789101112131415161718192021222324252627282930313233#edit the makefile suitable for the own computer#dowanload page: https://www.flexpart.eu/downloadstar -xvzf flexpart10.01beta.tar.gzcd ./flexpart10.01beta/flexpart# Both ECMWF and GFS data can be used as the inputs for FLEXPART, while the specific model for them should be clarified befoe installation. cp -r src/ test1/cd test1vi Makefile# the lines below should be revised depending on the own computer system. F90 = $GCC/gcc-6.1.0/bin/gfortranINCPATH1 = /disk2/hyf/lib/grib_api/includeINCPATH2 = /disk2/hyf/lib/jasper/includeINCPATH3 = /disk2/hyf/lib/netcdf-4.6.1/includeLIBPATH1 = /disk2/hyf/lib/grib_api/libLIBPATH2 = /disk2/hyf/lib/jasper/libLIBPATH3 = /disk2/hyf/lib/netcdf-4.6.1/libvi par_mod.f90# the default parameters in the par_mod.f90 was set for ECMWF windfield datasets. Therefore, some lines need to be revised. # Adjust the following parameters to switch between FNL/ECMWF datasets.!integer,parameter :: nxmax=361,nymax=181,nuvzmax=92,nwzmax=92,nzmax=92 !FNL XF!integer,parameter :: nxmax=361,nymax=181,nuvzmax=152,nwzmax=152,nzmax=152 !ECMWF new !integer,parameter :: nxmax=361,nymax=181,nuvzmax=92,nwzmax=92,nzmax=92 !ECMWF!integer,parameter :: nxmax=361,nymax=181,nuvzmax=26,nwzmax=26,nzmax=26integer,parameter :: nxmax=721,nymax=361,nuvzmax=64,nwzmax=64,nzmax=64!integer,parameter :: nxmax=1201,nymax=235,nuvzmax=58,nwzmax=58,nzmax=58!integer,parameter :: nxshift=359 ! for ECMWFinteger,parameter :: nxshift=0 ! for GFS or FNLmake -j 8 gfs 3. Installing FLEXPART-WRFIt has been known that the results of Lagrangian particle dispersion models (LPDM) would affected by the errors in spatio-temporal interpolation of the simulated meteorological field. Furthermore, there are some uncertainties in the aspect of simulating mixing status for particles due to the erros in the vertical velocity. Therefore, the output from mesoscale models which were finer in vertical/horizontial resolution and time interval have been introduced for partially address these issue. In 2006, Flexpart-WRF model has been developed combining the Flexpart utilities and WRF output. Compared to the standard model using global GFS/ECMWF datasets, Flexpart-WRF can better depict the atmospheric transportation especially for small-scale applications. 123456789tar -xvzf ./flexpart_wrf_3.3.2.tar.gzcd flexpart_wrfcp -r src testcd testvi makefile.mom # set the ${NETCDF} environment variablemake -f makefile.mom omp#Done! 4. Some tips123456789# Error1 NetCDF: Invalid dimension ID or name when using forward mode of FLEXPART-WRF# REFERENCE: https://www.flexpart.eu/ticket/233# There is a typo error in the original netcdf_out_mod.F90!Change Line 624- ncret = NF90_DEF_VAR(ncid,'DRYDEP',NF90_FLOAT,ncdimsid2,ncddvid,&amp;+ ncret = NF90_DEF_VAR(ncid,'DRYDEP',NF90_FLOAT,ncdimsid22,ncddvid,&amp; 参考资料1. FlXEXPART Offical website 2. Problem compiling GRIB AP 3. Compile with flag -fPIC 4. FLEXPART various compilation problems","link":"/2018/08/10/10.%5BModel%5DFLEXPART_INSTALLATION_NOTE/"},{"title":"Satellite-Based Air Quality Mapping in Target Regions 卫星数据绘制特定区域的空气污染地图","text":"I’m writing today about downloading, handling, and plotting satellite derived air pollution maps with cartopy and fiona using Python. One key task in this post is to clip a raster-like (2-d array) dataset with a polygon in pure Python environment (i.e., no need for ArcGIS or QGIS GUI-based software). The satellite sensor can offer critical supplementary data of several atmospheric species, e.g., SO2, NO2, PM2.5. Comparaing to ground-based monitoring which might be sparse in some areas (e.g., Africa, South America, oceans), the satellite observation offers a full picture for better understanding the spatiotemporal patterns of some air pollutants. Below is an excerpt of a NO2 column maps within Chengyu urabn agglomeration in China. The tropospheric NO2 concentration derived from satellite remote sensing techniques have been used on global, regional and urban scales since the mid-nineties. After the launch of the Ozone Monitoring Instrument (OMI) in 2004, the spatial resolution (up to 13 x 24 km2) ave been largely improved. The vetical column NO2 dataset have contributed to reveal the hotspots of air pollution around the world, or show the episodes with long-range transport. The TEMIS website provide the image and data for NO2 concentration maps in individual days and monthly averages derived from different satellite sensors (e.g., OMI, GOME-2). Herein, I present the general processes to obtain DOMINO version 2.0 data Obtaining the datasetsWe start by downloading the essential data files using function of wget. 1234567#Creat an loop for downloading the monthly averages from 2013-2017import osfor i in ['2013','2014','2015','2016','2017']: for t in ['01','02','03','04','05','06','07','08','09','10','11','12']: filename = 'http://www.temis.nl/airpollution/no2col/data/omi/data_v2/'+i+'/'+t+'/'+'no2_'+i+t+ '.grd.gz' os.system('wget %s' %filename)print \"DOWNLOADING COMPLETE\" Merging the datasetsWe then define a reading function and open those files to obtain essential information. In the case of 2013, there are 12 monthly averages need to read and merge for further analysis. 12345678910111213141516171819202122232425262728import numpy as npdef read_grd(filename): with open(filename) as infile: ncols = int(infile.readline().split()[1]) nrows = int(infile.readline().split()[1]) xllcorner = float(infile.readline().split()[1]) yllcorner = float(infile.readline().split()[1]) cellsize = float(infile.readline().split()[1]) nodata_value = int(infile.readline().split()[1]) version = float(infile.readline().split()[1]) longitude = xllcorner + cellsize * np.arange(ncols) latitude = yllcorner + cellsize * np.arange(nrows) value = np.loadtxt(filename, skiprows=7) return longitude, latitude, value,nodata_valueset_year = 2013for i in range(0,12,1): # setting the file path which contains those original data. file_path = './NO2_POMINO/no2_'+str(set_year)+ '%02d' %(i+1)+'.grd' lon,lat,value,nodata_value = read_grd(file_path) value = value[::-1] value[value == nodata_value] = np.nan if i ==0: value_ = value.reshape(1,value.shape[0],value.shape[1]) else: value_ = np.vstack([value_,value[None, ...]])## np.nanmean() is utilized to compute the arithmetic mean igonring NaNsno2_2013 = np.nanmean(value_,axis = 0) ##Cut 2-d array by shapefile Next, we import the necessary library and the specific shapefile to clip the global 2-d map. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#1. transform the shapefile to fiona polygon objectimport fionafrom shapely.geometry import shape,Polygon, Pointcf_area = fiona.open(\"xxx.shp\")pol = cf_area.next()from shapely.geometry import shapegeom = shape(pol['geometry'])poly_data = pol[\"geometry\"][\"coordinates\"][0]poly = Polygon(poly_data)#2. substract the dataset that can cover the polygonx1 = np.array([t for t,j in (poly_data)]).min()x2 = np.array([t for t,j in (poly_data)]).max()y1 = np.array([j for t,j in (poly_data)]).min()y2 = np.array([j for t,j in (poly_data)]).max()extent = x1 -0.5, x2+0.5,y1-0.5, y2+0.5def find_nearest(array,value): idx = (np.abs(array-value)).argmin() return array[idx]nx_st = np.where(lon == (find_nearest(lon,extent[0])))[0]nx_en = np.where(lon == (find_nearest(lon,extent[1] )))[0]ny_st = np.where(lat == (find_nearest(lat,extent[2])))[0]ny_en = np.where(lat == (find_nearest(lat,extent[3] )))[0]lon_,lat_ = lon[nx_st:nx_en+1], lat[ny_st:ny_en+1]so2_data= so2_data[ny_st:ny_en+1, nx_st:nx_en+1] #3. test the dat points whether or not within the polygonimport timestart_time = time.time()sh = (len(lat_)*len(lon_),2)points = np.zeros(len(lat_)*len(lon_)*2).reshape(*sh)k = 0for i in range(0,len(lat_),1): for j in range(0,len(lon_),1): points[k] = np.array([lon_[j],lat_[i]],dtype=float) k+=1mask = np.array([poly.contains(Point(x, y)) for x, y in points]) mask = mask.reshape(len(lat_),len(lon_))print(\"--- %s seconds ---\" % (time.time() - start_time))print mask.shapenp.savetxt(\"mask_SO2.txt\",mask)#4. mask the data points outsied the polygondef shape_mask(array, lon_,lat_,mask): arr = np.zeros_like(array) for i in range(0,len(lat_),1): for j in range(0,len( lon_),1): if mask[i,j] == 1: arr[i,j] = array[i,j] else: arr[i,j] = -1 return arrno2_mask = shape_mask(no2_data, lon_,lat_,mask) The mask array as the rastered polygon is shown like this. 1234567fig = plt.figure()ax = back_main_clean(fig,extent)for spine in plt.gca().spines.values(): spine.set_visible(False)ax.outline_patch.set_visible(False)ax.pcolormesh(mask)plt.tight_layout() Visualization1234567891011121314151617181920from mpl_toolkits.axes_grid1 import make_axes_locatablefig = plt.figure()ax = back_main_clean(fig,extent) # the background map was prior-defined by specific uses.lon_x,lat_y = np.meshgrid(lon_,lat_)value_mask = np.ma.masked_array(no2_mask, np.isnan(no2_mask))value_mask = np.ma.masked_less(value_mask, 0.0)my_cmap = plt.cm.get_cmap('Spectral_r')my_cmap.set_under('w')cc = ax.pcolormesh(lon_x,lat_y,value_mask/100.0,cmap =my_cmap,alpha = 0.7,transform=ccrs.PlateCarree(),zorder =2,vmax = 25)#cbaxes = fig.add_axes([0.58, 0.12, 0.16, 0.015]) cbar = fig.colorbar(cc, cax=cbaxes, ticks=[4,12,20],orientation='horizontal')cbar.ax.set_title(r'$\\mathregular{NO_2\\ [10^{15}molec/cm^2]}$', fontsize = 8)cbar.ax.tick_params(color='k', direction='in',labelsize=8)for spine in plt.gca().spines.values(): spine.set_visible(False)ax.outline_patch.set_visible(False)plt.tight_layout()","link":"/2018/11/25/14.%5BCode%5DPlotting-satellite-derived-air-pollution-maps-using-Python/"},{"title":"QGIS: 依照现有图片资料绘制矢量文件","text":"有时，我们无法直接获取某些地理数据的精确位置信息，仅能从网络中下载他人的地图作品。因而，我请教了一名规划师同学，参考网络图像，通过适配地理范围，手工获取其实际地理属性。本文以我国集中采暖南北分界线的绘制为例，简述QGIS软件的具体实现步骤。 上世纪50年代，由于能源缺乏，我国以南北地理界限（秦岭-淮河线）为划分依据，在苏联的帮助下，在该线以北建立了集中供暖系统，沿用至今。下图可见，我国的集中供暖系统并不以省为界限，如四川省的甘孜和阿坝位于供暖线以北，江苏省的徐州位于供暖线以北，而河南省的信阳则没有集中供暖。 这条分界线具有多重地理学意义，它还是 我国800毫米等降水量线 1月份0℃等温线 温带季风与亚热带季风气候分界线 温带落叶阔叶林与亚热带常绿阔叶林带分界线 湿润与半湿润区分界线(摘录自 国家气象局 气象科普园)。 同时，该分界线同时具有特殊的历史意义，其东段是我国南北朝、金朝/蒙元-南宋时期的南北政权边界。 1. 安装Freehand raster georeferencerQGIS的Freehand第三方工具包可载入图片图层并简单处理，主要用途应是卫星遥感图像的像元分析。该工具包的功能界面非常简洁，工具栏 从左至右，功能依次为“载入”，“移动”，“旋转”，“放缩”，“复合操作”，“透明度减小”，“透明度增加”，“输出” 。 2. 绘制Polyline我们分别导入中国分省地图shapefile文件和jpeg格式底图。利用Freehand不断调整合图像位置、大小，使二者基本重合（tip: 按住“⌘”或”ctrl”可按原始比例缩放），如下图。可以看出南方各省份吻合较好，但台湾与海南有较大的偏差，可能是原始图片投影系统上的差异。此处作为案例，未深究其原因。 选择”Layer”-&gt;”Creat Layer”-&gt;”New Shapefile Layer”新建point格式的矢量文件，利用 依照图片勾勒界限，如下图： 3. 后续应用利用Python语言读取编写好的矢量文件，通过Cartopy, Fiona, Shapely等工具包可做进一步分析。 例如，以该线为依据将我国空气质量监测站点进行分类，探究分采暖/非采暖城市间大气污染特征的差异。该工作可由简单的代码指令完成，如下： 1234567891011121314151617181920import warningswarnings.filterwarnings('ignore')from shapely.geometry import shapefrom shapely.geometry import LineString# loading the boundary layerimport fionafname = './N-S_boundary.shp'line1 = fiona.open(fname)line1 = shape(line1.next()['geometry'])# set a end point which is the southernmost for all stations.end_point = (dy[dy['lat']==dy['lat'].min()]['lon'].values[0],dy[dy['lat']==dy['lat'].min()]['lat'].values[0])# loop all monitoring stations for classificationdy['NS']= np.nanfor i in range(0,len(dy),1): start_point = (dy['lon'].iloc[i],dy['lat'].iloc[i]) line2 = LineString([start_point, end_point]) if line1.intersection(line2).is_empty: dy[\"NS\"].iloc[i]='S' else: dy[\"NS\"].iloc[i]='N' 按供暖分界线区分后，大气采样站点的分布如图所示","link":"/2019/02/28/16.%5BSoftware%5DQGIS%E6%93%8D%E4%BD%9C-%E4%BE%9D%E6%8D%AE%E5%9B%BE%E5%83%8F%E7%BB%98%E5%88%B6%E7%9F%A2%E9%87%8F%E6%96%87%E4%BB%B6/"},{"title":"Two method to mask the raster data by specific geometry","text":"Sometimes, we need to clip or extract the raster image with polygon features, e.g., only focus on the percipitation within China using global dataset. This post will introduce two methods to mask the 2-d array-like dataset by a specific geometry using Python. The first method use GeoPandas module to test those raster coordidnates within/out of the shapefile The second method which I strongly recommended with faster speed was based on shapely module. 1. Geopandas12345678910111213141516171819202122232425262728293031# referencing https://stackoverflow.com/questions/47781496/python-using-polygons-to-create-a-mask-on-a-given-2d-gridimport numpy as npimport irisimport geopandas as gpd# reading the mask shapefilefname = r'xxx/china.shp'cn =gpd.read_file(fname)# four cornersx0,x1 = cn.bounds['minx'].values,cn.bounds['maxx'].valuesy0,y1 = cn.bounds['miny'].values,cn.bounds['maxy'].values# the map background functiondef back_main_clean(fig,extent): ax = fig.add_subplot(111, projection=ccrs.PlateCarree()) ax.set_extent(extents=extent, crs=ccrs.Geodetic()) for i in ax.spines.itervalues(): i.set_linewidth(0.01) fname = r'xxx/china_provinces.shp' shape_feature = ShapelyFeature(Reader(fname).geometries(), ccrs.PlateCarree(),facecolor='#FFFFFF', linestyle='--',edgecolor ='k', linewidth = 0.25, alpha =1) ax.add_feature(shape_feature, zorder =3) # initializing the raster datax = np.linspace(x0,x1,30)y = np.linspace(y0,y1,20)fig = plt.figure(figsize=(8, 5))ax = back_main_clean(fig, extent)xx,yy = np.meshgrid(x,y)zz = np.random.random((len(x),len(y)))plt.scatter(xx.reshape(-1),yy.reshape(-1),zorder =4,transform=ccrs.PlateCarree(), c=zz.reshape(-1),s=8) 12345678910111213# creating the mask array and counting the processing timeimport timestart = time.time()lon2 = xx.reshape(-1)lat2 = yy.reshape(-1)mask = []for lat, lon in zip(lat2, lon2): this_point = gpd.geoseries.Point(lon,lat) res = cn.contains(this_point) mask.append(res.values[0])mask = np.array(mask)mask= mask.reshape(len(y),len(x))print \"Process time: \" + str(time.time() - start) Process time: 5.11 Seconds 1234567# plotting the raster data extracted by Chinazz_mask = np.ma.masked_array(zz,mask) # noted that ~mask can simply transform to clip function.fig = plt.figure(figsize=(8, 5))ax = back_main_clean(fig, extent)xx,yy = np.meshgrid(x,y)zz = np.random.random((len(x),len(y)))plt.scatter(xx.reshape(-1),yy.reshape(-1),zorder =4,transform=ccrs.PlateCarree(), c=zz_mask.reshape(-1),s=8) 2. shapely.vectorized12345678910111213141516171819202122# referencing https://gist.github.com/pelson/9785576import fionaimport shapely.vectorizedfrom shapely.geometry import shapefname = r'xxx/china.shp'cn_area = fiona.open(fname)pol = cn_area.next()geom = shape(pol['geometry'])## four corner x0,x1 = geom.bounds[0],geom.bounds[2]y0,y1 = geom.bounds[1],geom.bounds[3]## creating the mask arrayx = np.linspace(x0,x1,30)y = np.linspace(y0,y1,20)xx,yy = np.meshgrid(x,y)import timestart = time.time()mask_ = shapely.vectorized.contains(geom, xx,yy)print \"Process time: \" + str(time.time() - start) Process time: 0.01 Seconds (huge advantage here) 12345678zz = np.random.random((len(x),len(y)))zz_mask = np.ma.masked_array(zz,~mask_)fig = plt.figure(figsize=(8, 5))ax = back_main_clean(fig, extent)xx,yy = np.meshgrid(x,y)# zz = np.random.random((len(x),len(y)))# plt.scatter(xx.reshape(-1),yy.reshape(-1),zorder =4,transform=ccrs.PlateCarree(),)plt.scatter(xx.reshape(-1),yy.reshape(-1),zorder =4,transform=ccrs.PlateCarree(), c=zz_mask.reshape(-1),s=5) Some additional tips12345# 1. the error during shapefile readingimport fionacn_area = fiona.open('xxx.shp')&gt; Recode from CP936 to UTF-8 not supported, treated as ISO8859-1 to UTF-8.cn_area = fiona.open('xxx.shp', encoding ='utf-8') 12345# 2. select the polygon by attributes within geopandas modulefname = r'xxx/china_provinces.shp'cn =gpd.read_file(fname, encoding ='utf-8')test_p = cn[cn['NAME']==u'黑龙江']test_p.plot() 123456# 3. select the polygon by attributes within shapely modulewith fiona.open(fname, encoding='utf-8') as src: filtered = filter(lambda f: f['properties']['NAME']==u'黑龙江', src)pol = filtered[0]#.next()geom = shape(pol['geometry'])geom 12345678# 4. select and output one polygon from multiple polygons by attributesimport fionawith fiona.open(fname, encoding='utf-8') as input: meta = input.meta with fiona.open('test_heilongjiang.shp','w',**meta) as output: for feature in input: if feature['properties']['NAME']==u'黑龙江': output.write(feature)","link":"/2018/12/06/15.%5BCode%5DMethods-to-mask-raster-by-polygons/"},{"title":"Resampling the orignial data to 2-d array in specific grid system","text":"In this post, I address an common problem in geoscience research: how to arrange the original geodata into pre-defined grid system. Sometimes, we need to unify the resolution of various dataset or summary the scatter data to raster one. Specifically, this brief tutorial will look at two different original data, and allow you to creat gridded data in python. For better illustration, two practial cases with detailed code are shown: Creating an emission inventory based on the emissions from point sources (e.g., power plants, cement plants) Remapping a population density map to a coarser resolution 1. Scatter data point123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inlineimport cartopy.crs as ccrsfrom matplotlib.axes import Axesfrom cartopy.mpl.geoaxes import GeoAxesGeoAxes._pcolormesh_patched = Axes.pcolormeshfrom cartopy.mpl.gridliner import LATITUDE_FORMATTER, LONGITUDE_FORMATTERfrom scipy.spatial import KDTree# 1. generating the scatter dataxc1, xc2, yc1, yc2 = 100, 110, 35, 45N = 1000x = np.random.uniform(low=xc1, high=xc2, size=(N,))y = np.random.uniform(low=yc1, high=yc2, size=(N,))value = np.ravel(np.random.rand(1000,1))df_points = pd.DataFrame({\"x\":x, \"y\":y, \"v\":value})## 2. Clustering the scatter point Using KdTree methodXSIZE,YSIZE=50,50 # set the grid resolutionlon,lat = np.linspace(xc1,xc2,XSIZE),np.linspace(yc1,yc2,YSIZE)X, Y = np.mgrid[lon.min():lon.max():XSIZE*1j, lat.min():lat.max():YSIZE*1j]grid = np.c_[X.ravel(), Y.ravel()]points = np.c_[df_points.x, df_points.y]from scipy.spatial import KDTreetree = KDTree(grid)dist, indices = tree.query(points)grid_values = df_points.groupby(indices).v.sum()df_grid = pd.DataFrame(grid, columns=[\"x\", \"y\"])df_grid[\"v\"] = grid_values## 3. Visualizationdef plot_xy_tick(ax): xticks = [100,105,110] yticks = [35,40,45] ax.set_xticks(xticks, crs=ccrs.PlateCarree()) ax.set_yticks(yticks, crs=ccrs.PlateCarree()) lon_formatter = LongitudeFormatter(number_format='.1f', degree_symbol='', dateline_direction_label=True) lat_formatter = LatitudeFormatter(number_format='.1f', degree_symbol='') ax.xaxis.set_major_formatter(lon_formatter) ax.yaxis.set_major_formatter(lat_formatter)fig = plt.figure(figsize=(8,3))ax1 = plt.subplot(121, projection=ccrs.PlateCarree())ax1.scatter(df_points.x, df_points.y, alpha=0.2,c=df_points.v,s=15,transform=ccrs.PlateCarree())ax1.set_extent([xc1, xc2, yc1, yc2], crs=ccrs.PlateCarree())plot_xy_tick(ax1)ax2 = plt.subplot(122, projection=ccrs.PlateCarree())val_ = df_grid.v.values.reshape(len(lat),len(lon))ax2.pcolormesh(X,Y, val_,transform=ccrs.PlateCarree())ax1.set_extent([xc1, xc2, yc1, yc2], crs=ccrs.PlateCarree())plot_xy_tick(ax2) plt.show() &nbsp;2. Raster dataNoted in my last post, the resampling of tif-format dataset to raster file in other resolution relied on the skimage.transform function. Here, I present the method which directly deal with np.array. 2.1 equal coarse ratio on both axis12345## creat a original data representing populaiton mapxc1, xc2, yc1, yc2 = 100, 110, 35, 45XSIZE,YSIZE=100,100lon,lat = np.linspace(xc1,xc2,XSIZE),np.linspace(yc1,yc2,YSIZE)pop = np.random.uniform(low=1000, high=50000, size=(XSIZE*YSIZE,)).reshape(YSIZE,XSIZE) 123456789101112## Method 1 reshape the original data arrayshape = np.array(pop.shape, dtype=float)coarseness = 2 # the new shape is in 50 x 50new_shape = coarseness * np.ceil(shape/coarseness).astype(int)# # Create the zero-padded array and assign it with the old densityzp_pop = np.zeros(new_shape)zp_pop[:int(shape[0]), :int(shape[1])] = poptemp = zp_pop.reshape((new_shape[0] // coarseness, coarseness, new_shape[1] // coarseness, coarseness))coarse_pop = np.sum(temp, axis=(1,3))print (pop.sum())print (coarse_pop.sum()) 2.2 inequal coarse ratio on both axisMethod.1 interpolation 1234567891011from scipy import interpolatehfunc = interpolate.interp2d(lon,lat,pop)XSIZE_n,YSIZE_n=80,60coarse_pop = np.zeros(XSIZE_n * YSIZE_n)xx,yy=np.meshgrid(lon_n,lat_n)grid = np.c_[xx.ravel(),yy.ravel()]df_grid = pd.DataFrame(grid, columns=[\"x\", \"y\"])df_grid['v'] = [hfunc(x,y)[0] for x, y in zip(df_grid['x'].values,df_grid['y'].values)]print (pop.sum())print (df_grid['v'].sum()) Method.2 Fourier transform method Thanks to help from my new roomate who is expert in math, I learned to transfrom the 2-d array using Fourier transform method 12345from scipy import fftpackpop_fft = fftpack.fft2(pop*100*100/(60*80),shape = (60,80)) # 100 x 100 -&gt; 60 x 80pop_res = fftpack.ifft2(pop_fft).realprint(pop.sum())print(pop_res.sum())","link":"/2019/06/01/19.%5BCode%5DResampling-the-orignial-data-to-2-d-array/"},{"title":"Altering the resolution of a raster data using Python","text":"Sometimes, we need to transform the original raster dataset to a different resolution (usually coarser). In this post, I will introduce the simple workflow for cell resampling using Python. 1. Data import / preprocessingTo get started, first load the required library and then import the original raster data. In this case, we downloaded the Gross Domestic Product (GDP) distribution of China in 2010 with the resolution of 1 km x 1 km as the input. What’s more, a subset of China outlined by a shapefile was also used for clipping that raster. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import warningswarnings.filterwarnings('ignore')import cartopyimport matplotlib.pyplot as pltfrom osgeo import gdalimport rasteriofrom rasterio.mask import maskimport geopandas as gpdfrom shapely.geometry import mappingimport skimage.transform as st# original data importfrom osgeo import gdalpathToRaster = r\"./cngdp2010.tif\"input_raster = gdal.Open(pathToRaster)prj=input_raster.GetProjection()srs=osr.SpatialReference(wkt=prj)if srs.IsProjected: print srs.GetAttrValue('projcs')print srs.GetAttrValue('geogcs')# Albers_Conic_Equal_Area# GCS_Unknown_datum_based_upon_the_Krassowsky_1940_ellipsoid# transforming the projection of the input_rasteroutput_raster = r'./cngdp2010_adjust.tif'gdal.Warp(output_raster,input_raster,dstSRS='EPSG:4326',dstNodata = -9999)# clipping the raster by polygoninput_shape = r'./boundary.shp'shapefile = gpd.read_file(input_shape)shapefile = shapefile.to_crs(epsg=4326)input_raster = r'./cngdp2010_adjust.tif' # input rastergeoms = shapefile.geometry.values # list of shapely geometriesoutput_raster = r'./2+26city_gdp.tif' # output rastergeometry = geoms[0] # shapely geometrygeoms = [mapping(geoms[0])]with rasterio.open(input_raster) as src: out_image, out_transform = mask(src, geoms, crop=True) ## 出现了不配套的情况，怎么办 out_meta = src.meta.copy() out_meta.update({\"driver\": \"GTiff\", \"height\": out_image.shape[1], \"width\": out_image.shape[2], \"transform\": out_transform})with rasterio.open(output_raster, \"w\", **out_meta) as dest: dest.write(out_image) 2. Resampling the dataThere are many different approaches to adjust the resolution of the raster file. Here, I use one simple method suppoerted by skikit-image package. With the resize function, you are simply create a new 2-d array with a predefined mesh grid. 12345678pathToRaster = r'./2+26city_gdp.tif'with rasterio.open(pathToRaster) as src: arr = src.read() arr[arr ==-9999] = np.nan arr = arr[0,:,:] # In this case, the orignial resolution was adjusted to 3 km x 3 km. new_shape = (round(arr.shape[0]/3.0),round(arr.shape[1]/3.0)) newarr= st.resize(array, new_shape, mode='constant') 3. Exporting DataWhen the resampling works are done, the generated numpy array can be exported to geotiff format file for further uses. Luckily, some sufficient information for geotiff file can be easily assesed from the original one. 12345import cartopy.crs as ccrsimport rasteriopathToRaster = r'./2+26city_gdp.tif'raster = gdal.Open(pathToRaster, gdal.GA_ReadOnly)gt = raster.GetGeoTransform() Since the mesh grid has been altered, the GeoTransform information should be editted. The values in gt includes: gt[0] = top left x gt1 = w-e pixel resolution gt2 = 0 gt[3] = top left y gt[4] = 0 gt[5] = n-s pixel resolution (negative value) In this case, gt1 and gt[5] should be revised (i.e., multiplied by 3) 12345678910output = r'./2+26city_3km.tif'cols, rows =newarr.shape[1],newarr.shape[0]driver = gdal.GetDriverByName(\"GTiff\")outDs = driver.Create(output, cols, rows, 1, gdal.GDT_Float32)r, float, etc).outDs.SetGeoTransform((gt[0],gt[1]*3.0,gt[2],gt[3],gt[4],gt[5]*3.0))outDs.SetProjection(raster.GetProjection())outDs.GetRasterBand(1).WriteArray(newarr[::-1])outDs.GetRasterBand(1).SetNoDataValue(-9999)del outDs Done! PS:参考资料 Export Numpy Arrays to Geotiff Format Using Rasterio and Python","link":"/2019/04/07/18.%5BCode%5DAltering-the-resolution-raster-file-using-Pyton/"},{"title":"WRF post processing 1: Merging datafiles for specific spots","text":"When you’ve got the all the files from WRF simulations, you might want to merge them without the spin-up frames (to reach a balanced state with the boundary conditions, i.e., 12 hours for a 5-day simulation). Meanwhile, the variables/grids which are not focused on can be ignore. Therefore, a general workflow in pythonic way is presented. I will also rewritten this function as my first Python Package. Please note the updates on my website. 1. Pretreatment In this post, the technique applied here is to extracting five meteorological variables for several grids. Those variables contain wind speed (ws), wind direction (wd), relative humidity (RH), planetal boundary layer height (PBLH), Temperature at 2 m above ground (T2) and precipitation (PREP). Noticed that ws, wd and rh are not directly derived from standard output, some essential steps should be implemented. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465\"\"\"calulating wind speed by U and V components\"\"\"def calc_ws(fc,y,x): u = fc.variables[\"U10\"][:,y,x] v = fc.variables[\"V10\"][:,y,x] return (u**2+v**2)**0.5\"\"\"calulating wind directionn by U and V components\"\"\" def calc_wd(fc,y,x): # fc: input data, y/x: the index in y/x axis of targeted grid \"\"\" Reference: http://colaweb.gmu.edu/dev/clim301/lectures/wind/wind-uv \"\"\" u = fc.variables[\"U10\"][:,y,x] v = fc.variables[\"V10\"][:,y,x] wd = [] for i in range(0,len(u),1): if u[i]==0: if v[i]==0: wd.append(np.nan) if v[i]&gt;0: wd.append(180.0) if v[i]&lt;0: wd.append(0) if u[i]&gt;0: if v[i]&gt;=0: wd.append(270-np.arctan(v[i]/u[i])/2/np.pi*360.0) if v[i]&lt;0: wd.append(270+np.arctan(v[i]/u[i])/2/np.pi*360.0) if u[i]&lt;0: if v[i]&gt;=0: wd.append(90+np.arctan(v[i]/u[i])/2/np.pi*360.0) if v[i]&lt;0: wd.append(90-np.arctan(v[i]/u[i])/2/np.pi*360.0) return wd# calculate the relative humiditydef calc_rh(fc,y,x): \"\"\" referencing: t: perturbation potential temperature. \"T\" p: perturbation pressure. \"P\" pb: base state pressure. \"PB\" qv: Water vapor mixing ratio \"QVAPOR\" noted that all these parameters from WRF have the vertical dimension. \"\"\" t = fc.variables['T'][:,0,y,x] p0 = fc.variables['P'][:,0,y,x] pb = fc.variables['PB'][:,0,y,x] qv = fc.variables['QVAPOR'][:,0,y,x] theta = t+300 p = (p0+pb)/100.0 temp = theta*(p/1000.0)**0.2854 e_0 = 6.1173 ## mb t_0 = 273.16 ## K Rv = 461.50 ## J K-1 Kg-1 Lv_0 = 2.501 * 10**6 ## J Kg-1 K1 = Lv_0 / Rv ## K K2 = 1 / t_0 ## K-1 K3 = 1 / temp ## K-1 e_s = e_0 * np.exp(K1*(K2 - K3)) w_s = (0.622 * e_s)/(p - e_s) return (qv/w_s)*100.0 Those functions were wrote in an independent file (function_lib.py) for further importing. 2. Main functionNow that we have the pre-defined tools, we can proceed to read and merge all the outputs into one hdf5 file. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from netCDF4 import Datasetimport pandas as pdfrom pandas import HDFStore, DataFramefrom pandas import read_hdfimport os,sys,stringimport numpy as np# the Pretreatment functionsimport function_lib as func# targeted girds with specific namessite_= ['YZ','TY','HD','SS','XS','HS']loc_dic = {site_[0]:[37,65],site_[1]:[38,64], site_[2]:[37,66],site_[3]:[28,75], site_[4]:[34,63],site_[5]:[38,68]}# creat an empty h5 filehdf = HDFStore(\"simulated_meteo_chifeng.h5\")feature = ['WS','WD','PBLH','RH','T2','PREP']# loop all the original filesDir = \"/disk2/hyf/wrf/WRFV3/Chifeng_output/post-process/d03/\"files = os.listdir(Dir)files.sort()files = files[0:]# write the varibales into h5 filefor k in range(0,len(feature),1): data = {'date':[],site_[0]:[],site_[1]:[],site_[2]:[], site_[3]:[],site_[4]:[],site_[5]:[],} for file in files: filename,extname = os.path.splitext(file) if filename[7:10] =='d03': ## restrict to specific nested domain fc = Dataset(Dir+file) data['date'].extend([''.join(t) for t in fc.variables['Times']]) for i in range(0,len(site_),1): x,y = loc_dic[site_[i]][1],loc_dic[site_[i]][0] if feature[k]=='WD': data[site_[i]].extend(func.calc_wd(fc,y,x)) if feature[k]=='WS': data[site_[i]].extend(func.calc_ws(fc,y,x)) if feature[k]=='PBLH': data[site_[i]].extend(fc.variables['PBLH'][:,y,x]) if feature[k]=='RH': data[site_[i]].extend(func.calc_rh(fc,y,x)) if feature[k]=='T2': data[site_[i]].extend(fc.variables['T2'][:,y,x]) if feature[k]=='PREP': #RAINC is the rain calculation by cumulus scheme and RAINNC is the rain calculation by micrphysics scheme. rain = (fc.variables['RAINC'][:,y,x]+fc.variables['RAINNC'][:,y,x])*1000.0/997.0 data[site_[i]].extend(rain) data = pd.DataFrame(data) # a efficient way to deal with those spin-up frames. data = data.drop_duplicates(subset='date', keep='first', inplace=False) print len(data) data = data.reset_index() hdf.put(feature[k], data, format='table', encoding=\"utf-8\") And after those scripts completed successfully, we have merged all the targetd variables for the spots of interest. In our furture post, the technique dealing with a 2-d griided system (i.e., a city) will be presented.","link":"/2019/03/11/17.%5BModel%5DWRF-technique-merging-datasets/"},{"title":"Plotting WRF Nested Domains WRF嵌套网格的绘制","text":"提要：定义嵌套网格是运行WRF模型的重要工作。本文基于Python 3.7，在Salem工具包基础上加以改进，结合Cartopy对中国国界线进行替换，同时实现修改默认底图。 1. 替换底图Salem是用于地理数据处理和可视化的Python小工具，其功能强大，使用简便。例如，Salem可直接读取WRF模型的namelist.input，绘制网格嵌套图。 1234567import salemfrom salem.utils import get_demo_filefrom salem import geogrid_simulatorfpath = r'./namelist.wps'g, maps = geogrid_simulator(fpath)maps[0].set_rgb(natural_earth='lr') # add a background imagemaps[0].visualize(title='WRF Simlated Domains 1 to 3') Cartopy底图存放位置/User/HYF/anaconda3/lib/⁨python3.7/⁨site-packages⁩/cartopy/⁨data⁩/raster⁩高清图像下载地址 natural Earth I Natural Earth III Natural Earth III with no clouds⁩下载新image后，需修改相同文件夹下的image.json，举例如下: 1234567891011121314{\"__comment__\": \"JSON file specifying the image to use for a given type/name and resolution. Read in by cartopy.mpl.geoaxes.read_user_background_images.\", \"ne_shaded\": { \"__comment__\": \"Natural Earth shaded relief\", \"__source__\": \"http://www.naturalearthdata.com/downloads/50m-raster-data/50m-natural-earth-1/\", \"__projection__\": \"PlateCarree\", \"low\": \"50-natural-earth-1-downsampled.png\", \"high\": \"natural-earth-1_large8192px.png\", \"medium\": \"natural-earth-1_large4096px.png\"}, \"ne_shaded\": { \"__comment__\": \"Natural Earth shaded relief III\", \"__source__\": \"http://www.shadedrelief.com/natural3/pages/textures.html⁩\", \"__projection__\": \"PlateCarree\", \"high\": \"natural-earth-3_large4096px\"}} 2. 中国行政边界重新绘制原始Salem工具包的中国行政地图存在错误。因此，自行绘制国家边界，以准确绘制图形。具体实现相对简单，首先应用Salem工具读取WRF模型的namelist文件，然后应用Cartopy工具绘制，并换用上一节介绍的更为清晰的地形底图。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import salemfrom salem import geogrid_simulatorfrom salem import wgs84fpath = r'./namelist.wps'g, maps,rect= geogrid_simulator(fpath) x0,y0 = pyproj.transform(g[0].proj,wgs84,g[0].extent[0], g[0].extent[2])x1,y1 = pyproj.transform(g[0].proj,wgs84,g[0].extent[1], g[0].extent[3]) def background(ax): fname = './bou2_4p.shp' ## 省界shapefile shape_feature = ShapelyFeature(Reader(fname).geometries(), ccrs.PlateCarree(),facecolor='none',edgecolor ='k',linewidth = 0.25) ax.add_feature(shape_feature) fname = './中国轮廓_含南海.shp' ## 国界shapefile shape_feature = ShapelyFeature(Reader(fname).geometries(),ccrs.PlateCarree(),facecolor='none',edgecolor ='k',linewidth = 0.75) ax.add_feature(shape_feature) fname = './country1.shp' ## 各国边界shapefile shape_feature = ShapelyFeature(Reader(fname).geometries(),ccrs.PlateCarree(),facecolor='none',edgecolor ='k',linewidth = 0.75) ax.add_feature(shape_feature) ax.set_extent([x0,125,y0+3,55], crs=ccrs.PlateCarree()) xticks = [80,90,100,110,120,130,140] yticks = [10,20,30,40,50,60] fig.canvas.draw() ax.gridlines(xlocs=xticks, ylocs=yticks, color='gray', alpha=0.5, linestyle='--',linewidth = 0.75) ax.xaxis.set_major_formatter(LONGITUDE_FORMATTER) ax.yaxis.set_major_formatter(LATITUDE_FORMATTER) lambert_xticks(ax, xticks) lambert_yticks(ax, yticks) ## Lambert投影系统下网格线绘制代码参考https://gist.github.com/ajdawson/dd536f786741e987ae4edef find_side(ls, side): \"\"\" Given a shapely LineString which is assumed to be rectangular, return the line corresponding to a given side of the rectangle. \"\"\" minx, miny, maxx, maxy = ls.bounds points = {'left': [(minx, miny), (minx, maxy)], 'right': [(maxx, miny), (maxx, maxy)], 'bottom': [(minx, miny), (maxx, miny)], 'top': [(minx, maxy), (maxx, maxy)],} return sgeom.LineString(points[side])def lambert_xticks(ax, ticks): \"\"\"Draw ticks on the bottom x-axis of a Lambert Conformal projection.\"\"\" te = lambda xy: xy[0] lc = lambda t, n, b: np.vstack((np.zeros(n) + t, np.linspace(b[2], b[3], n))).T xticks, xticklabels = _lambert_ticks(ax, ticks, 'bottom', lc, te) ax.xaxis.tick_bottom() ax.set_xticks(xticks) ax.set_xticklabels([ax.xaxis.get_major_formatter()(xtick) for xtick in xticklabels]) def lambert_yticks(ax, ticks): \"\"\"Draw ricks on the left y-axis of a Lamber Conformal projection.\"\"\" te = lambda xy: xy[1] lc = lambda t, n, b: np.vstack((np.linspace(b[0], b[1], n), np.zeros(n) + t)).T yticks, yticklabels = _lambert_ticks(ax, ticks, 'left', lc, te) ax.yaxis.tick_left() ax.set_yticks(yticks) ax.set_yticklabels([ax.yaxis.get_major_formatter()(ytick) for ytick in yticklabels])def _lambert_ticks(ax, ticks, tick_location, line_constructor, tick_extractor): \"\"\"Get the tick locations and labels for an axis of a Lambert Conformal projection.\"\"\" outline_patch = sgeom.LineString(ax.outline_patch.get_path().vertices.tolist()) axis = find_side(outline_patch, tick_location) n_steps = 30 extent = ax.get_extent(ccrs.PlateCarree()) _ticks = [] for t in ticks: xy = line_constructor(t, n_steps, extent) proj_xyz = ax.projection.transform_points(ccrs.Geodetic(), xy[:, 0], xy[:, 1]) xyt = proj_xyz[..., :2] ls = sgeom.LineString(xyt.tolist()) locs = axis.intersection(ls) if not locs: tick = [None] else: tick = tick_extractor(locs.xy) _ticks.append(tick[0]) # Remove ticks that aren't visible: ticklabels = copy(ticks) while True: try: index = _ticks.index(None) except ValueError: break _ticks.pop(index) ticklabels.pop(index) return _ticks, ticklabels proj = salem.proj_to_cartopy(g[0].proj)fig = plt.figure(figsize=(6.,5), frameon=True)ax1 = plt.subplot(111, projection=proj)cs = cn_back_plot(ax1)ax1.background_img(name='ne_shaded3',resolution='low')ax1.add_geometries([rect[0][0]], crs = proj,facecolor='none', edgecolor='k', lw=2.5)plt.savefig(\"./模拟嵌套网格.png\",dpi=400)plt.show() 参考 从NetCDF中提取局部数据 WRF projection Python绘制实用地图 WRF和WPS部分配置选项简介 Springer Cartographics LLC","link":"/2020/08/04/23.%5BModel%5DWRF%E5%B5%8C%E5%A5%97%E7%BD%91%E6%A0%BC%E7%9A%84%E7%BB%98%E5%88%B6/"},{"title":"翻译计划: 大气化学电子教材-第一章PartA","text":"前沿大气化学是一门关于大气中化学物质的排放、相互作用、物理及化学转化的综合性学科。同时，其对大气关键的物理过程亦会涉及。本计划所翻译的电子课本对大气化学领域近年来的研究进展(该课本编撰于2013年)进行陈述，如数学模型的并行计算研究，空气污染物对全球气候变化的影响等。原著作者期望本书的内容将为本科生与硕士生提供帮助，以之为专业课程的基础性参考资料。 第一章 大气层的结构和组成大气层的总质量接近5.3x1018kg，而地球水圈(包括海洋、湖泊、河流、地下水以及冰雪)的总质量为1.4x1021kg，岩石圈总质量则为5.98x1024kg。大气层的上边界没有严格的定义。大气层中的物质随高度的上升而急剧减少，并最终融入星际空间之中。包含着大量气体及颗粒组分的大气圈层围绕在地球的外部，随地球一同绕日旋转。 地球大气层的密度分布极不均匀。距离地面5.5 Km以下的空气占据着大气层一半的质量，而大气层约99%的质量分布在距地30 Km以内。地球大气层的范围和地球磁气圈的范围一致(图1.1)，这一形态由地球本体磁场与太阳风[1]的相互作用而最终形成。 图1.1 地球外层的电离层由地球本体磁场与太阳风相互作用而形成 地球大气层的形成45亿年前的早期地球，是一个炙热的岩石星体。最初的大气组分可能包括氢气，氦气和一些含有氢元素的简单气体分子，包括氨气(NH3)和甲烷(CH4)。这一时期地球的大气圈层与今天的木星及土星十分类似。由于地磁场在当时并未形成，来自太阳表层的太阳风不断地侵扰着地球大气，使得早期地球大气不断丧失。同时在4亿年前左右，随着地球不断冷却，伴有强烈火山运动的固体外壳开始成型。火山的不断喷发，使得大量地球内部的气体进入表层。包括水蒸气，二氧化碳以及氨气等气体构成了地球第二阶段的大气圈(次生大气阶段)。火星和金星现在的大气层与当时地球的情形类似，主要由二氧化碳组成。 原始地球大气在历经数百万年的不断冷却中，水蒸气开始凝结成云。云滴作用形成降雨，海洋由此产生。而大气中的二氧化碳开始被海洋逐渐吸收，氨气则在太阳照射下被氧化，形成化学性质稳定的氮气(图1.2)。 图1.2 地球大气各历史时期的物质组成 一些理论认为，地球上最早的氧气由大气中的水蒸气光解产生: $$\\rm 2H_2 + 紫外线(UV) \\rightarrow 2H_2 + O_2 (R1.1)$$ 然而，由上述过程产生的氧气量极小。氧气更主要的来源是蓝藻等有机体的光合作用[2]，这一过程开始于25亿年左右。通过光合作用，有机体利用二氧化碳，水及太阳辐射生成氧气与碳氢化合物，其化学式如下: $$\\rm CO_2 + H_2O + sunlight \\rightarrow 有机物 + O_2 (R1.2)$$ 起初，大气中的少量氧气通氧化地球表层的岩石而被消耗(即岩石的风化过程)。当表层岩石被完全氧化后，大气氧含量的增长更为迅速。当氧含量增加到如今水平(21%)的1 - 2%时，其生成臭氧的过程(O3)能够保护地球表面免受强烈的紫外辐射: $$\\rm 2O_2 + 紫外线(UV) \\rightarrow O_3 + O (R1.3)$$ 以及: $$\\rm O + O_2 \\rightarrow O_3 (R1.4)$$ 在该环境下，原始植物得以进化产生。7亿年前前由海洋登陆的地球植物开始大量吸收大气中的二氧化碳(CO2从1 -5%降至0.04%)，并使得大气中氧气含量增加。大气氧含量在过去的几百万年中的不同时期，受气候变化，火山运动及板块运动的影响而不断波动(Holland, 2006)。最终，大气氧含量稳定在21%左右(图1.3)。 图1.3 氧气可能的历史浓度分布 大气化学简史古希腊的哲学家阿那克西米尼(585–528 BC)认为气体是万物之源。在其之后，恩培多克勒(约490-430 BC)将气体视为四种基本元素之一。直至18世纪，这一观点始终被接受。此时，。最初的研究在1700年左右发表。之后，大气中的各个组分被逐一发现，这证实了空气是由不同类型的气体组成的观点。 表1.1 大气组分的重要发现 时间 组分 发现者 1750s CO2 约瑟夫·布拉克(英) 1766 H2 亨利·卡文迪许(英) 1772 N2 丹尼尔·卢瑟福(英) 1774; 1772(1777年发表) O2 约瑟夫·普利斯特里; 卡尔·威廉·舍勒(瑞典) 1840, O3 弗里德里希·尚班(瑞士) 1894 Ar 约翰·斯特拉特(英)和威廉·拉姆齐(英) 表1.2 20世纪大气化学的重要里程碑 时间 科学家 发现 1924 戈登·多布森(英) 发明了分光光度计，并进行臭氧柱浓度的常规测量 1930 西德尼·恰普曼(英) 发现了臭氧层的生成原理 1960 哈根·施密(美) 解释了光化学烟雾的成因 1973 詹姆斯·拉夫洛克(英) 首次在大气中检测到氟氯烃(CFCs) 1995 保罗·克鲁岑(荷), 马里奥·莫利纳(墨), 弗兰克·舍伍德·罗兰(美) 因“他们对大气化学的研究工作，特别是臭氧的形成与分解研究”，获得1995年诺贝尔化学奖 大气的组成地球大气包含多种气体与气溶胶颗粒。我们可以依据不同气体在大气中的比重与停留时间将其分类。停留时间(亦被称为消除时间，大气寿命)是指一种颗粒物或气体在特定环境下存在时间的平均值。该数值可以用大气中这类物质的总量与去除速率的比值来表示。根据停留时间数值的不同，大气中的气体可以被归类为稳定气体与活性气体(包括高活性气体)，具体如表1.3 表1.3 大气中的气体分类 停留时间 主要组分 微量气体 稳定气体 氮气、氧气、氩气(大气的主要组分) 其它惰性气体 活性气体 二氧化碳 其它长寿命气体 高活性气体 水蒸气 其它短寿命气体 译者补图1.1 大气各类气体的时空尺度(来源: Seinfeld and Pandis, 2016) 我们可以采用多种数值单位对各类气体的量级进行描述。一般来说，质量浓度(kg m-3)，体积比(每m3空气所含某种气体的体积数m3)以及摩尔分数(mol mol-1)较为常用。对于痕量气体(除去氮、氧、氩、二氧化碳)而言，通常用ppmv或ppm来表示每。1 ppmv = 10–6 mol mol–1, 1 ppbv = 10–9 mol mol–1 and 1 pptv = 10–12 mol mol–1 表1.4 地球大气组成 注:水蒸气并未纳入该表格内 附注:[1]太阳风: 指从太阳上层大气喷涌而出的高速粒子流(包括质子和电子) [2]光合作用: 指绿色植物或其他生物将来自太阳的光能转化为化学能的过程，需要二氧化碳和水的参与。对植物而言，该进程发生于其叶片内部。 译者推荐的补充材料:地球大氣組成的演化The Origin of Life on Planet Water水循环","link":"/2015/09/08/2.%5BKnowledge%5D%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92-%E5%A4%A7%E6%B0%94%E5%8C%96%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%95%99%E6%9D%90-1/"},{"title":"可视化工具Altair学习","text":"提要：Altair是强大的可视化库，其基于Vega-lite,可快速生成简洁、美观、可互动的统计图形。本文介绍个人相关学习经历，具体包括：(1)数据载入与基本图形绘制; (2)基于web端呈现 背景知识介绍常用的数据可视化工具包，例如R语言下的ggplot2和Python语言下的matplotlib，均难以制作交互式图表。不同于传统的静态图表，交互式图表允许研究者及读者可以探索甚至操纵原始数据，便于我们理解数据关系，发现其隐藏规律。Python语言下可交互的可视化工具包括Altair, Bokeh及plotly等多种。 MatPlotlib的优势： Designed like MatLab: switching was easy Many rendering backends well-tested, standard tool for 15 years can reproduce just about any plot with a bit of effort MatPlotlib的劣势 API is imperative &amp; often overly verbose Poor/no support for interactive/web graphs存在问题: we are mixing the what with the how 在应用MatPlotLib的过程中，不仅需要考虑画什么，还需要考虑如何实现。坐标轴等亦需要自行设置，每次都在等微调中耗费大量时间。实际上，应当将更多的时间用于数据的分析及写作中，出图效率应提高。因此，我决定全面转向altair作为未来个人的主要可视化工具，在日常工作中频繁使用，不断积累。 123## 安装## https://raw.githubusercontent.com/vega/vega/master/docs/examples/bar-chart.vg.jsonpip install altair vega_datasets Web端数据展示先存成json文件，再利用vega embed 实现。其基本格式总结如下： 123456789101112131415161718{% raw %}&lt;html&gt;&lt;head&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vis\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; # spec中载入altar对象生成的json文件 var spec = \"/uploads/vconcat.json\"; var opt = {\"renderer\": \"canvas\", \"actions\": false}; vegaEmbed(\"#vis\", spec, opt); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;{% endraw %} 此处展示基本例子 123456789101112131415161718{% raw %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vis1\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var spec1 = \"/uploads/vconcat.json\"; var opt1 = {\"renderer\": \"canvas\", \"actions\": false}; vegaEmbed(\"#vis1\", spec1, opt1); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;{% endraw %} 在同一个静态网站中需要展示多个interactive plots, 基本方法如下：修改上述文件中3个变量, vis, spec, opt 123456789101112131415161718{% raw %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vis2\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var spec2 = \"/uploads/cc.json\"; var opt2 = {\"renderer\": \"canvas\", \"actions\": false}; vegaEmbed(\"#vis2\", spec2, opt2); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;{% endraw %} 123456789101112131415161718{% raw %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vis3\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var spec3 = \"/uploads/car-clickable-legend.json\"; var opt3 = {\"renderer\": \"canvas\", \"actions\": false}; vegaEmbed(\"#vis3\", spec3, opt3); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;{% endraw %} 常见问题 数据集长度过大 (The number of rows in your dataset is greater than the maximum allowed (5000).)官方回答摘录如下 This is not because Altair cannot handle larger datasets, but it is because it is important for the user to think carefully about how large datasets are handled. As noted above in Why does Altair lead to such extremely large notebooks?, it is quite easy to end up with very large notebooks if you make many visualizations of a large dataset, and this error is a way of preventing that.2.上标、下标应当如何展示altair不支持latex符号的显示，因此需要寻找各数学符号的unicode字符。常用字符整理如下： 微克/米三次方：’\\u03BC’+’g/m’+’\\u00B3’ 参考文献 PYCON 2018 D3 in hexo 嵌入bokeh绘图至hexo博客中 The reason I am using Altair for most of my visualization in Python 如何插入多个图片 vega-embed vega-lite How to find the unicode of the subscript alphabet?","link":"/2020/05/10/21.%5BCode%5DAltair%E5%AD%A6%E4%B9%A0/"},{"title":"Windows电脑工作环境配置笔记","text":"在国外新购置了一台PC，性能比从国内带来的Mac笔记本要好，也可以继续玩Steam上的游戏。决定将个人学习工作环境迁移至Windows系统。在此记录相关配置步骤，以便未来之需。 1. 常用软件安装首先安装常用软件，尽量选择免费开源软件替代相似功能的商业软件。 Markdown文本编辑：Mark text 视频播放工具：mpv 图片浏览工具：ImageGlass PDF文本阅读：Sumatra PDF (Ctrl+a 高亮文本) 小工具合集：PowerToys （Win+Shift+C 取色器) 截图工具：Snipaste (F1截图快捷键） 拼音输入法：RIME (安装后初始为繁体字，F4进入设置菜单调整为简体) 番茄时钟: Pomotroid FTP数据传输管理: WinSCP 2. Python+VSCODE+WSLPython及Jupyter notebook/lab是我主要的数据分析工具。VS CODE编辑器是功能强大，插件丰富的开发工具，未来会多尝试在该环境下工作。WSL即Windows subsystem for Linux，是Windows环境下Linux子系统，无需安装双系统，且可以在Linux环境可直接访问Windows文件系统，相比虚拟机有更好的运行性能。 具体下载安装环节包括 Anaconda VS Code Insiders (此处安装Insider，而非标准版VS Code。原因是因为在加载remote development等插件时总是报错，自己没找到解决方法) Ubuntu 20.04 LTS Windows Terminal 安装上述软件工具后，在VS Code中安装remote-WSL、jupyter、VS Code Jupyter Notebook Previewer、Visual Studio IntelliCode、Chinese Language pack等必要插件。 1234567891011121314151617## terminal环境下wsl切换至linuxcurl -O https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh环境，完成基本配置## 安装必要编译环境sudo apt updatesudo apt install build-essentia## 安装zsh, oh-my-zsh, powerlinesudo apt-get install zshgit clone https://github.com/ohmyzsh/ohmyzsh.git ~/.oh-my-zshcp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrcchsh -s /bin/zshsudo apt-get install powerline fonts-powerline##WSL环境安装anaconda3cd /tmpcurl -O https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.shbash ./Anaconda3-2021.05-Linux-x86_64.sh 接下来实现在wsl环境开启Jupter lab，在Windows环境chrome浏览器打开，直接开启jupyter notebook/lab，系统报错均为”Start : This command cannot be run due to the error: The system cannot find the file specified.”, 但并影响使用。 根据此问题的回答，Ubuntu 20.04系统会出现该问题。 有关terminal环境下文件夹出现绿色背景，影响美观，参考该问答进行修改。 bash环境下vi ~/.dir_colors修改OTHER_WRITABLE从”34;42”至“34;49”，如下 zsh环境下则不为所动，添加以下内容至~/.zshrc，并source之 12345if [[ -f ~/.dircolors ]] ; then eval $(dircolors -b ~/.dircolors) elif [[ -f /etc/DIR_COLORS ]] ; then eval $(dircolors -b /etc/DIR_COLORS)fi 3. Latex配置未来可能有Latex编辑文档需求，在此记录VS Code环境实现直接编译源码并展示PDF文档的主要步骤： 安装Texlive 安装VS Code插件Latex Workshop 修改settings.json 若选择外部PDF软件呈现工具，修改参考此网页。此处选择VS Code自带预览读取PDF，Ctrl+Shift+P进入Command Palette，键入settins.json，选择第二项，插入如下代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768\"latex-workshop.latex.tools\": [ { \"name\": \"pdflatex\", \"command\": \"pdflatex\", \"args\": [ \"-synctex=1\", \"-interaction=nonstopmode\", \"-file-line-error\", \"%DOC%\" ] }, { \"name\": \"xelatex\", \"command\": \"xelatex\", \"args\": [ \"-synctex=1\", \"-interaction=nonstopmode\", \"-file-line-error\", \"%DOC%\" ] }, { \"name\": \"bibtex\", \"command\": \"bibtex\", \"args\": [ \"%DOCFILE%\" ] } ], \"latex-workshop.latex.recipes\": [ { \"name\": \"pdflatex\", \"tools\": [ \"pdflatex\" ] }, { \"name\": \"xelatex\", \"tools\": [ \"xelatex\" ] }, { \"name\": \"xe-&gt;bib-&gt;xe-&gt;xe\", \"tools\": [ \"xelatex\", \"bibtex\", \"xelatex\", \"xelatex\" ] }, { \"name\": \"pdflatex -&gt; bibtex -&gt; pdflatex*2\", \"tools\": [ \"pdflatex\", \"bibtex\", \"pdflatex\", \"pdflatex\" ] } ], \"latex-workshop.view.pdf.viewer\": \"tab\", \"latex-workshop.latex.autoClean.run\": \"onBuilt\", \"security.workspace.trust.untrustedFiles\": \"open\" 当然，在线使用Overleaf更为便捷。好不容易配置好，才想到┗( T﹏T )┛。 4. Hexo博客多终端编辑Hexo是简单、便捷的基于Github Pages的博客框架。本博客即基于此构建建立。此处我设想在多台终端均可进行博客写作和发布。由于github端文件只包括编译后结果，无原始文章信息，因此无法直接在新PC上c直接clone并使用。 首先，在WSL环境下安装Hexo框架及Node.js，具体指令如下： 12345678910111213141516## 1. 安装Node.jscurl -sL https://deb.nodesource.com/setup14.x | sudo -E bash -sudo apt-get install -y nodejs## 报错 \"not foundram Files/nodejs/npm: 3: /mnt/c/Program Files/nodejs/npm:\"## 参考 https://stackoverflow.com/questions/45853530/npm-install-error-not-foundram-files-nodejs-npm-3-mnt-c-program-files-nodejs## 在~/.zshrc中添加以下行，并添加sourcePATH=\"$HOME/bin:$HOME/.local/bin:/usr/bin:$PATH\"## 2. 安装Node.js框架sudo npm install hexo-cli -g### 建立一个新文件夹blog，存放博客文件sudo npm installnpm install -S highlight.js --savenpm install -S cheerio --savenpm install -g hexo-renderer-sass --savenpm install hexo-deployer-git --save 进一步设置，有两类思路： 直接迁移原有计算机中hexo文章内容，主题设置和配置文件，具体包括_config.yml,theme、source； 在新PC中直接初始化hexo框架，参考原有*.yml文件重新设置。 经本人测试，方法1）在hexo clean阶段反复报错，难以解决。个人认为可能原有编译文件和现在的hexo版本不符。因此，我初始化了log文件夹，移入博客历史文章，修改配置文件，具体如下： 123456789hexo init## 转移文章cp ./../hexo_ori/source/_post/* ./source/_post/.## 安装主题git clone https://github.com/ppoffice/hexo-theme-minos.git themes/minos## 按照原设置，修改主文件夹下_config.yml文件和theme文件夹下_config.yml文件hexo ghexo server 本地编译测试无误后，设置SSh密钥，关联github个人账户，具体如下： 123456789101112131415161718## 1. 生成sshssh-keygen -t rsa -b 4096 -C \"your_github_email@example.com\"## 2. 复制/home/your_name/.ssh/id_rsa.pub全部内容，添加至github账户中的SSH keys中## 3. 检测成功设置ssh -T git@github.com## 4. git初始化git config --global user.email \"you@example.com\"git config --global user.name \"Your Name\"## 5. 配置个人域名，新建CNAME文件，添加个人域名vi ./source/CNAME ## 6. 移入本篇文章，开始部署hexo g hexo d 为实现Hexo博客可在多设备同时撰写发布，查询网络资料寻找可行方法。多数教程建议采用教程提供了hexo源文件新建github分支的做法。但该方法公开了ssh密钥信息。此处，我专设文件夹撰写.md格式文件，置于Dropbox文件夹内，不同终端间可共享。更新博客时，只需将其新写入的文章拖入hexo的编译文件夹内更新部署。 未来会继续整理在WSL环境下安装，调用学科专业软件内容。 PS. 2025.08.03一不小心将 Hexo 升级到了 7.0+ 版本，结果发现自己钟爱的 Minos 主题出现了运行报错（毕竟该主题多年未更新）。 主要错误为： 12ERROR Script load failed: themes/minos/scripts/01_check.jsTypeError: require(...) is not a function 这些错误主要出现在 Minos 的自定义 JS 文件中，由于本人并不熟悉 JavaScript，因此暂时没有深入修改。原本打算更换其他主题，但还是“不思进取”，觉得 Minos 的排版风格就是顺眼 😅。 搜索网页论坛信息，错误可能是由 Hexo 7.x 与旧主题/插件不兼容 导致的，比如 require(…) is not a function 这类 JS 模块调用方式的改变。因而学习如何回退。 12345678910111213141516# 初始化 Hexo 项目hexo init blogcd blog# 降级 Hexo 到 6.1.0npm install hexo@6.1.0# 安装兼容的 Sass 渲染器npm uninstall hexo-renderer-sass sassnpm install --save hexo-renderer-sass sass# 清理 + 编译 + 本地服务hexo cleanhexo ghexo server -p 5001 运行时再次出现另一个问题 1Error: Unknown output style \"nested\" 这是因为blog/_config.yml中设置了： 12node_sass: outputStyle: nested 将nested改为expanded以后报错取消，但warning阴魂不散： 1Deprecation Warning [import]: Sass @import rules are deprecated... 这是因为许多老主题仍使用了 @import，而非新标准中的 @use。目前不影响使用，后续如有精力我也不想花在这上面😅。 部分参考资料 Running Jupyter Notebook on WSL while Using Browser on Windows WSL2下用Hexo在github上搭建静态博客 · Joe Suen's Blog","link":"/2021/07/14/24.Windows%E7%94%B5%E8%84%91%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/"},{"title":"小工具开发：网络预览文献资料的下载合成","text":"部分互联网资料仅提供在线预览，无法直接下载。若预览系统设计欠佳，或限于网速内容加载过慢，都会对阅读体验造成严重影响。在此分别以国家自然基金研究报告和学位论文为例，介绍两种不同的下载合成方法，供有需要的老师同学们参考。欢迎大家提出宝贵意见。 1. 国家自然基金研究报告国家自然科学基金共享服务网中公开的项目报告只能在线阅览。以《颗粒物质基本性质的研究》研究为例，在其结题报告全文页，获取首页图片网址-http://output.nsfc.gov.cn/report/10/10274071_1.png确定目标页URL后，接下来的工作可用Python代码自动完成，代码具体如下： 1234567891011121314151617import osimport natsortfrom fpdf import FPDFfor i in range(0,200,1): os.system(\"wget http://output.nsfc.gov.cn/report/10/10274071_%i.png\" %i)files = os.listdir('./')files = natsort.natsorted(files, reverse = False)files = files[0:]pdf = FPDF()for image in files[0:]: if image[-3:]=='png': pdf.add_page() pdf.image(image,0,0,210,297) pdf.output(\"../颗粒物质基本性质的研究.pdf\", \"F\") 该方法缺陷有二： 报告总页数需要人工查询，下载时应尽量设置较大的循环次数(如200次)； 报告部分横置页面，在合成前未做旋转调整。 2. 研究生学位论文一些大学的学位论文不能在中国知网等数据库直接下载，本校在线预览方式也不太方便。考虑到原始网页采用javascript动态加载论文内容(实为jpg格式图片)，无法通过wget或urllib等简单工具直接获取。因此，选用selenium工具模拟chrome浏览器鼠标操作，逐页下载。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 主要参考网页 https://www.devdungeon.com/content/grab-image-clipboard-python-pillowfrom selenium import webdriverfrom time import sleepimport pyperclip,pyautoguiimport numpy as np# 下载环节chrome_options = webdriver.ChromeOptions()prefs = {'download.default_directory' : '/user/defined/path'}chrome_options.add_experimental_option('prefs', prefs)driver = webdriver.Chrome(executable_path=\"./geckodriver/chromedriver\", chrome_options=chrome_options)## 此url具体位置如下图driver.get(\"https://thesis.lib.pku.edu.cn/onlinePDF?dbid=72&amp;objid=53_57_54_50_56_49&amp;flag=online\")for t in np.arange(1,3,1): img_url = driver.find_element_by_id('ViewContainer_BG_0').get_attribute('src')[:-10] driver.get(img_url+\"_%s.jpg\" %\"{:05d}\".format(t)) # Move to the specified location, right click pyautogui.rightClick(x=600, y=500) # V pyautogui.typewrite(['V']) pyautogui.hotkey('ctrlleft','V') sleep(0.8) pyautogui.press('enter') sleep(0.8) pyautogui.press('enter')driver.close()# 图片调整环节## 有些图片是横置的，逆时针旋转90度import osimport natsortfrom fpdf import FPDFimport cv2files = os.listdir('/DOWNLOAD_PATH/')files = natsort.natsorted(files, reverse = False)files = files[0:]for file in files: if file[-3:]=='jpg': img = cv2.imread('/DOWNLOAD_PATH/'+file) h,w,c = img.shape if h&lt;w: imgrot = cv2.rotate(img,cv2.ROTATE_90_COUNTERCLOCKWISE) cv2.imwrite('/DOWNLOAD_PATH/'+file,imgrot) # 合成PDF pdf = FPDF()for file in files[0:]: if file[-3:]=='jpg': pdf.add_page() pdf.image('/DOWNLOAD_PATH/'+file,0,0,210,297) pdf.output(\"FILENAME.pdf\", \"F\") 在北大图书馆检索后，点击目标文献，在该页码打开chrome开发者工具，寻找到下列标记字段，右键选择“copy link element”即为预览页url。","link":"/2020/05/10/20.%5BCode%5D%E5%B0%8F%E5%B7%A5%E5%85%B7%E5%BC%80%E5%8F%91%EF%BC%9A%E7%BD%91%E7%BB%9C%E9%A2%84%E8%A7%88%E6%96%87%E7%8C%AE%E8%B5%84%E6%96%99%E7%9A%84%E4%B8%8B%E8%BD%BD%E5%90%88%E6%88%90/"},{"title":"在低版本Linux安装科学计算库的曲折之路","text":"以前处理WRF等气象模型的输出文件，总是下载到本地电脑做后处理分析。由于计算量不断增加，模拟生成的文件往往会很大。因而，我考虑直接在服务器中处理数据。本来是很容易的事情，却因为课题组服务器的系统版本较旧，在安装有关工具时耗去了不少时间。在此记录我的探索过程。 服务器的基本信息 123456$cat /etc/redhat-releaseCentOS release 5.9 (Final)$gcc --versiongcc (GCC) 4.9.4$ncdump --versionnetcdf library version 4.6.1 1. gdal库安装gdal是一个常用的地学数据库，可用于读取、编写netCDF等格式的数值文件。尝试利用conda工具安装： 1$conda install -c conda-forge gdal 安装完毕后，无法启动python，错误信息如下： 1python: /lib64/libc.so.6: version `GLIBC_2.7' not found (required by /home/hyf/anaconda2/bin/../lib/libpython2.7.so.1.0) 上文已述，服务器系统为Cent OS 5.9版本，并不支持glibc_2.7。可用以下指令进行检查： 12345678910$strings /lib64/libc.so.6 |grep GLIBC_ GLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_PRIVATE 同时，gdal安装过程conda环境也升级至Anaconda5，不再适配于CentOS 5.X/glibc_2.5的系统环境，导致Python环境崩坏，无法启动。因而，我只得重新安装Anaconda，并设置安装package阶段不升级conda环境至5.x版本，以避免该错误的再次出现。 1234567# 已安装的库文件名称存一下printf '%s\\n' ~/anaconda2/pkgs/* | paste -sd \",\" - &gt; log.csv# 卸载anancondarm -rf ~/anaconda2rm -rf ~/.condarc ~/.conda ~/.continuum#重新安装Anaconda，在其下载文件夹内进行./Anaconda2-4.0.0-Linux-x86_64.sh 参考conda github issue#6041，按如下指令对conda环境进行限制，防止其升级 123$conda config --set auto_update_conda false# 新的错误再次出现Error: Error key must be one of always_yes, root_dir, channel_alias, add_anaconda_token, add_pip_as_python_dependency, show_channel_urls, update_dependencies, changeps1, allow_softlinks, anaconda_upload, binstar_upload, use_pip, offline, allow_other_channels, add_binstar_token, ssl_verify, always_copy, not auto_update_conda 查询网络资料得知，在conda 4.2版本后，该bug得以修复。因而conda install conda =4.2指令后再强制其不升级，不再出现上述错误。 重新安装gdal库文件，python可正常启用，但在调用gdal库函数时，又出现了新的错误（༼ ͠ຈ Ĺ̯ ͠ຈ ༽）： 12from osgeo import gdal&gt;ImportError: /home/hyf/anaconda2/lib/python2.7/site-packages/osgeo/../../../libgdal.so.20: ELF file OS ABI invalid 参考这篇文章的解释，该错误上文“glibc_2.7”报错是类似的。系统的ldd version同样陈旧(2.5)，无法支持gdal库正常使用。将Cent OS升级成6.x版本可一劳永逸地解决这类问题，但我并无权限，也无决心重新配置整个系统。 在反复安装测试未果后，我决定放弃╮(╯_╰)╭，转向安装其他支持读取netcdf格式的python库。在此期间，我还尝试在服务器上安装了R语言环境和ncdf package，以后有时间专文记述。 2. netCDF4-python库安装首先尝试Anaconda网站提供的方法，直接利用conda工具安装 1$conda install -c anaconda netcdf4 安装后，出现python无法启动的错误，具体为“importError: cannot import name md5”。Stackoverflow上类似题的回答表明，该错误是在anaocnda5.0.1版本中得以修复。但是，我不能安5.x版本的conda啊இдஇ，于是再想其他办法。 重新安装anaconda环境后，我尝试用手动install from source的方法来安装，具体netcdf4-python的github主页中的介绍： 1234567891011121314$git clone https://github.com/Unidata/netcdf4-python.gitcd netcdf4-python# 修改 setup.cfg中的netcdf, hdf5存放路径sed -i '/\\[directories\\]/a \\HDF5_dir = $DIR/hdf5 \\netCDF4_dir = $DIR/netcdf' setup.cfg# run setup.py$python setup.py build$python setup.py install# run test.py$cd test$python run_all.py 不出意外，安装过程肯定是要报错的，具体的错误信息如下： 1234/usr/bin/ld: /disk2/hyf/lib/netcdf/lib/libnetcdf.a(libdispatch_la-att.o): relocation R_X86_64_32 against `a local symbol' can not be used when making a shared object; recompile with -fPIC/disk2/hyf/lib/netcdf/lib/libnetcdf.a: could not read symbols: Bad valuecollect2: error: ld returned 1 exit statuserror: command 'gcc' failed with exit status 1 该结果表明，netcdf库在安装过程，关闭了enable-shared，共享库未进行编译，并进一步导致了\"a local symbol\" can not be used …的问题。正好在flexpart安装过程(参见之前的博文FLEXPART installation notes)中，我又安装了一套netCDF库，在该环境下重新手动安装，该错误不再显示。 123456789101112# change netCDF$source ~/.bashrc_forFlex$ncdump --versionnetcdf library version 4.6.1# run setup.py$python setup.py build$python setup.py install# run test.py$cd test$python run_all.py 成功安装后，在/netcdf4-python/test/文件夹下可运行，并调用netCDF4库函数(from netCDF4 import Dataset)。测试结果表明，写netCDF格式文件(mode=’w’)一切正常，但有两个致命问题：1. 仅能在该文件夹内，方可成功import netCDF4 ，2. 无法读取现有的nc数据，报错如下： 1filepath method not enabled. To enable, install Cython, make sure you have version 4.1.2 or higher of the netcdf C lib, and rebuild netcdf4-python. Cython我已经安装了，netcdf C 版本也高于4.1.2，到底是哪里出问题了呢？参考github上netcdf-python issue#263，我检查了python setup.py build时的output，其明确显示 123456netcdf lib does not have group rename capabilitynetcdf lib does not have nc_inq_path functionnetcdf lib does not have nc_inq_format_extended functionnetcdf lib does not have nc_open_mem functionnetcdf lib does not have cdf-5 format capabilitynetcdf lib does not have netcdf4 parallel functions 初步断定”nc_inq_path function” 的缺失，是导致”filepath method not enabled”的根本原因。反复搜索相关资料，也未查询到相似的问题以及解决方法。 完全放弃前，我尝试conda install netcdf4 ，竟然意外成功，可以正常读取nc格式文件了。 ✧｡٩(ˊᗜˋ)و✧*｡ ✧｡٩(ˊᗜˋ)و✧*｡ ✧｡٩(ˊᗜˋ)و✧*｡ 参考资料1. GCC install 2. NetCDF in R 3. Installing NetCDF and R ‘ncdf’ 4. Aqua-Duct installation guide","link":"/2018/09/04/11.%5BModel%5D%E6%B0%94%E8%B1%A1%E6%A8%A1%E6%8B%9FNetCDF%E8%BE%93%E5%87%BA%E6%96%87%E4%BB%B6%E5%90%8E%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85/"},{"title":"2 Easy Steps to Download Historical Weather Data","text":"The National Oceanic and Atmospheric Administration (NOAA) Integrated Surface Database (ISD) provides one of the richest sources of historical weather data consisting of hourly and synpoptic observation. This blog will introuce the simple way to retrieve and process the raw data into Python dataframe. Selection of weather stationBased on the location of the city/sampling site, we could seach for the best/most close weather station for getting its ID. Here, Milan, Italy will be treated as the example, and the main procedures are listed as follows. The detailed information of weathe station globally can be downloaded here link. 1234567891011import pandas as pdisd_st = pd.read_csv(\"/mnt/d/Dropbox/data/geo/Meteo/NOAA/global/isd-history.csv\")## filtering based on country nameisd_st[isd_st['CTRY'] =='IT']## filtering based on city nameisd_st[isd_st[\"STATION NAME\"].astype(str).str.contains(\"MILAN\")]## Then, we could choose the perferred monitoring station, and note its \"USAF\" and \"WBAN\"## For Milan, USAF == 160800, WBAN == 99999, then the target ID will be \"160800-99999\"station_ID = 160800-99999 A more visualized way could be achieved through this link Find a station. FTP downloading and processing1234567891011121314151617181920212223242526272829303132333435363738from ftplib import FTPfrom pathlib import Pathftp = FTP(\"ftp.ncdc.noaa.gov\")ftp.login()## there are multiple dataset within NOAA ISD. Here we choose NOAA ISD-Lite, which is hourly-recoreded.ftp.cwd('/pub/data/noaa/isd-lite/2013/')# Get all filesfiles = ftp.nlst()# Download the file matched with the station IDfor file in files: if file == station_ID +'.gz':# print (file) print(\"Downloading...\" + file) ftp.retrbinary(f'RETR {file}', open(str(Path(r'./isd_data') / file), 'wb').write)# # ftp.close()def calc_rh(T,TD): RH = 100*(np.exp((17.625*TD)/(243.04+TD))/np.exp((17.625*T)/(243.04+T))) return RHdef read_year_meteo(filepath): df= pd.read_csv(filepath,sep='\\s+',header = None) #NOTE: Trace precipitation is coded as -1 # SCALING FACTOR: 10 df.columns = ['Year','Month','Day','Hour','Air temp','Dew point','Pressure','WD','WS','Cloud','1h prep','6h prep'] df['Date'] = df['Year'].astype(str)+'-'+df[\"Month\"].apply(\"{0:0=2d}\".format)+'-'+df[\"Day\"].apply(\"{0:0=2d}\".format)+' '+df[\"Hour\"].apply(\"{0:0=2d}\".format)+':00:00' df = df[['Date','Air temp','Dew point','Pressure','WD','WS','Cloud','1h prep','6h prep'] ] df = df.replace(-9999,np.nan) for t in ['Air temp','Dew point','Pressure','WS','1h prep','6h prep']: df[t] = df[t]/10.0 df['RH'] = calc_rh(df['Air temp'],df['Dew point']) return dfmilan_2013 = read_year_meteo('./isd_data/160800-99999-2013') milan_2013.to_csv(\"./milan_weather_data_hourly.csv\", index = False) Done! Reference How We Process the NOAA Integrated Surface Database Historical Weather Data Preparing the NOAA ISD, Hourly, Global Dataset for Time-series Databases Getting Weather Data in 3 Easy Steps","link":"/2022/07/10/26.%5BDataset%5D-Easy-Steps-For-NOAA-Weather-Data/"},{"title":"Dataset and Tools Collection for Earth Science","text":"本文持续更新个人网络搜集的各类数据集，包括中国/世界行政区划、自然地理、气象水文、空气质量及社会经济数据等。 1. 行政区划 / 社会经济数据 Countries States Cities Database (全球行政区)提供 JSON, SQL, XML, YAML, CSV 等多种格式 World Countries in 31 Languages提供世界国家信息的多语种版本和格式（CSV, JSON, PHP, SQL, XML） 中国县级以上行政区划变更查询民政部官方平台，更新至2021年 印度行政区划 GeoJSON 文件） 2. 污染排放与能源数据 Open-Source Mapping of China’s Energy Infrastructure由莱斯大学中国研究中心发布，包含全国能源设施空间分布 Steel, Chemicals and Heavy Industries in China简要介绍中国重工业发展状况 世界能源消耗与化石燃料格局法国 Sciences Po 地理资源平台 全球火电厂分布图2020年全球火电厂地图 Global Power Plant Database包含各国火电、水电、核电等发电厂信息 3. 大气化学组分, 空气质量数据及可视化平台 Sentinel-5P逐日数据包含气溶胶光学厚度(AOD)以及CH4, C2H4O2, CO, HCHO, O3, NO2, SO2等气态组分的全球逐日分布数据。User case里有非常好的制图参考。 全球逐日PM2.5浓度分布 2017-2022 1Km分辨率, 类似的其他污染物数据，同样是韦晶博士提供的，详见[https://weijing-rs.github.io/product.html] 全球1998-2023 卫星反演PM2.5分布 AQICN 全球空气质量信息全球分布 | 亚洲8日预测 中国主要城市空气质量对比图表（网友作品） PiMi 空气质量可视化平台 绿色地球 - 可视化空气质量地图 Green Earth from NOAA 韩国WRF-CHEM模拟动画 NASA Earth Observatory NASA Worldview 实时真彩色地球 NASA Visualization Studio Maphub 地图云集可查看并自制各种类型的历史与现代地图 AirVisual Earth – 空气质量与风场可视化 4. 人体健康相关数据集Global Burden of Disease (GBD) by IHME 华盛顿大学健康指标与评估研究所（IHME）提供的全球疾病负担数据库，包括死亡、致残、风险因子、暴露、期望寿命等高分辨率数据，可按国家、省、性别、年龄等维度下载。","link":"/2023/01/03/27.%5BDataset%5DEarth-Science-Tools-and-Data/"},{"title":"The work transformed from shapefile to geojson file","text":"Geojson provides several advantages compared to shapefiles: lightweight, text-based, and easily readable format that can be easily shared, transmitted and used on the web. It supports a wider range of data types compared to shapefile and can be used across multiple platforms and programming languages. Additionally, geojson files have smaller file sizes, making them easier to store and process. These benefits make geojson a popular choice for geographic information data storage and exchange. 1. Python-based GeoJson manipulation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657## 1. Search specific data from raw data and load as Shapely objectfname = './india_state_geo.json'with open(fname) as f: data = json.load(f)for i, c in enumerate(data['features']): if c['properties']['NAME_1'] in ['Haryana', 'Uttar Pradesh', 'Punjab']: print (i,c['properties']['NAME_1'] )hary_shp = shape(data['features'][12]['geometry'])## 2. Mask 2-d array by shapely objectimport timestart = time.time()from warnings import filterwarningsfilterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')xx_, yy_ = np.meshgrid(lon_sa,lat_sa)mask_hary = shapely.vectorized.contains(hary_shp, xx_,yy_)## 3. Merge of multiple selected polygons from shapely.geometry import shape, MultiPolygonimport json### 3.1 Load the GeoJSON data into a Python objectwith open(fname) as f: data = json.load(f)### 3.2 Convert the polygon features into shapely objectspolygons = [shape(feature['geometry']) for feature in data['features']]### 3.3 Merge the polygons using the union methodpoly_index = [3,4,15,19,21,22,23,24,31,34]merged_polygon = polygons[poly_index[0]]for i,polygon in enumerate(polygons): if i in poly_index[1:]: merged_polygon = merged_polygon.union(polygon)### 3.4 Create a MultiPolygon object if necessaryif type(merged_polygon) != MultiPolygon: merged_polygon = MultiPolygon([merged_polygon])## 4. Transforming Shapefile to Geojsonimport fionaimport jsonfname2 = './example.shp'# Open the shapefile using fionawith fiona.open(fname2) as src: # Create a new GeoJSON file with open( 'example.json', \"w\") as output: # Write the GeoJSON representation of the shapefile to the file output.write(json.dumps(list(src)))## 5. Merge A with Bwith open('A.json') as f: A = json.load(f)with open('B.json') as f: B = json.load(f)merged_polygon = A.union(shape(B[0]['geometry'])) 2. Javascript-based Geojson visualizationcontinue","link":"/2023/02/23/28.%5BCode%5DFrom-Shapefile-to-Geojson/"},{"title":"Making Your Own Python Package Publicly Available","text":"In this post, I document the full procedure for turning a personal Python project into a public package, available on both GitHub and the Python Package Index (PyPI). As a demonstration, I create a small package named PalAniSh (Palette of Animation of Shanghai), which extracts and displays color palettes from classical Chinese animations produced by the Shanghai Animation Film Studio. 1. Color Palette generationHerein, I will use some of the screenshots from the Chinese traditional animations, e.g., the 1956 animation _The Proud General (骄傲的将军). All raw images as source of color palette were derived from Bilibili. The extraction and combination of colors were similar to the web-based tool Adobe color or Colormind (http://colormind.io/image). Below is the core workflow: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384from PIL import Imageimport matplotlib as mplfrom matplotlib.offsetbox import OffsetImage, AnnotationBboxfrom matplotlib.colors import ListedColormapimport cv2import extcolorsfrom colormap import rgb2hex,hex2rgbimport colorsysdef get_hsv(hexrgb): hexrgb = hexrgb.lstrip(\"#\") # in case you have Web color specs r, g, b = (int(hexrgb[i:i+2], 16) / 255.0 for i in np.arange(0,5,2)) return colorsys.rgb_to_hsv(r, g, b)def get_hls(hexrgb): hexrgb = hexrgb.lstrip(\"#\") # in case you have Web color specs r, g, b = (int(hexrgb[i:i+2], 16) / 255.0 for i in np.arange(0,5,2)) return colorsys.rgb_to_hls(r, g, b)### HSV sorting def get_sorted_df_color(input_color,sort_type): input_ = input_color colors_pre_list = str(input_).replace('([(','').split(', (')[0:-1] df_rgb = [i.split('), ')[0] + ')' for i in colors_pre_list] df_percent = [i.split('), ')[1].replace(')','') for i in colors_pre_list] df_color_up = [rgb2hex(int(i.split(\", \")[0].replace(\"(\",\"\")), int(i.split(\", \")[1]), int(i.split(\", \")[2].replace(\")\",\"\"))) for i in df_rgb] df_color_up_new = df_color_up.copy() if sort_type == 'hsv': df_color_up_new.sort(key = get_hsv) if sort_type == 'hls': df_color_up_new.sort(key = get_hls) df_color_up_new = df_color_up_new[::-1] if \"#FFFFFF\" in df_color_up_new: df_color_up_new.insert(0, \"#FFFFFF\") df_color_up_new = df_color_up_new[0:-1] re_index = [df_color_up.index(c) for c in df_color_up_new] df_percent_new = [df_percent[i] for i in re_index] df = pd.DataFrame(zip(df_color_up_new, df_percent_new), columns = ['hsv_code','occurence']) if sort_type == 'none': df = pd.DataFrame(zip(df_color_up, df_percent), columns = ['hsv_code','occurence']) df['rgb_color'] = [[hex2rgb(c)[0]/255.0,hex2rgb(c)[1]/255.0,hex2rgb(c)[2]/255.0,] for c in df['hsv_code']] return dfdef get_color_x(input_name,tol,num): output_width = 200 #set the output size img = Image.open(input_name) wpercent = (output_width/float(img.size[0])) hsize = int((float(img.size[1])*float(wpercent))) img = img.resize((output_width,hsize), Image.ANTIALIAS) # 1. Show the resized image resize_name = './resize_' + 'test_.png'#the resized image name img.save(resize_name) img_url = resize_name colors_x = extcolors.extract_from_path(img_url, tolerance=tol, limit = num) return colors_x# plot the result color palattedef plot_image_with_color_palette(df_color,input_name,palatee_name,axs): img_url = input_name img = plt.imread(img_url) axs[0].imshow(img) axs[0].axis('off') name = palatee_name color_list = df_color['rgb_color'].values color_name = df_color['hsv_code'].values cmap_diy = ListedColormap(color_list, name) col_map = cmap_diy new_val = [] ticks = np.linspace(0.0,1, len(color_list)+1) for i in range(0,len(ticks)-1,1): new_val.append((ticks[i]+ticks[i+1])/2.0) cbar = mpl.colorbar.ColorbarBase(axs[1], cmap=col_map, orientation = 'horizontal', ticks =new_val, alpha = 0.75) cbar.ax.set_xticklabels(color_name, fontsize =10, rotation = 30) ttl = plt.title(name,fontweight=\"bold\",fontsize =16,) ttl.set_position([.5, 1.15]) Example output 12345input_name = './SA_palette/九色鹿/九色鹿9.jpg'colors_x = get_color_x(input_name,10,10)df_color_jsl = get_sorted_df_color(colors_x,'hsl')fig, axs = plt.subplots(2,1,figsize=(6,5), gridspec_kw={'height_ratios': [5, 1]})plot_image_with_color_palette(df_color_jsl,input_name,'A Deer of Nine Colors', axs) Other examples are shown as follows 2. Packaging the Code and Uploading to GitHubUsually, a package will have such structure: 12345678910palanish/├── palanish/ # main folder│ ├── __init__.py # essential even it is empty. With this file included, Python treats the folder as a package.│ └── color_extractor.py # main functions├── tests/ # optional│ └── test_basic.py├── README.md├── LICENSE├── setup.py└── requirements.txt First step, add the folder of palanish as previous listed and add the main function into the color_extractor.py. Later, within the __init__.py, add following line: 1from .color_extractor import get_color_palette, hex_sort Then, write the script of the setup.py as follows: 123456789101112131415161718192021222324from setuptools import setup, find_packagessetup( name='palanish', version='0.1.0', description='Palette extraction from Shanghai Animation images', author='Yufang Hao', author_email='your_email@example.com', url='https://github.com/envhyf/palanish', packages=find_packages(), install_requires=[ 'pillow', 'extcolors', 'matplotlib', 'colormap', 'pandas' ], classifiers=[ 'Programming Language :: Python :: 3', 'License :: OSI Approved :: MIT License', ], python_requires='&gt;=3.6',) As well as writing a small README.md, 12345678910111213141516171819202122# PalAniShExtract and visualize color palettes from classic Shanghai animations or any screenshots.## Installationpip install git+https://github.com/envhyf/palanish.git## Usage```pythonfrom palanish import get_color_palettecolors = get_color_palette(\"screenshot.jpg\")### 🔑 4. `LICENSE`You can use [MIT License](https://choosealicense.com/licenses/mit/) for open source:```txtMIT LicenseCopyright (c) 2024 Yufang HaoPermission is hereby granted, free of charge, ... The final step is uploading to Github 1234567cd palanishgit initgit add .git commit -m \"Initial commit\"git branch -M maingit remote add origin git@github.com:envhyf/palanish.gitgit push -u origin main After all steps done, anyone can install via: 1pip install git+https://github.com/envhyf/palanish.git 3. Web appWith","link":"/2023/06/01/29.%5BCode%5DMaking_my_own_Python_Package/"},{"title":"Create your first web app: Interactive data panel for visualizatoin correlations","text":"In this tutorial, I will demonstrate how to create a simple, interactive web app that visualizes correlations between variables. We will also walk you through the necessary environment setup and deployment steps. Environment SetupFirst, create a new conda environment and install the required packages: 1234conda create --name new_panel_env python=3.8conda activate new_panel_env## those package version are tested by myself with no conflictconda install -c conda-forge panel bokeh==2.4.3 holoviews==1.14.8 pandas==1.2.4 hvplot==0.8.2 Plotting Script123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import panel as pnimport holoviews as hvimport pandas as pdimport numpy as npimport hvplot.pandashv.extension('bokeh')# 1. Read the datadf_merge = pd.read_csv(\"./data/df_merge.csv\")factor_name = [c for c in df_merge.columns if 'factor_' in c]var_ = factor_name + [...]# Replace 'if tag bellow after columns definitiondf_var = df_merge[var_]df_var = df_var.replace('&lt;LD', np.nan)df_var = df_var.replace('&lt;DL', np.nan)df_var = df_var.dropna(axis=1, how='all')df_corr = df_var.corr()corr = df_corr # .abs()# Mask the upper triangle of the heatmapcorr.values[np.triu_indices_from(corr, 0)] = np.nantick_label = [...]# 2. Create heatmapheatmap = hv.HeatMap((corr.columns, corr.index, corr)) \\ .opts( tools=['hover'], height=1400, width=1400, fontsize=12, toolbar='above', colorbar=False, cmap='RdBu_r', invert_yaxis=True, xrotation=90, xlabel='', ylabel='', title='Correlation heatmap' )# Define tap stream with heatmap as sourcetap_xy = hv.streams.Tap(source=heatmap, x='As', y='K')# Calculate correlation plot based on tapdef tap_corrplot(x, y): # Drop missing values if there are any df_notnull = df_merge[['time_x', x, y]].dropna(how='any') scatter1 = hv.Scatter((df_notnull[x], df_notnull[y]), x, y, label='scatterplot') \\ .opts( tools=['hover'], height=700, width=900, fontsize=12, size=5, alpha=0.2, ylim=(df_notnull[y].min(), df_notnull[y].max()), color='#30a2da', framewise=True, ) timeseries1 = df_merge.hvplot(x='time_x', y=x, label=x, ylabel='Value').opts( height=700, width=900 ) timeseries2 = df_merge.hvplot(x='time_x', y=y, label=y, ylabel='Value').opts( height=700, width=900 ) return hv.Layout([scatter1, timeseries1 * timeseries2]).cols(1)tap_dmap = hv.DynamicMap(tap_corrplot, streams=[tap_xy])app_bar = pn.pane.Markdown(\"# Correlation Matrix Checker\")app = pn.Column( app_bar, pn.Spacer(height=10), pn.Row( heatmap, pn.Column( tap_dmap, ), ),)app.servable(title='Correlation matrix checker') Save the above code in a Python file named app.py. This app has a heatmap of correlations between variables, which serves as the backdrop for an interactive scatterplot and time series based on your selections. Running the Web AppFinally, to run your web app, open a terminal in the folder containing the app.py script and execute the following command: panel serve app.py --show Your web app will open in your default web browser with an interactive visualization panel showing a correlation matrix of the dataset, the scatter plots, and time series of two variables based on your selection. In summary, this tutorial showed you how to build a simple and interactive web app for visualizing correlations between variables. We covered the steps for setting up the appropriate environment, creating visualization scripts, and running the web application. You can use these techniques to analyze complex datasets and produce clear visualizations that aid in your understanding and interpretation of the data.","link":"/2023/08/09/29.%5BCode%5DWeb_App_Development_Interactive_Data_Panel/"},{"title":"WRF安装札记","text":"实验室的服务器的四块硬盘全坏，而未进行备份，不得不将以前的内容全部重装一次。Python部分有Anaconda的分发包，比较容易。但常用的WRF模型却研究了好久。 之前，服务器采用的是ifort编译器。重装后，系统中只有gcc, 版本也比较旧(4.1.2)。 WRF编译失败，我认为原因可能是gcc太旧了。 因而，我考虑升级gcc的版本，同时由于本人不是root用户，也不能借助yum install gcc的简单办法，只得step by step地做。 本文记录了自gcc升级至WRF-Chem成功安装的全部流程，作为今后类似工作的参考。 GCC的安装遵循GCC官网中的介绍，由于GCC的安装有赖于GMP, MPFR, MPC等库，而自己以前是逐个编译。官网并不推荐这种方法,而推荐采用如下所示的方案。 GCC文件夹内含有download_prerequisites的信息，可自行直接下载有关依赖资料库，避免自己选择库文件时的版本不一致。GCC官方网址 123456789101112#下载某个版本的gcc(尽量选择较新的版本)后tar xzf gcc-4.6.2.tar.gzcd gcc-4.6.2./contrib/download_prerequisites ## 按照依赖资料库cd ..mkdir objdir ## 编译信息储存的文件夹设为objdircd objdir$./../gcc-4.6.2/configure --prefix=$HOME/lib/gcc-4.6.2 \\ --enable-languages=c,c++,fortran,go ## $HOME/gcc-4.6.2即为安装位置makemake install cd .. gcc的编译过程比较慢，需要等待一会儿 安装完毕后，打开~/.bashrc, 在其中添加一行: export LD_LIBRARY_PATH=$HOME/gcc-4.9.4/lib64 利用gcc -v核查版本，查询是否安装成功 WRF与WPS有关依赖库的安装主要依赖的资料库包括mpich, netcdf, Jasper,libpng以及zlib。zlib和libpng间有先后关系，需先编译zlib。 NETCDF安装此处安装的是用以支持NetCDF格式文件创建、访问和共享的函式库。安装步骤如下: 12345678910# 在~/.bashrc中添加以下逐行export DIR=$HOME/libexport PATH=\"$DIR/netcdf/bin:$PATH\"export NETCDF=\"$DIR/netcdf\"export CC=gccexport CXX=g++export FC=gfortranexport FCFLAGS=-m64export F77=gfortranexport FFLAGS=-m64 下载NETCDF库文件，进行解压与编译: 123456789tar xzvf netcdf-4.1.3.tar.gz #or just .tar if no .gz presentcd netcdf-4.1.3./configure --prefix=$DIR/netcdf --disable-dap \\ --disable-netcdf-4 --disable-sharedmakemake installsetenv PATH $DIR/netcdf/bin:$PATHsetenv NETCDF $DIR/netcdfcd .. 若安装成功，terminal会出现“Congratulations, xxx“之类的输出。 MPICH安装MPICH用以支持WRF等数值模式的并行计算，安装步骤如下: 12345678tar xzvf mpich-3.0.4.tar.gz cd mpich-3.0.4./configure --prefix=$DIR/mpichmakemake installvim ~/.bashrcexport PATH=\"$DIR/mpich/bin:$PATH\"cd .. zlib, libpng, Jasper安装zlib是WPS程序用以提取grib格式数据的必需函式库，安装步骤如下: 123456789vim ~/.bashrcexport LDFLAGS=-L/diks2/hyf/lib/grib2/libexport CPPFLAGS=-I/disk2/hyf/lib/grib2/includetar xzvf zlib-1.2.7.tar.gz cd zlib-1.2.7./configure --prefix=$DIR/grib2makemake installcd .. libpng同样是WPS程序用以提取grib格式数据的必需函式库，安装步骤如下: 123456tar xzvf libpng-1.2.50.tar.gz cd libpng-1.2.50./configure --prefix=$DIR/grib2makemake installcd .. Jasper同样是WPS程序用以提取grib格式数据的必需函式库，安装步骤如下: 123456tar xzvf jasper-1.900.1.tar.gzcd jasper-1.900./configure --prefix=$DIR/grib2makemake installcd .. 三者均安装在grib2这个文件夹下。 WRF和WPS的安装请注意, WRF及其前处理WPS工具应安装于同一文件夹下. WRF的安装12345## 解压WRFV3.8gunzip WRFV3.8.TAR.gztar -xf WRFV3.8.TARcd WRFV3mv ../chem . ## 将预先下载好的wrf-chem解压文件放到该文件夹下 开始编译,./configure.对编译器和并行方式进行选择，此处我采用的是gfortran编译器以及dmpr并行方式。 dmpr指的是分布式并行方式，与smpr共享内存型并性方式有所区别。一般将这两类并行方式称为OpenMP(OMP)和Message Passing Interface(MPI)。_ 键入:./compile em_real &gt;&amp; log.compile进行编译，等待…… 键入:./compile convert_emiss&gt;&amp; log.compile进行编译，等待…… 完成后, 利用ls ./main/*.exe进行检查，若出现五个 .exe 格式的可执行文件，打开chem文件夹，出现convert_emiss.exe, 说明安装成功。 WPS的安装WPS是用以整合静态地理数据和气象场初始条件、边界条件资料的工具，其安装较快。 123456789# 解压文件gunzip WPSV3.8.TAR.gztar -xf WPSV3.8.TARcd WPSvim ~/.bashrcexport JASPERLIB=$DIR/grib2/libexport JASPERINC=$DIR/grib2/include./configure./compile &gt;&amp; log.compile 若安装成功后，会出现geogrid.exe, ungrib.exe, metgrid.exe三个可执行文件。 参考资料1. GCC install 2. Compiling WRF 3. Install WRF on Ubuntu Server","link":"/2016/08/10/3.%5BModel%5DWRF%E5%AE%89%E8%A3%85%E6%9C%AD%E8%AE%B0/"},{"title":"PLotting FLEXPART trajectories in 3D terrain map","text":"This blog post demonstrates how to plot FLEXPART model trajectories on a 3D terrain map, integrating topography and transport data for enhanced visualization. The workflow includes: Loading terrain data Plotting air mass trajectories in 3D Creating illustrative particle dispersion effects 1. Load and Visualize Terrain Data123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dimport cartopy.crs as ccrsimport cartopy.feature as cfeaturefrom cartopy.io import shapereaderfrom netCDF4 import Dataset# load the terrain fileetopo_file = \"./../../../../../4.Punjab_fire/Analysis_notebook/ETOPO_2022_v1_60s_N90W180_bed.nc\" # Replace with your local ETOPO1 filenc = Dataset(etopo_file)# Extract data from ETOPO1lon = nc.variables['lon'][:]lat = nc.variables['lat'][:]elevation = nc.variables['z'][:]# Mask the ocean by setting all elevation &lt;= 0 (sea level or below) to NaNelevation_masked = np.where(elevation &lt;= 0, np.nan, elevation)# Extract the topo data to the area of interestlon_min, lon_max = 60, 89lat_min, lat_max = 22, 36lon_indices = np.where((lon &gt;= lon_min) &amp; (lon &lt;= lon_max))[0]lat_indices = np.where((lat &gt;= lat_min) &amp; (lat &lt;= lat_max))[0]subset_lon = lon[lon_indices]subset_lat = lat[lat_indices]subset_elevation = elevation_masked[np.ix_(lat_indices, lon_indices)]lon_grid, lat_grid = np.meshgrid(subset_lon,subset_lat)# Normalize elevation data for better 3D visualizationelevation_normalized = np.clip(subset_elevation/ 1000, 0, subset_elevation.max()/1000.0) # Convert to kmfrom matplotlib.colors import LinearSegmentedColormapimport shapefilefrom shapely.geometry import Polygonfrom shapely.geometry import mappingfrom mpl_toolkits.mplot3d.art3d import Line3DCollectionfrom scipy.interpolate import RegularGridInterpolatordef truncate_colormap(cmap, minval=0.2, maxval=1.0, n=100): new_cmap = LinearSegmentedColormap.from_list( f\"trunc({cmap.name},{minval:.2f},{maxval:.2f})\", cmap(np.linspace(minval, maxval, n)), ) return new_cmap# Create a truncated terrain colormap starting from greenterrain_truncated = truncate_colormap(plt.cm.terrain, minval=0.3, maxval=1.0)from matplotlib.colors import LinearSegmentedColormapimport shapefilefrom shapely.geometry import Polygonfrom shapely.geometry import mappingfrom mpl_toolkits.mplot3d.art3d import Line3DCollectionfrom scipy.interpolate import RegularGridInterpolatordef truncate_colormap(cmap, minval=0.2, maxval=1.0, n=100): new_cmap = LinearSegmentedColormap.from_list( f\"trunc({cmap.name},{minval:.2f},{maxval:.2f})\", cmap(np.linspace(minval, maxval, n)), ) return new_cmap# Create a truncated terrain colormap starting from greenterrain_truncated = truncate_colormap(plt.cm.terrain, minval=0.3, maxval=1.0) 2. Load and Plot FLEXPART TrajectoriesIn trajectory mode, the output txt format information can be seen herehttps://confluence.ecmwf.int/display/METV/FLEXPART+output Column Number Name Unit Description 1 time s The elapsed time in seconds since the middle point of the release interval 2 meanLon degrees Mean longitude position for all the particles 3 meanLat degrees Mean latitude position for all the particles 4 meanZ m Mean height for all the particles (above sea level) 5 meanTopo m Mean topography underlying all the particles 6 meanPBL m Mean PBL (Planetary Boundary Layer) height for all the particles (above ground level) 7 meanTropo m Mean tropopause height at the positions of particles (above sea level) 8 meanPv PVU Mean potential vorticity for all the particles 9 rmsHBefore km Total horizontal RMS (root mean square) distance before clustering 10 rmsHAfter km Total horizontal RMS distance after clustering 11 rmsVBefore m Total vertical RMS distance before clustering 12 rmsVAfter m Total vertical RMS distance after clustering 13 pblFract % Fraction of particles in the PBL 14 pv2Fract % Fraction of particles with PV &lt; 2 PVU 15 tropoFract % Fraction of particles within the troposphere 16* clLon_N degrees Mean longitude position for all the particles in cluster N 17* clLat_N degrees Mean latitude position for all the particles in cluster N 18* clZ_N m Mean height for all the particles in cluster N (above sea level) 19* clFract_N % Fraction of particles in cluster N (above sea level) 20* clRms_N km Total horizontal RMS distance in cluster N 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122input_file = \"./data/retroplume_raw/traj/clustered_ind/2018-03-30-example.txt\"with open(input_file, \"r\") as file: lines = file.readlines()[7:]data = [list(map(float, line.split())) for line in lines]df_traj = pd.DataFrame(data, columns=columns[:len(data[0])]) # Adjust column count based on the actual data length# 36 hr in totalkp_traj = df_traj[df_traj['receptor'] == 1.0].reset_index(drop = True)dl_traj = df_traj[df_traj['receptor'] == 2.0].reset_index(drop = True)trajectories = { \"KP\": { \"lon\": kp_traj['meanLon'], \"lat\": kp_traj['meanLat'], \"alt\": kp_traj['meanZ']/1000.0 + kp_traj['meanTopo']/1000.0, \"time_delta\": -1*(kp_traj['time'] - kp_traj['time'].iloc[0])/60/60.0, }, \"DL\": { \"lon\": dl_traj['meanLon'], \"lat\": dl_traj['meanLat'], \"alt\": dl_traj['meanZ']/1000.0 + dl_traj['meanTopo']/1000.0, 'time_delta':-1*(kp_traj['time'] - kp_traj['time'].iloc[0])/60/60.0 },}elevation_interpolator = RegularGridInterpolator( (lat, lon), elevation / 1000.0 # Convert elevation to km)def set_3d_background(ax,sel_df_bio): ax.scatter( 80.331871, 26.449923, 0.35, # Kanpur at ground level zdir=\"z\", marker=\"s\", facecolor='none', edgecolor=\"b\", s=40, alpha=1,lw = 2, label=\"Kanpur\",zorder = 3 ) ax.scatter( 77.069710, 28.679079, 0.53, # New Delhi at ground level zdir=\"z\", marker=\"s\", facecolor='none', edgecolor=\"r\", s=40, alpha=1,lw = 2, label=\"New Delhi\",zorder = 3 ) # Plot the real-world elevation data as the base map ax.plot_surface( lon_grid, lat_grid, subset_elevation/1000.0, cmap=plt.cm.gray,#terrain_truncated, rstride=10, cstride=10, edgecolor='none', alpha=0.15, ) # Customize grid appearance ax.xaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0) # Transparent X grid ax.yaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0) # Transparent Y grid ax.zaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0) # Transparent Z grid ax.set_xlim([lon_min,lon_max]) ax.set_ylim([lat_min, lat_max]) ax.set_xlabel(\"Longitude\") ax.set_ylabel(\"Latitude\") ax.set_zlabel(\"Altitude (km)\")def plot_individual_day_traj(ax,traj):# colors = ['#97ff80','#679b4e','#ffe64c','#cd4040','purple','k']# time_categories = [0, 12,24, 48, 72, 96, 120] # 5 days (0-24, 24-48, etc.)# for i in range(len(time_categories) - 1):# mask = (traj[\"time_delta\"] &gt;= time_categories[i]) &amp; (traj[\"time_delta\"] &lt; time_categories[i + 1])# ax.plot(# np.array(traj[\"lon\"])[mask],# np.array(traj[\"lat\"])[mask],# np.array(traj[\"alt\"])[mask],# color=colors[i],# linewidth=3.5,# label=f\"&lt;{time_categories[i+1]} hr\",# ) norm = mcolors.Normalize(vmin=0, vmax=72) # Assuming time_delta ranges from 0 to 48 hours cmap = cm.get_cmap('Spectral_r') # Choose a continuous colormap # Plot trajectories with continuous color representation for i in range(len(traj[\"time_delta\"]) - 1): ax.plot( traj[\"lon\"][i:i+2], traj[\"lat\"][i:i+2], traj[\"alt\"][i:i+2], color=cmap(norm(traj[\"time_delta\"][i])), # Map time_delta to color linewidth=2.5,zorder =15, )# Create a 3D plotfig = plt.figure(figsize=(6, 5))norm = mcolors.Normalize(vmin=0, vmax=72) # Assuming time_delta ranges from 0 to 48 hourscmap = cm.get_cmap('Spectral_r') # Choose a continuous colormapax = fig.add_subplot(111, projection='3d')plot_individual_day_traj(ax, trajectories['DL'])set_3d_background_dl(ax,sel_df_bio)ax.set_zlim([0, 7])ax.set_title('Delhi', loc = 'left', fontweight = 'bold',y=0.85)ax.legend(ncol = 3, loc = (0.55, 0.45), fontsize = 8, frameon=True)ax.view_init(elev=21, azim=-110) # plt.show() 3.Particle Dispersion Illustration1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import numpy as npdef plot_individual_day_traj_particles(ax, traj, n_particles=10, spread=0.05,initial_spread=0.05, final_spread=1): \"\"\" Plot trajectory as particles instead of lines by generating points around the trajectory. Parameters: - ax: Matplotlib 3D axis. - traj: Dictionary with trajectory data (lon, lat, alt, time_delta). - n_particles: Number of particles to simulate per trajectory step. - spread: Standard deviation for particle dispersion around each step. \"\"\" norm = mcolors.Normalize(vmin=0, vmax=72) # Normalize time_delta cmap = cm.get_cmap('Spectral_r') # Colormap for time # Loop through each trajectory point for i in range(len(traj[\"time_delta\"])): current_spread = np.interp( traj[\"time_delta\"][i], [0, max(traj[\"time_delta\"])], [initial_spread, final_spread] ) # Generate random offsets for particles lon_offsets = np.random.normal(traj[\"lon\"][i], current_spread, n_particles) lat_offsets = np.random.normal(traj[\"lat\"][i], current_spread, n_particles) alt_offsets = np.random.normal(traj[\"alt\"][i], current_spread / 2, n_particles) # Less spread for altitude # Get color for the current time_delta color = cmap(norm(traj[\"time_delta\"][i])) # Plot the particles ax.scatter( lon_offsets, lat_offsets, alt_offsets, color=color, alpha=0.45, s=2, # Small points for particles )# Create a 3D plotfig = plt.figure(figsize=(6, 5))norm = mcolors.Normalize(vmin=0, vmax=72) # Assuming time_delta ranges from 0 to 48 hourscmap = cm.get_cmap('Spectral_r') # Choose a continuous colormapax = fig.add_subplot(111, projection='3d')plot_individual_day_traj_particles(ax, trajectories[\"DL\"], n_particles=200, spread=0.25)set_3d_background_dl(ax,df_pun)ax.set_zlim([0, 7])ax.set_title('Delhi', loc = 'left', fontweight = 'bold',y=0.85)# ax.legend(ncol = 3, loc = (0.3, 0.7), fontsize = 8, frameon=True)ax.view_init(elev=17, azim=-110) # sm = cm.ScalarMappable(cmap=cmap, norm=norm)axcolor = fig.add_axes([0.275, 0.6, 0.15, 0.015]) # Custom position and size for the colorbar axiscbar = plt.colorbar(sm, cax=axcolor, orientation='horizontal') # Create horizontal colorbarcbar.set_label('Air Mass Transport Time (hr)', labelpad=-40, fontsize=9) plt.savefig(\"./Backward_trajectories_example_20181125_pollution_cases_particle_simulation_version.png\", dpi =400)plt.show()","link":"/2024/10/05/30.FLEXPART-trajectories-in-3D-terrain-map/"},{"title":"OrbiTrack Dev Log 1: Parsing Orbitrap MS1 Data","text":"The ultra-high resolution of Orbitrap mass spectrometry enables an unprecedented level of chemical detail. In my current research, we employ a direct infusion–Orbitrap approach for non-targeted analysis. However, due to the lack of dedicated tools tailored for this application, I developed a new software package called OrbiTrack. It supports two key workflows: (1) Guiding ion fitting in EESI-TOF using high-resolution chemical information acquired at 120k resolution on the Orbitrap. (2) Directly utilizing non-targeted Orbitrap data by integrating full time-series MS1 outputs for real-time or online analysis. This blog series will present the OrbiTrack framework step by step, explaining the underlying principles and sharing the original scripts for each component. Feedback and suggestions from the community are most welcome. In this first post, I will introduce Part 1: loading Orbitrap raw MS1 data by user-defined time period. Raw data reading1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980## Module 1. MS readingdef get_last_retention_time(file_path): \"\"\" Function to read a file and extract the last retention time. Args: - file_path (str): Path to the file. Returns: - last_retention_time (float): The last retention time found in the file. \"\"\" last_retention_time = None # Initialize as None with open(file_path, 'r') as f: for line in f: if line.startswith(\"I\"): # Look for the lines with retention time info parts = line.strip().split('\\t')[1:] if len(parts) == 2: key, value = parts if key == \"RetTime\": last_retention_time = float(value) # Update with the latest retention time return last_retention_timedef get_retention_time_indices(file_path, start_retention_time, end_retention_time): retention_time_indices = [] current_segment_index = -1 # Initialize segment index retention_time_value = [] with open(file_path, 'r') as f: for line in f: if line.startswith(\"S\"): # Check for segment start current_segment_index += 1 # Increment segment index elif line.startswith(\"I\"): parts = line.strip().split('\\t')[1:] if len(parts) == 2: key, value = parts if key == \"RetTime\": retention_time = float(value) if start_retention_time &lt;= retention_time &lt;= end_retention_time: retention_time_indices.append(current_segment_index) retention_time_value.append(retention_time) return retention_time_indices,retention_time_valuedef read_ms1_segments_between_retention_times(file_path, retention_time_indices): segments = [] current_segment = [] current_segment_index = -1 with open(file_path, 'r') as f: for line in f: if line.startswith(\"S\"): current_segment_index += 1 if current_segment_index in retention_time_indices: if current_segment: segments.append(current_segment) current_segment = [] elif line.strip() and current_segment_index in retention_time_indices: current_segment.append(line.strip().split()) if current_segment: segments.append(current_segment) return segmentsdef save_segments_as_dataframes(segments,thres): dataframes = [] for idx, segment in enumerate(segments): retention_time = float(segment[0][2]) df = pd.DataFrame(segment[3:], columns=[\"m/z\", \"intensity\", \"unknown1\", \"unknown2\"]) # Convert columns to numeric (excluding the first two columns) df.iloc[:, 2:] = df.iloc[:, 2:].apply(pd.to_numeric) # Keep only the first two columns df = df.iloc[:, :2] df['m/z'] = df['m/z'].astype(float) df['intensity'] = df['intensity'].astype(float) df = df[df['intensity']&gt;thres] df = df.reset_index(drop = True) df['rt'] = retention_time # Save the DataFrame dataframes.append(df) return dataframesdef merge_segments(dataframes): merged_df = pd.concat(dataframes, ignore_index=True) return merged_df 2. Data inputFor TOF peak list fitting, we incorporate the raw TOF spectrum output from Tofware. This dataset is used to: Evaluate the fitting accuracy of Orbitrap-assigned ions, and Supplement ions that may be missing or undetectable in the Orbitrap measurement. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ========== 1. INPUT FILE ==========file_path = \"./../../../../../Africa/Angola/Data/Raw/Orbitrap/Angola_mixed_samples_20250620020218.ms1\"# ========== 2. Orbitrap Real Time Series ==========end_time = '2025-06-20 16:40:00' # Define end time for the time seriesstart_time = pd.to_datetime(end_time) - pd.Timedelta(minutes=get_last_retention_time(file_path)) # Calculate start time based on the file retention time# ========== 3. Pre-filtering Threshold for Noisy Ions ==========pre_filtering_threshold = 1000 # Default threshold set to 1000 to filter out noisy ions# ========== 4. Processing Mode Selection ==========# Options: \"full_ts\" for full time series or \"subset\" for a specific peak list subset# mode = 'full_ts' # Use full time series mode# start_retention_time, end_retention_time = 0, get_last_retention_time(file_path) # Set for full time seriesmode = 'sub_set'start_retention_time, end_retention_time = 3.51, 13#get_last_retention_time(file_path) # Define subset range (retention times of the place we are interested)# ========== 5. Parameters for M/Z Calibration ==========ideal_calibrants_mass = [ 172.88347, 322.77771, 472.67196, 108.9638, 166.9163] # 5 calibrants for m/z calibrationcalibration_tolerance_ppm = 20 # Set calibration tolerance to 20 ppmoverlapped_scan_option = 1 # Handle overlapping regions in neighboring segment scans (1 to enable handling)# ========== 6. Parameters for Filtering and Clustering ==========# Pre-filtering based on \"Air-beam\" (Na2I range)Na2I_lower_bound, Na2I_upper_bound = 0, 50 * 1E6 # Filter to remove bad air-beam ions# Data size threshold for KDE clustering chunkdata_size_threshold = 500 # Minimum data size for performing KDE clustering# KDE Parameters (for clustering noisy ions)kde_bandwidth = 0.0005 # KDE bandwidth for peak density estimationkde_percentile = 5 # Remove ions below the 5th percentile of density score (to filter out rare/noisy ions)# DBSCAN Clustering Parameterseps_value = 0.5 # Epsilon value for DBSCAN (maximum clustering distance in ppm)clustering_thre = 0.025 # Retain clusters appearing more than 2.5% of Na2I showing frequency# ========== 7. EESI-TOF reference ==========# tof_spectrum = pd.read_excel(\"./../Input/EESI-TOF_reference/EESI_LTOF_raw_curve_Punjab.xlsx\")# tof_add_ion_threshold = 50 # 50 ppm means there is no Orbitrap ions within 50 ppm window of a EESI-TOF identifed peak apex# inorganic_ion_file = './../Input/EESI-TOF_reference/EESI_TOF_common_inorganic_ions.csv'# tof_peak_list_template = './../Input/EESI-TOF_reference/ToFware_peak_list_template.txt'# ========== 8. Peak List Output and refining parameters ==========peak_list_prefix = './output/ToF_peak_list/20250720_Angola_Orbitrap+TOF_peak_list'refining_distance,refining_ratio = 25, 4 3. Reading data within user-defined time period123456retention_time_indices,rt_values = get_retention_time_indices(file_path, start_retention_time, end_retention_time)segments = read_ms1_segments_between_retention_times(file_path, retention_time_indices)df_segments = save_segments_as_dataframes(segments, pre_filtering_threshold)merged_spectrum = merge_segments(df_segments)merged_spectrum = merged_spectrum.sort_values(by = 'm/z', ascending=True)merged_spectrum = merged_spectrum.reset_index(drop=True) The number of full scans read, as indicated by the above parameters, will be important for subsequent analysis. 12345678tolerance_ppm = 20 ## need to re-adjust based on the actual datapeak_results = {}masses = [172.88347, 322.77771, 472.67196,622.566700,108.9638,166.9163]#for mass in masses: peaks = find_peak_near_mass(merged_spectrum, mass, tolerance_ppm) peak_results[mass] = peaksshowing_freq = len(peak_results[masses[0]]) print (showing_freq) We can also evaluate how closely the measured m/z values of calibrants match their theoretical values before applying calibration. This step is useful for pre-defining the tolerance_ppm parameter. 1234567891011121314151617181920212223242526subplot_titles = ['Na2I', 'Na3I2', 'Na4I3','Na2[15]NO3', 'Na3[34]SO4']masses = [172.88347, 322.77771, 472.67196,108.9638,166.9163][0:] colors = ['#447f44', '#cd4040', '#636efa', '#ddc927', '#9271dd','#2d85cc'] subplot_titles = []for i in range(len(masses)): ppm_differences = calculate_ppm(np.array(peak_results[masses[i]]['m/z']), masses[i]) mean_ppm = np.mean(ppm_differences) subplot_titles.append(f\"{['Na2I', 'Na3I2', 'Na4I3', 'Na2[15]NO3', 'Na3[34]SO4'][i]}: {mean_ppm:.2f} ppm\")fig = make_subplots(rows=1, cols=5, subplot_titles=subplot_titles)for i in range(len(masses)): fig.add_trace( go.Histogram(x=peak_results[masses[i]]['m/z'], marker=dict(color=colors[i % len(colors)])), row=1, col=i+1 ) fig.add_vline(x=masses[i], line_width=2, line_dash=\"dash\", line_color=\"grey\", row=1, col=i+1) fig.update_xaxes(title_text=\"m/z\", showline=True, linecolor='black', range=[masses[i]-0.0025, masses[i]+0.0025], col=i+1, row=1) fig.update_yaxes(title_text=\"Count\", showline=True, linecolor='black', col=i+1, row=1)fig.update_layout( height=400, width=1500, title_text=\"Calibrant Raw M/Z Distribution\", plot_bgcolor='white', showlegend=False)fig.show()","link":"/2024/10/21/32.%5BMS%5DOrbiTrack_Dev_Log_1_Parsing_Orbitrap_data/"},{"title":"读书笔记-张笑宇文明三部曲","text":"《产业与文明》 第一章总结主题：技术改变社会并非直接线性发生，而是必须通过“漏斗—喇叭”模型的检验——即产业化、商业化检验，才能真正改变世界。 技术本身无法自动改变社会，必须经过“商业化/产业化”漏斗筛选 事实支撑： 希罗发明汽转球、自动门、自动售货机等超前机械，但因社会缺乏商业需求与产业应用，仅成为神庙的奇技异巧，无广泛应用。 巴努·穆萨三兄弟、加扎利发明自动演奏装置、大象钟等复杂机械，但依附神职机构或暴力集团，未能推动生产革命。 古代社会资源集中在暴力集团与宗教集团手中，缺乏广泛消费能力与高劳动力成本，因此新技术难以普及应用。 社会经济结构决定了技术能否应用与扩散 事实支撑： 古罗马、伊斯兰社会中大量奴隶劳动使起重机等机械发明缺乏实际应用场景。 中国瓷窑（斜上排气）重视节省燃料而忽略工人福祉，反映工人劳动力极其廉价。 英国瓷窑（直上排气）热量浪费大，但劳动力价值较高，工业革命前夜英格兰已出现高工资经济模式。 产业环境和经济激励，是技术发明能否大规模应用的关键 事实支撑： 蒸汽机的普及不是因为科学理论本身，而是因为煤炭贵、人力贵，蒸汽机能节省成本，符合市场需求。 工业革命真正启动，不靠单一科学发现，而靠大量日常需求叠加，激励工程师不断改良、微创新。 纽卡门、瓦特能成功，是因为他们改进技术能在当时获得实际盈利，而非仅依赖皇室、学院赞助。 科技推动社会演化，必须警惕“理性主义的自负” 事实支撑： 儒家社会、古希腊社会都存在轻视实用技术的文化，但真正阻碍技术普及的，是经济与社会结构。 20世纪苏联OGAS计划试图通过计算机计划经济改变社会，但技术本身无法拯救僵化体制，OGAS失败。 纳粹利用孟德尔遗传学发展优生学，说明技术本身价值中性，如何应用取决于社会环境。 自由市场是扩大“漏斗”宽度、促进新技术存活与发展的最佳机制 事实支撑： 17世纪英国通过自由贸易、国债市场建立信用秩序，支撑了资本市场与高科技产业的繁荣。 英国从小规模商贸正增长开始，逐渐积累出可以支撑高工资、高投资回报率的产业环境，为工业革命打下基础。 总体结论科技创新在复杂社会中无法简单直接推动变革，只有经过产业化、商业化（漏斗检验），符合社会实际需求，才能从无数创新中脱颖而出，引发“喇叭效应”，大规模扩展、彻底改变社会。 第二章总结主题：蒸汽机革命并非偶然爆发，而是由商业繁荣、生活细节需求变化、自然资源优势和社会激励机制共同驱动，在微小生活变化中积累起巨大技术突破动力。 燃料危机是蒸汽机革命的生活源头 事实支撑： 16世纪，伦敦人口由5.5万人增至20万人，燃料需求激增。 木柴供应因运输限制（泰晤士河、水陆运力瓶颈）而不足，木柴价格飙升，木炭甚至比煤炭贵两倍。 煤炭因能量密度高、运输效率高，逐渐替代木柴成为主要燃料（1550年以后煤价优势显著）。 煤炭供给的地理偶然性，奠定了英国的能源基础 事实支撑： 纽卡斯尔地区煤层裸露地表，容易开采。 16世纪中后期，纽卡斯尔煤产量从15000吨激增到1625年的40万吨。 玛丽一世取消贵族对煤矿的垄断，促进资本流入和煤炭大规模开发。 排水难题催生对抽水技术的刚需，直接导致蒸汽机出现 事实支撑： 煤矿采深遇到严重矿井透水问题（如河水倒灌事故）。 传统修建排水渠方法逐渐失效，必须寻找新技术。 托马斯·纽卡门（牧师兼铁匠）1712年发明纽卡门蒸汽机，首次通过蒸汽驱动活塞抽水，解决了矿井排水难题。 科技创新需要金钱激励，而不是单靠皇家学会或贵族赞助 事实支撑： 皇家学会虽提出矿井排水需要科技，但未能实际解决问题。 煤老板出于实际利益需求，愿意为蒸汽机支付高额费用，形成稳定商业模式（按煤矿产能收取长期使用费用）。 1733年英国已有100台蒸汽机，1800年增至2500台。 日常生活需求（如烹饪）驱动了材料革命，为蒸汽机改进奠基 事实支撑： 煤炭火力强，伦敦家庭必须更换更耐用、更轻薄的锅具。 亚伯拉罕·达比1709年成功用焦炭炼出灰铸铁，量产耐用锅具。 达比家族四代推动了从铸铁锅→工业铸铁材料→铁路、桥梁建设的材料革命。 瓦特蒸汽机的突破源于能源效率革命与材料进步的积累 事实支撑： 瓦特1765年发明分离式冷凝器，蒸汽机煤耗降低50%。 依赖达比焦炭炼铁技术，才能制造高效耐热气缸。 瓦特式蒸汽机1780年代起普及，成为动力源，不再局限于抽水，扩展到制造业、纺织业。 总体结论蒸汽机革命不是单一技术天才的产物，而是由能源需求、生活细节、材料革命、激励机制和地理优势多重因素在微观层面长期积累与选择下自然孕育的必然结果。","link":"/2024/10/11/31.%5BReading%5D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%BA%A7%E4%B8%9A%E4%B8%8E%E6%96%87%E6%98%8E/"},{"title":"OrbiTrack Dev Log 3: Ion Filtering and Clustering","text":"After m/z calibration, the next step is to align ions detected across different retention times that likely originate from the same chemical species but appear at slightly shifted m/z values. To ensure robust clustering, it’s important to first remove rare or noisy ions that could interfere with pattern recognition. Basic principleWe apply two types of filtering strategies onto Orbitrap MS1 data to isolate meaningful signals. Density-Based Filtering (KDE)Kernel Density Estimation (KDE) is used to evaluate the local density around each m/z value: Low-density points are likely to represent noise or isolated random signals and are filtered out using a user-defined percentile threshold (default: 5%). To balance accuracy and computational performance, the full m/z range is dynamically split into smaller chunks (e.g., 500 points), and KDE is applied to each chunk independently.(Note: Since KDE has a time complexity of O(n²), chunking significantly reduces the computational burden. See slides below for more details.) Ion Clustering (DBSCAN)The remaining m/z values after KDE filtering are grouped using the DBSCAN algorithm: DBSCAN identifies ion clusters based on spatial proximity, enabling detection of true ion peaks even with minor m/z drifts over time. After clustering, a frequency threshold (e.g., ≥2% of scans) is applied to exclude unstable or sporadic clusters. For example, if a stable calibrant ion appears consistently in 100 scans, meaningful signals are expected to recur in at least a small fraction (e.g., ≥2 scans). This step removes random spikes that lack temporal consistency. Note: The clustering_thre threshold is critical and should be carefully tuned based on the actual dataset and scanning strategy. It is highly recommended to validate the setting by checking the fitting quality in the post-analysis stage. Code library1def correct_mz_dynamically(merged_spectrum, ideal_calibrants_mass, tolerance_ppm, overlapped_scan): histor","link":"/2024/10/23/34.%5BMS%5DOrbiTrack_Dev_Log_3_Ion_Filtering_and_Clustering/"},{"title":"OrbiTrack Dev Log 2: Dynamic M/Z Calibration","text":"After reading m/z data from Orbitrap raw files, it is essential to perform m/z calibration based on known reference masses of internal calibrants. For long-term measurements, especially those spanning days or weeks, dynamic calibration is critical—m/z drift can occur over time, and calibration parameters from Day 1 may not hold by Day 5. The function below allows automated chunk-wise recalibration across retention time. Dynamic M/Z Calibration FunctionThe function below performs dynamic m/z calibration using known calibrant ions. It splits the full dataset by identifying where the calibrant signals are strongest (typically their peak retention times), and fits a linear correction model within each chunk. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495## Module 2 MS calibrationdef calculate_ppm(observed_mass, ideal_mass): ppm = ((observed_mass - ideal_mass) / ideal_mass) * 1e6 return ppmdef find_peak_near_mass_after_mzcal(merged_spectrum, target_mass, tolerance_ppm): # Calculate the tolerance in m/z tolerance = target_mass * tolerance_ppm / 1e6 # Find peaks within the tolerance range peaks_within_tolerance = merged_spectrum[ (merged_spectrum['corrected_m/z'] &gt;= target_mass - tolerance) &amp; (merged_spectrum['corrected_m/z'] &lt;= target_mass + tolerance) ] return peaks_within_tolerancedef find_peak_near_mass(merged_spectrum, target_mass, tolerance_ppm): # Calculate the tolerance in m/z tolerance = target_mass * tolerance_ppm / 1e6 # Find peaks within the tolerance range peaks_within_tolerance = merged_spectrum[ (merged_spectrum['m/z'] &gt;= target_mass - tolerance) &amp; (merged_spectrum['m/z'] &lt;= target_mass + tolerance) ] return peaks_within_tolerancedef find_calibrant_matches(merged_spectrum, ideal_calibrants_mass, tolerance_ppm): calibrant_matches = [] for ideal_mass in ideal_calibrants_mass: tolerance = ideal_mass * tolerance_ppm / 1e6 # Tolerance based on ppm peaks_within_tolerance = merged_spectrum[ (merged_spectrum['m/z'] &gt;= ideal_mass - tolerance) &amp; (merged_spectrum['m/z'] &lt;= ideal_mass + tolerance) ] if not peaks_within_tolerance.empty: # Store the actual masses and their RTs for _, peak in peaks_within_tolerance.iterrows(): calibrant_matches.append((ideal_mass, peak['m/z'], peak['rt'])) return calibrant_matchesdef find_rt_for_maximum_mz(merged_spectrum, ideal_calibrants_mass, tolerance_ppm,overlapped_scan): calibrant_matches = find_calibrant_matches(merged_spectrum, ideal_calibrants_mass, tolerance_ppm) if not calibrant_matches: print('No calibrants found in the spectrum.') return [] max_ideal_mass = max(ideal_calibrants_mass) max_calibrant_matches = [(ideal_mass, actual_mass, rt) for ideal_mass, actual_mass, rt in calibrant_matches if ideal_mass == max_ideal_mass] max_calibrant_rts = sorted([match[2] for match in max_calibrant_matches]) # For overlapped mz scan, choose the second scan's rts (be careful!) if overlapped_scan == 1: max_calibrant_rts = max_calibrant_rts[1::2] return sorted(max_calibrant_rts) # Sort the RTsdef correct_mz_dynamically(merged_spectrum, ideal_calibrants_mass, tolerance_ppm, overlapped_scan): merged_spectrum = merged_spectrum.reset_index(drop = True) max_calibrant_rts = find_rt_for_maximum_mz(merged_spectrum, ideal_calibrants_mass, tolerance_ppm, overlapped_scan) chunk_boundaries = list(sorted(([merged_spectrum['rt'].min()] + max_calibrant_rts + [merged_spectrum['rt'].max()]))) model_params = [] # processing for each chunk split by the rts of the maximum m/z for i in range(len(chunk_boundaries) - 1): rt_start = chunk_boundaries[i] rt_end = chunk_boundaries[i + 1] chunk = merged_spectrum[(merged_spectrum['rt'] &gt;= rt_start) &amp; (merged_spectrum['rt'] &lt; rt_end)] actual_masses = {} for ideal_mass in ideal_calibrants_mass: tolerance = ideal_mass * tolerance_ppm / 1e6 peaks_within_chunk = chunk[ (chunk['m/z'] &gt;= ideal_mass - tolerance) &amp; (chunk['m/z'] &lt;= ideal_mass + tolerance) ] if not peaks_within_chunk.empty: actual_masses[ideal_mass] = np.mean(peaks_within_chunk['m/z']) # Step 5: Apply calibration if at least one calibrant is found if actual_masses: ideal_actual_pairs = np.array([(actual_masses[m], m) for m in actual_masses]) weights = np.abs(1 - (ideal_actual_pairs[:, 0] / ideal_actual_pairs[:, 1])) # Weight by proximity to ideal mass model = LinearRegression() model.fit(ideal_actual_pairs[:, 0].reshape(-1, 1), ideal_actual_pairs[:, 1], sample_weight=weights) # Apply the calibration to the current chunk chunk_rows = (merged_spectrum['rt'] &gt;= rt_start) &amp; (merged_spectrum['rt'] &lt; rt_end) merged_spectrum.loc[chunk_rows, 'corrected_m/z'] = model.predict(merged_spectrum.loc[chunk_rows, 'm/z'].values.reshape(-1, 1)) slope = model.coef_[0] intercept = model.intercept_ model_params.append({'rt_start': rt_start, 'rt_end': rt_end, 'slope': slope, 'intercept': intercept}) return merged_spectrum, model_params Execution and Output ParametersTo address potential m/z drift over long measurement periods, I developed a dynamic calibration procedure that allows for chunk-wise m/z correction using varying fitting parameters. This approach avoids the limitations of applying a single static equation to the entire dataset and ensures more accurate mass alignment throughout the run. A. Chunking by Retention Time of Maximum Calibrant The full time series is segmented into chunks based on the retention time (RT) of the maximum calibrant ion.Each chunk corresponds to a distinct time window, accounting for potential overlap to preserve peak continuity.Note: In our untargeted direct infusion study, the retention time refers to the time stamp of each specific MS1 scan. B. Calibrant-Based Correction Per Chunk For each time chunk: The actual m/z values of selected calibrant ions are extracted. A linear regression is performed between the measured m/z values and their corresponding ideal masses. This model is then used to recalibrate all ions in that chunk, shifting their m/z values to more accurate positions. Before the calibration 123456merged_spectrum_mzcal, model_params = correct_mz_dynamically( merged_spectrum, ideal_calibrants_mass, calibration_tolerance_ppm, # Predefined tolerance based on the typical mass error between actual and ideal calibrant values, I typically set to 20 ppm overlapped_scan_option) We then inspect the variation in calibration parameters (slope and intercept) across segments. 123456789101112131415161718slope_ = []intercept_ = []for c in model_params: slope_.append(c['slope']) intercept_.append(c['intercept'])fig = make_subplots(rows=1, cols=2)fig.add_trace(go.Scatter(x=np.arange(len(slope_)), y=slope_, mode='lines+markers'), row=1, col=1)fig.update_yaxes(title_text=\"Slope\", col=1, row=1)fig.add_trace(go.Scatter(x=np.arange(len(intercept_)), y=intercept_, mode='lines+markers'), row=1, col=2)fig.update_yaxes(title_text=\"Intercept\", col=2, row=1)fig.update_xaxes(title_text=\"Chunk\", showline=True, linecolor='black')fig.update_layout( height=400, width=900, title_text=\"Dynamic Fitting Parameters Across Retention Time\", plot_bgcolor='white', showlegend=False)fig.show() Check Calibration Accuracy on Known IonsWe visualize the distribution of m/z values for five reference ions before and after correction. This comparison confirms the improvement in accuracy. 123456789101112131415161718192021222324252627282930subplot_titles = ['Na2I', 'Na3I2', 'Na4I3','Na2[15]NO3', 'Na3[34]SO4']masses = [172.88347, 322.77771, 472.67196, 108.9638, 166.9163]colors = ['#447f44', '#cd4040', '#636efa', '#ddc927', '#9271dd']subplot_titles = []for i in range(len(masses)): ppm_differences = calculate_ppm(np.array(peak_results[masses[i]]['corrected_m/z']), masses[i]) mean_ppm = np.mean(ppm_differences) subplot_titles.append(f\"{['Na2I', 'Na3I2', 'Na4I3', 'Na2[15]NO3', 'Na3[34]SO4'][i]}: {mean_ppm:.2f} ppm\")fig = make_subplots(rows=1, cols=5, subplot_titles=subplot_titles)for i in range(len(masses)): fig.add_trace( go.Histogram(x=peak_results[masses[i]]['corrected_m/z'], marker=dict(color='blue')), row=1, col=i+1 ) fig.add_trace( go.Histogram(x=peak_results[masses[i]]['m/z'], marker=dict(color='red', opacity=0.75)), row=1, col=i+1 ) fig.add_vline(x=masses[i], line_width=2, line_dash=\"dash\", line_color=\"grey\", row=1, col=i+1) fig.update_xaxes(title_text=\"m/z\", range=[masses[i]-0.0025, masses[i]+0.0025], col=i+1, row=1) fig.update_yaxes(title_text=\"Count\", col=i+1, row=1)fig.update_layout( height=400, width=1500, title_text=\"Calibrant Mass Distribution Before and After Calibration\", plot_bgcolor='white', showlegend=False)fig.show() Here is an example of the post-check on those calibrants as well as those important ions (analytes).","link":"/2024/10/22/33.%5BMS%5DOrbiTrack_Dev_Log_2_MZ_calibration/"},{"title":"OrbiTrack Dev Log Final: Formula Assignment","text":"In this post, I document the full formula assignment pipeline using MFAssignR, tailored for Orbitrap-MS data post-peak list processing. The steps include quality control filtering, noise estimation via Kendrick Mass Defect (KMD) plots, isotopic filtering, recalibrant inspection, and final formula assignment. 1. Load Input and Install Required Packages12345setwd(\"./output/ToF_peak_list/\")devtools::install(\"MFAssignR\")library(MFAssignR)input_name = './output/ToF_peak_list/20250731_Punjab_Delhi_Orbitrap+TOF_peak_list'Data &lt;- read.csv(paste0(input_name, '-raw_mz.csv'))[ , c('m.z', 'intensity')] 2. Reading note: How MFAssignR filter raw peaks and assign formulasNoise filteringMFassignR uses a Kendrick Mass Defect (KMD)-based noise filtering approach (KMDNoise) before formula assignment. It analyzes raw spectra to identify background regions free from analyte signals. By slicing along two KMD lines (default intercepts: 0.05 and 0.2), it estimates the baseline noise level. Peaks below a user-defined signal-to-noise (SN) threshold (e.g., 3–10× noise) are removed. This enhances data quality by filtering out low-intensity noise and multiply charged ions. Formula assignmentMFAssignR applies several non-optional quality assurance (QA) rules to screen out chemically invalid formulas during assignment. Below is a summary of key rules: 🔍 Fundamental Rules Rule Description Senior Rule(Kind &amp; Fiehn, 2007) Ensures molecular formulas follow known valency and bonding constraints. Useful for identifying feasible adduct or fragment ions. Nitrogen Rule For odd vs even nominal masses: odd → odd number of N atoms. Large Atom Rule Large atoms tend to fragment at weak bonds; used to predict fragmentation patterns. Max Hydrogen Rule Limits H count based on allowed bonding from other atoms. Prevents over-saturation. Max DBE Rule(Lobodin et al., 2012) Ensures formulas have chemically valid unsaturation: DBE = (2C + 2 + N − X − H)/2 3. Noise Estimation Using KMDKMDNoise isolates low-intensity regions via Kendrick Mass Defect linear slice filtering: 123456Noise &lt;- KMDNoise(Data)plot &lt;- Noise[[\"KMD\"]]plotKMDN &lt;- Noise[[\"Noise\"]]KMDNSNplot(Data, cut = KMDN * 3, mass = 301.1, window.x = 900, window.y = 25) Signal-to-noise (S/N) plot Spectrum after noise removal 4. Isotope Prescreening123Isotope &lt;- IsoFiltR(Data)Mono &lt;- Isotope[[\"Mono\"]]Iso &lt;- Isotope[[\"Iso\"]] 5. Initial CHO Formula Assignment1234Assign &lt;- MFAssignCHO(Mono, Iso, ionMode = \"pos\", lowMW =50, highMW = 1000, POEx= 0, Zx = 1, Mx = 2, Ex = 1, ppm_err = 3, H_Cmin = 0.3, HetCut = \"off\", NMScut = \"on\", SN = 1*KMDN) 6. Review Assignment Quality1234567Unambig1 &lt;- Assign[[\"Unambig\"]]Ambig1 &lt;- Assign[[\"Ambig\"]]Unassigned1 &lt;- Assign[[\"None\"]]MSAssign &lt;- Assign[[\"MSAssign\"]]Error &lt;- Assign[[\"Error\"]]MSgroups &lt;- Assign[[\"MSgroups\"]]VK &lt;- Assign[[\"VK\"]] 7. Identify Recalibrant Series and RecalibrateThis step uses high-confidence assigned ions (from Unambig1) to refine m/z accuracy via internal recalibration. 12345678910check &lt;- RecalList(Unambig1)Test &lt;- Recal(df = Unambig1, peaks = Mono, isopeaks = Iso, mode = \"pos\", SN = 2*KMDN, mzRange = 50, series1 = \"O5_Na_2\", series2 = \"O5_Na_3\", series3 = \"O6_Na_3\", series4 = \"O2_Na_3\", series5 = \"O3_Na_3\")Plot &lt;- Test[[\"Plot\"]]PlotMono2 &lt;- Test[[\"Mono\"]]Iso2 &lt;- Test[[\"Iso\"]]List &lt;- Test[[\"RecalList\"]] 8. Final Formula Assignment with Extended Elements1234Assign &lt;- MFAssign(Mono2, Iso2, ionMode = \"pos\", lowMW =50, highMW = 1000, POEx= 0, Zx = 1, Mx = 2, Ex = 0, Nx = 3, Sx = 3, ppm_err = 20, H_Cmin = 0.3, HetCut = \"off\", DeNovo = 300, NMScut = \"on\", SN = 0.25*KMDN) A summary of the parameters used in the MFAssign function: Parameter Value Meaning Mono2 [input] Dataframe of monoisotopic masses (from Recal step) Iso2 [input] Dataframe of isotopic masses (from Recal step) ionMode \"pos\" Specifies positive ionization mode lowMW 50 Lower limit of molecular mass to be assigned highMW 1000 Upper limit of molecular mass to be assigned POEx 0 Whether to allow odd-electron positive mode ions (0 = no) Zx 1 Charge state allowed in formula assignment Mx 2 Maximum number of sodium adducts (Na) allowed Ex 0 Amount of 13C isotopes allowed Nx 3 Maximum number of nitrogen atoms (14N) allowed Sx 3 Maximum number of sulfur atoms (32S) allowed ppm_err 20 Error tolerance for formula assignment in ppm H_Cmin 0.3 Minimum hydrogen-to-carbon (H/C) ratio HetCut \"off\" Disable high heteroatom QA filter DeNovo 300 Cutoff for de novo formula generation (masses above this value are not considered) NMScut \"on\" Enable nominal mass series QA check (Koch et al., 2007) SN 0.25*KMDN Signal-to-noise threshold for formula assignment, scaled by KMD-based noise estimate 9. Save Final Outputs1234567Unambig2 &lt;- Assign[[\"Unambig\"]]Ambig2 &lt;- Assign[[\"Ambig\"]]Unassigned2 &lt;- Assign[[\"None\"]]MSAssign &lt;- Assign[[\"MSAssign\"]]Error &lt;- Assign[[\"Error\"]]MSgroups &lt;- Assign[[\"MSgroups\"]]VK &lt;- Assign[[\"VK\"]] 123456input_name = './output/ToF_peak_list/20250731_Punjab_Orbitrap+TOF_peak_list'write.csv(Unambig2, file = paste0(input_name, '-assigned_mz.csv'))write.csv(Unassigned2,file = paste0(input_name, '-un-assigned_mz.csv'))write.csv(Ambig2, file = paste0(input_name, '-ambiguous_ion.csv'))write.csv(Iso2, file = paste0(input_name, '-isotope_mz.csv')) The assigned formula will be shown as: Reference Kind, T. &amp; Fiehn, O. (2007). Seven Golden Rules for heuristic filtering of molecular formulas obtained by accurate mass spectrometry. BMC Bioinformatics, 8, 105 Schum, S.K., Brown, L.E., &amp; Mazzoleni, L.R. (2020). MFAssignR: Molecular formula assignment software for ultrahigh resolution mass spectrometry analysis of environmental complex mixtures. Environmental Research, https://doi.org/10.1016/j.envres.2020.11011 MFAssignR github page, https://github.com/skschum/MFAssignR","link":"/2024/11/01/37.%5BMS%5DOrbiTrack_Dev_Log_5_Formula_Assignment/"},{"title":"OrbiTrack Dev Log 4: TOF missing peaks adding","text":"After m/z calibration, the next step is to align ions detected across different retention times that likely originate from the same chemical species but appear at slightly shifted m/z values. To ensure robust clustering, it’s important to first remove rare or noisy ions that could interfere with pattern recognition. We apply two types of filtering strategies to isolate meaningful signals: Frequency filtering using K-means to eliminate low-frequency clusters. Density-score filtering using Kernel Density Estimation (KDE) to adaptively remove low-density regions. This pre-filtering step reduces the data size and computational load while preserving the integrity of true signal clusters, as shown in the examples below. Dynamic M/Z Calibration FunctionThe function below performs dynamic m/z calibration using known calibrant ions. It splits the full dataset by identifying where the calibrant signals are strongest (typically their peak retention times), and fits a linear correction model within each chunk. 12def correct_mz_dynamically(merged_spectrum, ideal_calibrants_mass, tolerance_ppm, overlapped_scan): ... Execution and Output ParametersRun the calibration and extract linear model parameters from each retention time segment. 123456merged_spectrum_mzcal, model_params = correct_mz_dynamically( merged_spectrum, ideal_calibrants_mass, calibration_tolerance_ppm, overlapped_scan_option) We then inspect the variation in calibration parameters (slope and intercept) across segments. 123456789101112131415161718slope_ = []intercept_ = []for c in model_params: slope_.append(c['slope']) intercept_.append(c['intercept'])fig = make_subplots(rows=1, cols=2)fig.add_trace(go.Scatter(x=np.arange(len(slope_)), y=slope_, mode='lines+markers'), row=1, col=1)fig.update_yaxes(title_text=\"Slope\", col=1, row=1)fig.add_trace(go.Scatter(x=np.arange(len(intercept_)), y=intercept_, mode='lines+markers'), row=1, col=2)fig.update_yaxes(title_text=\"Intercept\", col=2, row=1)fig.update_xaxes(title_text=\"Chunk\", showline=True, linecolor='black')fig.update_layout( height=400, width=900, title_text=\"Dynamic Fitting Parameters Across Retention Time\", plot_bgcolor='white', showlegend=False)fig.show() Check Calibration Accuracy on Known IonsWe visualize the distribution of m/z values for five reference ions before and after correction. This comparison confirms the improvement in accuracy. 123456789101112131415161718192021222324252627282930subplot_titles = ['Na2I', 'Na3I2', 'Na4I3','Na2[15]NO3', 'Na3[34]SO4']masses = [172.88347, 322.77771, 472.67196, 108.9638, 166.9163]colors = ['#447f44', '#cd4040', '#636efa', '#ddc927', '#9271dd']subplot_titles = []for i in range(len(masses)): ppm_differences = calculate_ppm(np.array(peak_results[masses[i]]['corrected_m/z']), masses[i]) mean_ppm = np.mean(ppm_differences) subplot_titles.append(f\"{['Na2I', 'Na3I2', 'Na4I3', 'Na2[15]NO3', 'Na3[34]SO4'][i]}: {mean_ppm:.2f} ppm\")fig = make_subplots(rows=1, cols=5, subplot_titles=subplot_titles)for i in range(len(masses)): fig.add_trace( go.Histogram(x=peak_results[masses[i]]['corrected_m/z'], marker=dict(color='blue')), row=1, col=i+1 ) fig.add_trace( go.Histogram(x=peak_results[masses[i]]['m/z'], marker=dict(color='red', opacity=0.75)), row=1, col=i+1 ) fig.add_vline(x=masses[i], line_width=2, line_dash=\"dash\", line_color=\"grey\", row=1, col=i+1) fig.update_xaxes(title_text=\"m/z\", range=[masses[i]-0.0025, masses[i]+0.0025], col=i+1, row=1) fig.update_yaxes(title_text=\"Count\", col=i+1, row=1)fig.update_layout( height=400, width=1500, title_text=\"Calibrant Mass Distribution Before and After Calibration\", plot_bgcolor='white', showlegend=False)fig.show()","link":"/2024/10/24/35.%5BMS%5DOrbiTrack%20Dev%20Log%204_Enhancing_Peak_Coverage_via_TOF_Integration/"},{"title":"[WRF-Chem学习笔记①]排放预处理模块的安装与使用","text":"气象场和化学物质排放是大气化学模式的两项最重要的前驱资料。在实验之余，我将着力学习WRF-Chem的有关内容，并记录、发布。 按照WRF-Chem Tutorial中的说明，在Learning to Run WRF-Chem之前，我先学习How to Generate Emission for WRF-Chem 本文记录Exercise 1的实现过程，可供有兴趣的同学参考。 运行WRF和WRF-Chem的一个重要区别: 排放源清单的输入。Exercise 1__将会介绍建立WRF-Chem可读取的源清单文件的最基本方法(转置现有全球清单)，其核心工具为__PREP_CHEM_SOURCES。 安装过程 HDF5安装该程序的顺利运行需要依赖HDF5, NetCDF, zlib等标准库。之前在安装WRF-Chem时已经安装了NetCDF和zlib，这里只列出HDF5和szip的安装步骤。 12345678910111213#1. szip安装wget http://www.hdfgroup.org/ftp/lib-external/szip/2.1/src/szip-2.1.tar.gztar -xvzf szip-2.1.tar.gzcd szip-2.1./configure --prefix=/The_path/you/want/to/install/make &amp; make check &amp; make install#2. HDF5安装wget http://www.hdfgroup.org/ftp/HDF5/current/bin/linux-centos7-x86_64-gcc485/hdf5-1.8.17-linux-centos7-x86_64-gcc485-shared.tar.gz#解压并进入文件夹./configure --prefix=/disk2/hyf/lib/hdf5 --enable-fortran \\ --with-szlib=/szip/install/path\\ --with-zlib=/zlib/install/path ## 此处注意与后文prep_chem_source的编译器应对应上，否则lib中的文件无法编译成功make &amp; make check &amp; make install 排放预处理程序安装下载并解压后，进入/bin/build文件夹，修改与编译器对应的配置文件，如我将采用gfortran编译器编译，则vim include.mk.gfortran, 修改其中的NETCDF与HDF5库对应位置，按以下命令进行编译: 1`make OPT=gfortran CHEM=RADM_WRF_FIM` 其中OPT=对应编译器类别(i.e, intel, pgi,gfortran, …)，CHEM=对应特定化学机理，此处选择__RADM__机理。这将决定WRF-Chem读取源清单文件的化学物质分类格式。 值得注意的是，按上述操作，将在编译__edgar_emission.f90__过程出错，WRF论坛中也有用户提交了同样的报错，幸有大神回复并解决，解决方法是调整某些行的字符串的长度，可参考该网页进行修改。 修改完成后，再次编译，若出现Finished building === ../prep_chem_sources_RADM_WRF_FIM_.exe，说明编译成功!ヽ(✿ﾟ▽ﾟ)ノ PS: 安装完成后，编辑~/.bashrc 12345# 添加以下几行export HDF5=\"$DIR/hdf5\" # $DIR为所有库存放位置export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HDF5/libexport NETCDF=\"$DIR/netcdf\"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NETCDF/lib 案例 排放预处理程序安装按照Exercise 1中的input文件，替换prep_chem_src\\bin文件夹下原有的input文件，并按照自己的global_emission文件夹修改读取原始排放数据的路径。 运行$./prep_chem_sources_RADM_WRF_FIM_.exe 运行成功后，会出现五个输出文件，其中有三个后缀名为.bin的文件，分别为: 123WRF-TUTORIAL-T-2010-07-14-000000-g1-ab.bin ## 人为源排放清单WRF-TUTORIAL-T-2010-07-14-000000-g1-bb.bin ## 生物质燃烧清单WRF-TUTORIAL-T-2010-07-14-000000-g1-gocartBG.bin ## GOCART的全球背景化学场资料 将prep_chem_src\\bin文件夹中三个.bin后缀的输出文件移动到WRF-Chem文件夹中, 此处我在WRFV3\\test\\文件夹内新建em_ex1文件夹，内容复制同目录下的em_real。 运行./convert_emiss.exe 按照论坛上大家的观点，目前WRF-Chem 3.4版本以上会报错，参考1参考2。 若运行成功，则会生成三个排放源文件，其中 (1) __wrfchemi_d01__文件对应设计模拟区域内各类污染物的排放清单结果,包括CO, NH3, PM10, PM2.5及各类有机物(按照RACM机理分类)。(2) __wrffirechemi_d01__文件对应生物质燃烧排放情况.(3)__wrfchemi_gocart_bg_d01__文件对应二甲基硫(DMS), NO3, OH, H2O2的背景浓度分布。DMS单位为nM/L,其他物种单位为体积混合比(volume mixing ratio)。 依据上述的步骤 原始全球清单 → 模拟区域排放二进制文件 → WRF-Chem可读取的nc格式排放源文件完成了排放源资料的制作 初始场与边界场文件的生成此处链接同模拟区域、时间的met_em.xxx文件(由WPS模型完成)链接，运行./real.exe, 生成初始场和边界场文件,内部包括了化学物质和气象资料信息。 输出文件生成运行./wrf.exe，生成wrfout文件。下图为PM2.5的分布情形，可以看出如阿尔卑斯山、安纳陀利亚高原等高海拔地区浓度相对低。 附图: 参考曼彻斯特大学教程，绘制WRF-Chem运行流程如下: 此处需要运行./real.exe两次，目前我还没有弄明白。 参考资料 convert_emiss.exe","link":"/2016/09/04/4.%5BModel%5D%5BWRF-Chem%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%91%A0%5D%E6%8E%92%E6%94%BE%E9%A2%84%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"title":"Visualizing Chemical Structures 绘制化学物质的结构式","text":"For the purpose of visualizing chemical structures, I will now post a few method that can achive this goal. 1. ChemDrawChemDraw is a software that can be used to draw chemical structures (though commerical). 2. RDKitRDKit Python library can reproduce the structurs based on the smiles code. 12345678910111213141516171819202122232425from rdkit import Chemfrom rdkit.Chem import Draw# Define the correct Myosmine SMILESprecursor_smiles = \"C1CC(=NC1)C2=CN=CC=C2\" # Correct Myosmine (C9H10N2)precursor_mol = Chem.MolFromSmiles(precursor_smiles)# Define predicted fragment SMILESfragments_smiles = { \"79.0544\": \"C1CC(=NC1)\", # Benzene radical cation C6H6-H+ \"104.0495\": \"C1=CC=C(C=C1)C=[NH+]\", # Pyridine-benzyl cation \"106.0653\": \"C1=CC=C(C=N1)C=[NH+]\", # Methyl-pyridine \"132.0683\": \"CCC(=[NH+])C2=CN=CC=C2\", # Pyridine-imidazole \"147.0919\": \"C1CC(=NC1)C2=CN=CC=C2\" # Correct Myosmine (precursor)}# Convert SMILES to RDKit moleculesfragment_mols = {mz: Chem.MolFromSmiles(smiles) for mz, smiles in fragments_smiles.items()}# Draw precursor and fragmentsDraw.MolsToGridImage( [precursor_mol] + list(fragment_mols.values()), molsPerRow=3, subImgSize=(200, 200), legends=[\"Precursor\"] + list(fragments_smiles.keys())) Save the structures to SVG vector file. 12345678910111213141516from rdkit import Chemfrom rdkit.Chem import Draw# Define the moleculeprecursor_smiles = \"CC1=CN2C=CC=C2C(=N1)C\"precursor_mol = Chem.MolFromSmiles(precursor_smiles)# Generate SVG as a stringdrawer = Draw.MolDraw2DSVG(600, 600) # width, height in pixelsdrawer.DrawMolecule(precursor_mol)drawer.FinishDrawing()svg = drawer.GetDrawingText()# Save SVG to filewith open(\"C9H10N2_structure_pyrazine.svg\", \"w\") as f: f.write(svg) 3. SmileDrawerSmileDrawer is a lightweight JS library that renders chemical structures from SMILES strings in the browser. It supports canvas and SVG, with optional export and styling. I receommend this method with the nice looking and the interactive feature. Relevant link: Playground Simple example Online editor Two examples for pasting 3.1. Quick draw using Canvas12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"utf-8\" /&gt; &lt;meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\" /&gt; &lt;title&gt;Smiles Drawer Example&lt;/title&gt; &lt;meta name=\"description\" content=\"A minimal smiles drawer example.\" /&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" /&gt; &lt;link href=\"https://fonts.googleapis.com/css?family=Droid+Sans:400,700\" rel=\"stylesheet\" /&gt;&lt;/head&gt;&lt;body&gt; &lt;input id=\"example-input\" name=\"example-input\" /&gt; &lt;canvas id=\"example-canvas\" width=\"500\" height=\"500\"&gt;&lt;/canvas&gt; &lt;script src=\"https://unpkg.com/smiles-drawer@1.0.10/dist/smiles-drawer.min.js\"&gt;&lt;/script&gt; &lt;script&gt; let input = document.getElementById(\"example-input\"); let options = {}; // Initialize the drawer to draw to canvas let smilesDrawer = new SmilesDrawer.Drawer(options); // Alternatively, initialize the SVG drawer: // let svgDrawer = new SmilesDrawer.SvgDrawer(options); input.addEventListener(\"input\", function() { // Clean the input (remove unrecognized characters, such as spaces and tabs) and parse it SmilesDrawer.parse(input.value, function(tree) { // Draw to the canvas smilesDrawer.draw(tree, \"example-canvas\", \"light\", false); // Alternatively, draw to SVG: // svgDrawer.draw(tree, 'output-svg', 'dark', false); }); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.2 Exportable SVG with Save Button12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"utf-8\" /&gt; &lt;title&gt;SmilesDrawer SVG Example&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;input id=\"example-input\" name=\"example-input\" value=\"C1=CC=CC=C1\" /&gt; &lt;svg id=\"example-svg\" width=\"500\" height=\"500\" style=\"background:white; border:1px solid #ccc;\"&gt;&lt;/svg&gt; &lt;br&gt; &lt;button id=\"saveBtn\"&gt;💾 Save SVG&lt;/button&gt; &lt;script src=\"https://unpkg.com/smiles-drawer@2.0.1/dist/smiles-drawer.min.js\"&gt;&lt;/script&gt; &lt;script&gt; const input = document.getElementById(\"example-input\"); const svg = document.getElementById(\"example-svg\"); const svgDrawer = new SmilesDrawer.SvgDrawer({ width: 500, height: 500 }); input.addEventListener(\"input\", () =&gt; { while (svg.firstChild) svg.removeChild(svg.firstChild); // Clear old structure SmilesDrawer.parse(input.value, tree =&gt; { svgDrawer.draw(tree, svg, \"light\"); }, err =&gt; { console.error(\"SMILES parse error:\", err); }); }); // Initial draw input.dispatchEvent(new Event(\"input\")); // Save as SVG document.getElementById(\"saveBtn\").addEventListener(\"click\", () =&gt; { const serializer = new XMLSerializer(); const svgString = serializer.serializeToString(svg); const blob = new Blob([svgString], { type: \"image/svg+xml\" }); const url = URL.createObjectURL(blob); const a = document.createElement(\"a\"); a.href = url; a.download = \"molecule.svg\"; a.click(); URL.revokeObjectURL(url); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","link":"/2024/05/29/40.%5BMS%5DVisualizing-Chemical-Structures/"},{"title":"[WRF-Chem学习笔记②]仅含沙尘源的模拟练习","text":"本文是我学习官网练习的记录，其模拟区域为北非，中东以及欧洲部分地区，无嵌套设置。共分为5个练习，分别为: 初始场添加沙尘排放源的模拟 采用GOCART全球气溶胶排放清单的模拟 加入生物源排放的化学模拟(MEGAN引入) 加入气溶胶直接/间接辐射效应的模拟 WRF-Chem数值预报实验 下文记录Exercise 1的流程和注意事项 WPS运行12345678910111213141516-------cd /wrf/WPS/rm geogrid.log ungrib.log metgrid.log1. 地面静态资料数据生成cd geogridln -sf GEOGRID.TBL.ARW_CHEM GEOGRID.TBLcd .../geoerid.exe-------2. 气象资料读取./link_grib.csh /disk2/data/ncep/fnl_2015071*./ungrib.exe &gt;&amp; ungrib.log-------3. 数据整合mpirun -np 8 metgrid.exe------- 注意此处geogrid.exe输出结果会包含沙尘信息: Processing EROD Processing CLAYFRAC Processing SANDFRAC WRF运行在WRFV3/test文件夹下复制em_real文件夹，新建tutor文件夹。此处主要进行WRF运行过程中边界场和初始场文件的生成，chem_opt(化学选项) = 401， 表示仅考虑沙尘。 1234链接WPS的输出结果ln -svf /disk2/hyfmpirun -np 16 ./real.exempirun -np 16 ./wrf.exe 成功运行，会在rsl.out.xxxx出现 __wrf: SUCCESS COMPLETE WRF__字段。 绘制输出结果利用Python对输出文件进行读取和绘制图像，如下图中的温度: 参考资料 Exercise 1","link":"/2016/09/05/5.%5BModel%5D%5BWRF-Chem%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%91%A1%5D%E4%BB%85%E5%90%AB%E6%B2%99%E5%B0%98%E6%BA%90%E7%9A%84%E6%A8%A1%E6%8B%9F%E7%BB%83%E4%B9%A0/"},{"title":"From Formula to Function: Inferring Compound Class from MS1 + MS2","text":"In this post, I document the full formula assignment pipeline using MFAssignR, tailored for Orbitrap-MS data post-peak list processing. The steps include quality control filtering, noise estimation via Kendrick Mass Defect (KMD) plots, isotopic filtering, recalibrant inspection, and final formula assignment. Tool 1. MetFraghttps://msbi.ipb-halle.de/MetFrag/ Tool 2. CFM-IDhttps://msbi.ipb-halle.de/MetFrag/ Tool 3. SIRIUShttps://msbi.ipb-halle.de/MetFrag/ Simple code for results visualizationhttps://msbi.ipb-halle.de/MetFrag/ MS2 Analyzerhttps://chemrxiv.org/engage/chemrxiv/article-details/6492507524989702c2b082fc https://fiehnlab.ucdavis.edu/projects/ms2analyzer Polaris drug discoveryhttps://polarishub.io/competitions/asap-discovery/antiviral-drug-discovery-2025 To do list. Orbitrack tool make 4-5 tools as the blog webpage 开发笔记 Drawing structure as a blog page MS2结构分析","link":"/2025/04/01/41.%5BMS%5DFrom_Formula_to%20Function_Inferring_Compound_Class_from_MS1+MS2/"},{"title":"读书笔记-意大利文化简史","text":"《意大利文化简史》王军，王苏娜著，外研社，2009 知识点摘录与总结，配图除特别说明外，均来源于网络。 1. 古代文明1.1 古希腊文明希腊是西方文化的摇篮，直接影响西方世界的生活方式和思维方法，艺术文学的内容和表现形式均来源于此。 克里特文明 19世纪末，英国考古学家亚瑟·伊文斯爵士发现克诺萨斯遗址。 克里特岛位于希腊，小亚和埃及之间，岛上木材丰富，便于造船开展海上贸易，5天到达埃及。 米诺斯文明 (2000BC-1400BC)，名称源于古希腊神话克里特国王米诺斯的名字。克里特文明有两种文字，甲种线形文字和乙种线形文字，前者至今无人破译，后者即为古希腊文字。 迈锡尼文明 1400BC, 火山和海啸，克里特文明湮灭。居住在希腊半岛南部的迈锡尼人，距离仅100km，占领岛屿，继承该文明。 1600BC-1200BC 到达鼎盛，武士阶级，城市设有王城，主要从事农牧业。 贸易开始发展至黑海地区，入口城市特洛伊成为贸易阻碍，特洛伊战争的真正原因 （记载于荷马《伊利亚特》） 黑暗时代 1200BC, 希腊北部多利亚人（Dori）南下（希腊神话大力神赫拉克勒斯的后代，组织严密，英勇善战），势力曾远及克里特岛。其对斯巴达影响较大，使该城始终保持军事寡头政治统治的特征。 多利亚人文明程度相对较低，一般认为其入侵后数百年是古希腊文明的黑暗时代，但也可能是其将重要的冶铁技术引入希腊。 重要的相关神话 米诺斯国王的牛头人儿子米诺陶洛斯，被关闭在迷宫中。雅典每年需要贡献童男童女各7名。雅典国王爱琴（Ego，埃勾斯）儿子忒修斯（Tesco）在米诺斯女儿阿里阿德涅（Arianna）的帮助下诛杀米诺陶洛斯。 忒修斯和父亲约定，若杀死米诺陶洛斯返航将黑帆挂为白帆。忒修斯带阿里阿德涅返回雅典，半路将其遗弃。忒修斯忘记换白帆，埃勾斯误以为儿子已死，跳海自杀。爱琴海（Mar Egeo）的由来。 忒修斯走出迷宫的线团由代达洛斯（迷宫的设计者）提供。米诺斯国王将代达罗斯父子关押。两人用米拉制作翅膀，儿子伊卡洛斯（Icarus）心高气傲飞得太高，双翼融化溺毙。 忒修斯返航的船在雅典保留很久，不断用新木料替换更新，保持主体结构不变。在西方文化，保持主体，修改局部的行为因此被称为“忒修斯的船”（la nave di Teseo）。 《伊利亚特》取材自特洛伊战争。特洛伊小王子帕里斯掠走斯巴达国王墨涅拉俄斯妻子海伦（Elena）。墨涅拉俄斯寻求迈锡尼国王阿迦门农的支持，联合各希腊王国向特洛伊宣战，9年攻城不利。第10年，阿迦门农与阿喀琉斯发生激烈冲突（战利品分配不均），后者拒绝出战，希腊联军损失惨重。阿喀琉斯好友帕特洛克里斯披上前者战甲，被特洛伊国王长子赫克托耳杀死。好友殒命，阿喀琉斯斩杀赫克托耳，但惹恼了赫克托耳的保护神阿波罗。阿波罗射中阿喀琉斯脚踵，致其死亡。 阿喀琉斯之踵（Tallone di Achille）意为“致命的弱点”，未入冥河浸泡。 《奥德赛》另一部荷马史诗，讲述特洛伊战争后，英雄奥德修斯（Odissea）艰难返乡（10年）。奥德修斯是伊塔刻（Itaca）国王，木马计设计者。特洛伊的保护神们憎恨奥德修斯，在其返乡途中不断加害。独眼巨人波吕斐摩斯（海神波塞冬之子）-&gt; 风王艾奥罗斯 -&gt;海妖赛壬-&gt; 海怪斯库拉和卡律布狄斯-&gt;神女卡吕普索7年挽留。奥德修斯妻子珀涅罗珀（Penelope）苦等丈夫20年，拒绝无数的求婚者。在丈夫杳无音信的情况下，她被迫要在求婚者（也是觊觎王位者）中选择一位新的配偶。奥德修斯恰巧返回，乔装成乞丐，混入王宫，向妻子的求婚者们提出挑战，比试箭法。战胜所有的对手，与妻儿相认，保住财产和王位。 1.2 希腊城邦国和希腊殖民地因半岛多山，交通不便，各人类聚居区相对独立，并逐渐形成独立邦国，且几乎全部位于沿海。各城邦及其周边农村构成一个”Polis”，拥有独立政府，法律，军队。城邦内公共事务被称为“politicus”。各国政治独立，但语言文化相同/通，被视为同一个民族。 2. 中世纪文明神圣罗马帝国与基督教文化神罗诞生 （800 AD） 基督教已在欧洲不同民族间相当普及，罗马教会保持崇高微信。加洛林家族的法兰克王查理需要罗马主教的支持，有助于维持统治；同样，罗马主教需要查理保护。 公元799年，教皇利奥三世被罗马人民驱逐，查理立即南下支援，恢复权位。800 AD，教皇授予查理皇冠，以上帝名义宣布其为“罗马人的皇帝” 查理大帝的加冕仪式 东罗马帝国认为帝位被篡夺。自此，罗马教会完全摆脱懂罗马帝国的控制，僭越了加冕皇帝的权力，并将权力由神权延伸至政权，确立对意大利中部地区的统治。 查理重新统一欧洲，史称“查理大帝”（另译“查理曼”，“曼”意为“伟大的”），其创立帝国被称为“神圣罗马帝国”，神圣代表帝国具有的基督教性质，罗马表明对罗马帝国正统性的继承。 欧洲大部分地区实现政治统一，传统的古罗马文化与基督教文化和日耳曼文化融合，欧洲延续数百年的政治体制—封建制，也逐步形成。 封建制 大地主拥有附庸的武士。二者依据“献身”仪式（hommage）确立主仆关系，武士可获得土地，磨坊等收益，同时负有效忠主人的义务。 大地主部分或出身于西罗马帝国的元老院元老家族（前朝贵族），或出身于蛮族武士首领（征服者）。他们形成的新社会阶级，被称为“望族阶级”，即中世纪贵族阶级（拉丁语nobilis）。 查理被加冕后，定都亚琛，这也意味新罗马不再以环地中海作为主要发展方向，而是转入经营欧洲的中部和北部。 神罗疆域过于广大而交通不变，中央征服难以直接管理，故查理大帝采取早已存在的分封方式，建立帝国的分封制度。 帝国的土地分为数十块封地，分封于部下。这些人成为大封建主，被称作伯爵、侯爵、公爵。在法兰克语中： 公爵一词为dux，来自拉丁语，意为首领，其封地最广，地位最显赫 伯爵一词为conte，来自拉丁语的comite，意为战友，其封地较小，地位低于公爵； 侯爵一词为marchis，来自日耳曼语的marka，意为边界地区封地的统治者。 每年5月，皇帝召集封建主大会，颁布重要决定，形成国家的法律。同时，皇帝经常派巡按使到各地巡视（刺史），监察法律执行情况和封建主是否忠于皇帝巡按使。有时，皇帝还亲率巡按使视察各封地。 封建主在各自封地高度自由，视自己为封地的“君主”，豢养大批骑士，发号施令，横征暴敛，欺压封地居民。 查理建立封建制度的一个重要目的是满足军事需求，即皇帝可随时调配其附庸（封建领主），以及其附庸的附庸（封建主的骑士）。但该机制存在致命问题，即建立在皇帝与封建主个人的遗存关系上。随时间推移，这种关系很容易被破坏：封建主经常佯装忘记誓言，当皇帝宣召时，借机要求赏赐，否则按兵不动。帝国扩张阶段，查理新夺取的土地可满足封建主不断膨胀的欲望；查理继承者却无法夺取新的疆土，只能分封自己的土地，导致皇帝势力衰弱，丧失了中央政权的统治权威。 公元9世纪中叶，随着中央政权的不断削弱，皇帝逐渐失去了对封建主的控制；封建主的势力增强，不断扩大的封地变成了他们的世袭私产。封建主越来越不服从皇帝的命令，欧洲出现了“封建的无政府状态”，并一直延续到10世纪中叶。这一个世纪恰恰是阿拉伯人和蛮族人再度侵扰欧洲的时期。 软弱的中央政权无力抵抗外来侵略，人民对其丧失了信心。封建主便趁机加强自己的力量，把宫殿改造成城堡以抵御来犯之敌。人们因惧怕侵略者的肆意屠杀，纷纷寻求封建主的保护；为了生命安全，他们甘愿失去自由成为封建主的农奴。这样，封建体制在欧洲社会便牢牢地扎下了根。 瑞士阿尔高州的哈布斯堡城堡（拍摄于2021年4月25日） 再次的分裂与混乱 814 AD，查理大帝儿子“虔诚者”路易一世（Ludovico il Pio或Luigi I）继位，他将帝国领土分封给3个儿子。围绕领土和皇权的战争直至843 AD，最终签订凡尔登条约，形成三个王国，即意大利王国、日耳曼王国和法兰西王国。 长孙洛泰尔（Lotario）承袭帝号，但有名无实，任意大利国王，其领土包括低地国家，阿尔萨斯和洛林，勃艮第，普罗旺斯，意大利北部及亚琛，罗马两座城市；855 AD，洛泰尔去世，中法兰克帝国分裂（直至19世纪）。 “日耳曼人”路易（Ludovico il Germanico）任日耳曼国王，领土包括意大利以北，莱茵河以东，及莱茵河以西的莱茵兰地区； “秃头查理”（Carlo il Calvo）任法兰西国王，领土包括罗讷河以西所有领土。 《凡尔登条约》签订后的加洛林帝国版图 南方，掌握先进航海技术的阿拉伯人成为地中海霸主，夺取科西嘉和西西里，并以此为跳板不断侵袭意大利和法兰西沿海，俘虏男女作为奴隶。公元845年，阿拉伯人洗劫了罗马城。公元10世纪，他们深入欧洲腹地的阿尔卑斯山脉，控制连接意大利和法兰西的交通要道数十年之久，劫掠过往商人和旅行者的财物。 北方，公元8世纪末，来自北欧丹麦等地得诺曼人丹麦人，开始不断侵染欧洲其他国家，从英格兰直至黑海地区。公元10世纪，诺曼人直抵法兰西首都巴黎，查理三世与其首领罗洛（步行者罗夫，因身材过于高大，没有载他的马，只能步行）签订《圣-克莱尔-埃普特条约》，获得塞纳河口的卢昂地区，获得公爵头衔。公元911年，诺曼底公国建立，并皈依基督教。 11世纪，大批诺曼骑士南下意大利，击败教皇国军队及统治意大利半岛南部的拜占庭和伦巴第人，驱逐了撒拉逊人，夺取了坎帕尼亚、卡拉布利亚、普利亚等地区。 公元1059年，诺曼人的首领圭斯卡德（Guiscardo）与教皇尼古拉二世达成政教协议，被教皇加封为普利亚与卡拉布利亚公爵，其征服行为被罗马教廷所认可；同时他承诺进军西西里，驱逐阿拉伯人，使该岛重回基督教的怀抱。后来，在圭斯卡德的兄弟鲁杰罗一世（Ruggero I）的率领下，经过30年的激战，诺曼人终于打败了阿拉伯人，于1091年占领了西西里岛。公元1130年，鲁杰罗二世（Ruggero II）被加冕为西西里国王，兼领普利亚与卡拉布利亚公国。 东方，公元9世纪，匈牙利人的祖先马扎尔人由遥远的东方草原迁徙至欧洲，侵入日耳曼、法兰西和意大利；公元955年，匈人被日耳曼国王奥托一世（Ottone I）打败，被迫退回匈牙利，之后皈依了基督教。 日耳曼神圣罗马帝国的建立 936 AD， 萨克森公爵奥托一世登上日耳曼王位，955 AD打败马扎尔人入侵者，成为基督教卫士。961 AD，受教皇邀请，南下维护教皇利益，控制意大利北部。962 AD，受教皇加冕为“日耳曼神圣罗马帝国皇帝” （与查理曼大帝不同，虽同承担保卫基督教的义务，但帝国疆域缩减至日耳曼地区（德奥）和意大利北部）。 955 AD, 马札儿人在今德国奥格斯堡附近横渡莱茵河 奥托一世加冕后试图控制罗马教廷，发布《奥托特权》声明皇帝具有批准教皇选举结果的权力，神权依附于政权。自此西欧数百年教皇与皇帝的权斗历史开始上演。 “主教—伯爵制”与克吕尼改革 考虑到封建领主可能不顺从皇帝，奥托一世将土地分封给主教，委托其管理地方事务。由于主教是神职人员，无子嗣，无法传位，故去世后，封地返还君主。皇帝和教皇都希望同时掌握政权和神权，皇帝试图干预教会的宗教事务，如自己向教会推荐主教人选，教皇也试图干预帝国的政治事务，让自己人成为伯爵。因此，二者之间必然会出现不可调和的矛盾和斗争。 “主教—伯爵制”有利于巩固皇权，却严重地影响了教会。人们谋求高级教职，不为宗教理想，而为牟取权力与财富，教会一步步腐坏堕落。自11世纪中叶起，一些虔诚的教士奋起反抗，呼吁教会改革，严禁皇帝和封建主干预教务，非宗教人士不得参与选举和任命教职 （Tips: 修道院直接受教皇保护，土地财产归于禄马教廷，封建主无法随意侵犯，故在改革中具有强大的战斗力）。 位于法国马孔附近的克吕尼修道院，克吕尼改革运动的发起地 格列高利七世与亨利四世 1073年，克吕尼修道院的代表人物格列高利成为教皇。公元1075年，格列高利七世发布敕令：教皇高于皇帝，有权评判皇帝和国王的行为正确与否，必要时有权罢黜皇帝或国王。此敕令将政“一切权力来自于上帝”解释为一切权力来自于上帝在人间的代表——教皇，相当于对神罗帝国皇帝宣战。 1076年，亨利四世（Enrico IV）皇帝无法忍受耻辱，召集效忠皇帝的主教开会，宣布罢黜格列高利七世教皇，重申皇帝至高无上的地位。格列高利七世随即宣布开除亨利四世教籍（等同于剥夺统治权）。大封建主开始解除与皇帝的义务（基督徒无需承担对非基督徒君主的义务），纷纷反叛。亨利四世被迫在1月前往意大利中部的卡诺萨（Canossa）城堡，身穿僧袍在外静候3日，获得教皇宽恕，并恢复教籍。 亨利四世在卡萨诺城堡前向格里高利七世忏悔 之后，亨利四世卷土重来，拘捕格里高利七世，关押于罗马的圣天使古堡，并重新任命教皇为其加冕。占据意大利南方的诺曼人圭斯卡德干预，格里高利得以释放，但软禁于意大利南部的萨勒诺直至去世。 亨利四世和格里高利去世后，帝国与教廷之争延续数十年。1122AD，双方在沃尔姆签订《沃尔姆斯协议》，确定：主教由教士组成的选举会议推选，日耳曼主教的选举会议须在皇帝或其代表莅临的情况下进行，如果主教兼任封建主，皇帝则有权举行主教的叙爵仪式，授予其象征世俗权力的权杖。两个仪式的先后顺序极为重要，因为先者有权确定“主教——伯爵”的人选，而后者只有否定权；为了避免冲突，后者一般均予以认可。在两个仪式的先后顺序上，教廷也与帝国达成了妥协：在意大利等地，授予主教——伯爵世俗权力的仪式应在授予宗教神权之后6个月举行，在日耳曼，皇帝则可以先举行主教——伯爵的叙爵仪式，授予主教——伯爵相声世俗权力的权杖。两种不同的规定造成了两种不同的结果：在意大利，教皇的权力越来越大，而在日耳曼，皇帝的权力则大于教皇的权力。《沃尔姆斯协议》宣告激烈的对抗得以缓和。历史证明，该协议最大受益者是罗马教廷，它进一步加强了教廷在整个天主教会中的统治地位。 沃尔姆斯主教座堂内景（拍摄于2022年8月2日）。其主祭坛下方安放有康拉德二世等皇帝或主教的灵柩。在此与教宗签订《沃尔姆斯协议》的亨利五世则安葬于施派尔主教座堂 格里高利的改革使教皇在教会掌握绝对权力，建立森严的等级制度，依次为教皇、枢机主教(即红衣主教，枢机团有权选举下一任教宗，教宗也由枢机团成员中选出)、大主教、主教、本堂神甫、助祭等。枢机主教享有教皇的选举权和被选举权。此组织结构延续至今。同时，东罗马帝国所控制的东欧不承认教皇权威，视君士坦丁堡宗主教为精神领袖。东方教会有着一套与西方教会不同的祭祀礼仪和对神学的理解。所以，当罗马教皇试图成为整个基督教世界的最高领袖时，东西方教会的分裂在所难免。公元1054年，罗马教皇利奥九世与君士坦丁堡宗主教色路拉里乌相互革除对方的教籍，东西方教会彻底决裂 。 中世纪封建等级贵族 (aristocracy)、僧侣 (monk) 和农民 (peasant) 贵族包括皇帝、国王、封建主、骑士，主要从事军事活动，保卫和扩张国家或封地的领土。事实上，中世纪的贵族就是骑士，皇帝和国王是最大的骑士，封建主是拥有封地和爵位的骑士。绝大多数普通骑士没有封地，为领主服务，同时受领主保护。为了豢养大批骑士，封建主必须残酷地剥削劳动者。 封建主为防御需要，修建城堡。封地居民在战乱时可进入避难。周边居民为寻求乱世中的安全，将土地献给城堡主，称为封地居民。封建主土地、权力、威信由此不断扩张。小封建主渴望战争，取得竣工，获得丰富的战利品和更大的封地。大封建主奔波于国王或皇帝的宫廷与封地之间，在宫廷中他们是臣子，在各自的封地，他们却是名符其实的“君主”。城堡中设有法庭，可以处理封地居民之间的纠纷，对偷窃、抢劫等轻罪，可做出罚款、杖罚、鞭罚、拘禁等判决；叛国、凶杀等重罪，则须由国王或皇帝指定的法官审理。 公元10世纪后，随马掌(保护战马不易受伤)和马镫(双手武器)的普遍采用，战马成为名符其实的战争机器。因此，随技术发展，可披挂与步兵相同的重甲，配备重型武器，并拥有“长枪冲击战术”的重骑兵在军队中比重越来越大。 理论上，任何拥有战马和武器装备且受过基督教洗礼的人，无论出身，都可以被豢养他的封建主加封为骑士，成为封建主的封臣（法国文学“武功歌”）。但平民很难不从事劳作而专门训练，并购置昂贵的骑士装备。故骑士多出身贵族，自幼学习骑术，练习使用兵器。几乎每一个大封建主的城堡中都开设骑士学校。小封建主和骑士的子弟7岁时便被送到那里接受教育，从侍童做起，兼做家务或陪伴夫人。12岁时开始习武，成为城堡主的正式侍从，并随其狩猎，参加比武和征战。当主人确认他已经完全具备了骑士的条件时，便举行加封仪式。主教首先为受封骑士的矛、剑、盔、盾、马刺等物品祈福，接着受封骑士披挂上这些装备，朗诵祈祷词和誓词，表示承担骑士义务。最后，主教或城堡主走到下跪受封的骑士面前，在其颈部或肩部重拍一下，庄重地宣布“我以上帝的名义加封你为骑士”。当新骑士配剑时，主教/城堡主会用剑轻击其脸颊，被称为dubbing，即配音。 骑士们在城堡内比试剑法（拍摄于瑞士伦茨堡，2022年9月11日) 日耳曼神圣罗马帝国建立之后，大规模对外战争结束，但骑士尚武习俗并未改变，封建主争权夺利冲突仍然存在，故骑士间争斗继续。 为保护家族的财产和权势，使封地和封号世代相传，封建主去世时只能把封地和爵位传给一个儿子，因此出现了长子继承制，即长子继承封建主的爵位和封地，其他的儿子只能选择成为僧侣或骑士。僧侣凭家族的势力支持发展较快，一些人可能成为主教，甚至教皇，其成就又会大大提升家族社会地位（如美第奇族产生四位教皇）。骑士则往往为另一封建主服务，成为封臣。既不能成为僧侣又无缘封臣的贵族子弟，则往往成帮结伙地在社会上游荡，打架抢掠，对社会造成极大破坏。为维护社会秩序，教会进行干预，试图引导他们走上正路：公元11世纪后期，成立了旨在将纨绔子弟改造成上帝卫士的教会骑士团，将这些游手好闲、恶习成性的贵族子弟加封为骑士，其必须改邪归正，宣誓保护神职人员、穷人、妇女和儿童等弱者，惩恶扬善，捍卫上帝的王国。 僧侣僧侣包括俗间僧侣（clero secolare）和隐修僧侣（clero regolare）两类，最高代表为罗马教皇。俗间僧侣分布于各地教堂，被称作神甫，服从各地主教的领导，与民众直接接触，帮助民众解决世俗生活问题，取得民众信任，引导其灵魂得以救赎。主教生活在城市，成为中世纪城市真正统治者，不仅管理教会财产，行使司法权，还掌控军队。 沃尔姆斯教堂及其附属建筑模型（拍摄于2022年8月2日）。教堂北侧附属建筑为主教宫殿，与教堂侧廊直接相连，主教为城市实际统治者 隐修僧侣生活在深山老林中的修道院，被称作修道士，他们服从修道院院长的领导，严格地遵守修道院的规章，祈祷上帝，从事生产，把自己的一生完全奉献给上帝。在中世纪早期的混战中，隐修僧侣帮助了民众，并为保护文化做出了巨大的贡献，因而赢得了信徒们的信任和支持，修道院的重要性和财富急剧增长，隐修僧侣的社会地位也越来越高。控制僧侣等级的高级教职一般由贵族出身的人担任，因为只有在有权势的家族的支持下，教士才有可能当选修道院院长、主教和教皇。 德国毛尔布隆修道院内役工(lay brother)聚会的房间，现已辟为音乐厅（拍摄于2022年6月5日） 僧侣等级本应掌控神权，但实际上，却在很大程度上控制着中世纪欧洲社会的世俗权力，它与贵族等级一起构成了欧洲封建中世纪的统治阶级 农民农民的绝大多数是农奴，战乱年代为自保不得不寻求封建主或教会保护，丧失个人土地及人身自由，劳役世代相传(serfdom 农奴制)，但人身不可买卖。 封建主、主教和修道院的土地被分成许多庄园，由主管负责管理，一般分为三类： “领主耕地”，包括田地、葡萄园、果园等，由佃农（农奴）耕种，其收获全部归领主所有； “佃农耕地”，由小块田地和葡萄园组成，也由佃农耕种，领主从收成中抽取租税； “公地”，用作牧场的草地和树林，领主和佃农都可以使用，但佃农要支付一定的使用费。 除为领主无偿劳作并缴税外，佃农还要承担其他名目繁多的赋税和劳役，如：战争税、婚嫁税、磨房税、养路税、过桥税、砍柴税、遗产税等。此外，还要向教会缴纳“什一税”（欧洲基督教会依据《圣经》所载农牧产品十分之一“属于上帝”的说法，向居民征税）。 除农奴外，封建主的领地上还生活着许多奴隶，没有人身自由，是封建主的私人财产。奴隶可以娶妻生子，成立家庭，但他们本人、子女、生产工具，乃至生活用品都归封建主所有。中世纪奴隶的生存条件比古罗马奴隶要好一些，一般都接受过基督教洗礼。由于教会禁止虐待基督徒，这在一定程度上避免了奴隶处境的恶化。奴隶分为“居家奴”和“奴仆”两类。“居家奴”为封建主耕种土地，服劳役，封建主为他们提供住房和土地，他们的生存条件与佃农相差无几。“奴仆”一般生活在封建主的家中，主要服侍封建主及其家人。有些心腹奴仆还会被封建主任命为庄园主管，代其管理庄园事务。 中世纪战乱不断，海盗和山贼肆虐。因运输困难，风险大，远途贸易基本中断。封地的经济几乎是封闭的，唯一的商品交换活动是封地上的集市贸易，换取食盐及铁制农具等生活和生产资料。 斯特拉斯堡教堂附近的集市 (拍摄于2021年12月26日） 有关农业/集市的知识补充 德国，先有教堂后有城市。神罗向东扩张，强制传教。诸侯赏赐土地于教士，建立传教据点（主教领地）。核心位置建设小教堂，周边土地分给愿意开垦的农民，并收取粮食为田租。教会获得收入后，招募石匠、木匠进一步扩建教堂，逐渐，附近出现铁匠铺，仓库，货场，小酒馆等。朝圣者、旅游者和行商也开始出现，城市诞生。 德国的犁与其他国家不同。10世纪，重犁发明，便于深耕，德意志人终于找到适合自己土壤和气候的耕作方式。但犁刀太深，需要4头牛，故村民集体劳作。一条垄沟连绵，各人田地产权无法分清，因此集体劳作、协商并由村里德高望重的老人决断。自此，德国逐渐形成了尊重权威，服从集体，重视协作的传统。 中世纪基督教文化状况文化掌握在贵族（皇帝、国王、封建主和骑士）与僧侣两大统治阶级手中。政治和军事的中心在城堡，文化中心在修道院。战火使得中世纪的城市丧失古罗马时期的繁荣，位于乡村和山区的城堡与修道院中涌入幸存下来的市民，以获取保护。主教却依然坚守在城市的教堂中，在自己管辖的教区里继续行使权力，在一定程度上避免了城市的灭亡。此时的城市徒有其名，实际上已经乡村化了（一般仅有两三千居民）。 城堡内的贵族尚武轻文，修道院成为文化传播的中心。所有渴望获得文化知识的人都要在修道院里学习，接受基督教的文化教育，包括拉丁语、历史、算术、几何及各类自然科学，如植物学、矿物学、医学等。皇帝、国王和封建主身边的文职官员也必然由掌握文化的僧侣担任。 教会完全控制文化，使其彻底地基督教化。教会篡改灿烂辉煌的古希腊和古罗马文化，为传教服务，神学也顺理成章地成为至高无上的学科，“信则明”成为中世纪哲学思想的基础。基督教会大力宣扬上天重于人间、来世重于今生、灵魂重于肉体的神秘主义 圣事（sacramento）亦称“圣礼”，是基督教的重要礼仪。天主教和东正教都认为“圣事”共有7件，即洗礼、坚振、告解、圣体圣事（圣餐）、终傅、授神职礼和婚配。新教一般只承认“洗礼”和“圣餐”为圣事。 洗礼亦称圣洗，是基督教的入教仪式，分为“注水洗礼”和“浸礼”两种。行注水洗礼时，主持洗礼者在受洗者额上洒少许清水，水顺额头流下，同时念诵规定的洗礼词。行浸礼时，主持洗礼者念诵规定的洗礼词，引领受洗者全身浸泡在水池中片刻。早期基督教会经常举行“浸礼”，有些古老的教堂附近还设有专门的浸礼池。8世纪以后，因“浸礼”很不方便，尤其对体弱病残者更是困难，西部教会行“注水洗礼”者日增，12世纪以后，天主教会全部采用“注水洗礼”。东正教和新教的某些教派（如“浸礼会”等组织）至今仍然举行“浸礼”。基督教认为，洗礼（Battesimo）是耶稣基督定立的圣事，它可以赦免入教者的“原罪”和“本罪”（peccato personale又称“现犯罪”，意为除“原罪”之外，人们违背上帝意志所犯下的罪），赋予其“恩宠”和“印号”，使其成为教徒，并有权领受其他圣事. 坚振 “坚振”（Confermazione）亦称“坚振礼”或“坚信礼”，是基督教的重要圣事之一。洗礼后经过一段时间，入教者要接受“坚振礼”，这是一种“按手礼”，由主教施行。行礼时，主教把手按在领受者头上，念诵规定的礼仪词。教会认为，该礼仪能够使“圣灵”降临至受礼者的身上，以坚定其信仰，振奋其灵魂，故称其为“坚振”。据《使徒行传》记载，圣彼得、圣约翰和圣保罗都曾为信徒行过“按手礼”，使他们得到“圣灵”。主教被认为是使徒的继承人，故“按手礼”一般应由主教施行。新教有些宗派也行“坚振礼”，但并不认为该礼仪是基督亲自设立，故不称其为圣事。 3.告解 亦称“办神工”，俗称“忏悔”。基督教认为，为了赦免教徒领洗后对上帝所犯下的罪过，使他们重新获得上帝的恩宠，耶稣基督亲自定立了这一圣事。行告解（Confessione）礼时，教徒向神甫说明对上帝所犯下的罪过，并表示忏悔；神甫则应对教徒的忏悔保密，并向教徒阐明如何补赎罪过，取得上帝的宽赦。 4.圣体圣事 亦称“神交圣礼”。天主教称其为“圣体圣事”（Eucarestia），其礼仪被称作“弥撒”（Messa）；东正教称其为“圣体血”（Divina Liturgia）；新教称其为“圣餐”（Santa cena）。据《圣经·新约》记载，耶稣在与使徒共进最后晚餐时，对面饼和红酒做了祝祷，然后将其分给使徒食用，称面饼是自己的身体，红酒是自己的血液，象征他为了免除众人的罪过而舍弃自己的身躯和鲜血，并命令其后世门徒施此礼仪，以纪念他为解救人类而献身。行此礼仪时，主持礼仪的神职人员首先祝祷面饼和葡萄酒，然后由信徒领食。 5.终傅 在教徒临终前，神甫用主教祝圣过的橄榄油涂抹其耳、目、口、鼻和手足，并诵念一段祈祷经文。基督教认为，通过终傅（Estrema Unzione）可以赋恩宠于受傅者，减轻其身体与精神的痛苦，赦免其罪过。 6.授神职礼 亦称“授圣职礼”或“派立礼”（Ordinazione）。基督教会对神职人员授予神职时所举行的礼仪，各教派所采用的方式不尽相同。 7.婚配 在教堂中，由神甫主持，按照教会的规定举办结婚仪式（matrimonio）。仪式的主要内容有：神甫询问男女双方是否愿意结为夫妻；得到肯定的回答之后，神甫诵念规定的祈祷经文，并根据《玛窦福音》中的信条宣布：“凡天主所结合的，人不可拆散”，(11)然后向婚姻双方祝福。新教不视其为圣事，但有邀请牧师证婚的习惯. – 相关链接Map of the Ancient World - World History Encyclopedia FOLLOWING HADRIAN Hadrian’s travels What was the ceremony for making a knight?","link":"/2022/03/01/25.%5BReading%5D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%84%8F%E5%A4%A7%E5%88%A9%E6%96%87%E5%8C%96%E7%AE%80%E5%8F%B2/"},{"title":"MODIS卫星火点数据的简易处理与可视化分析","text":"提要：此文介绍利用Python语言处理NASA MODIS火点数据（Global Monthly Fire Location Product，MCD14ML），可实现的基本功能包括:（1）特定时期的火点信息提取;（2）特定区域内的火点信息提取;（3）火点密度空间分布的计算与可视化表达 火点数据背景介绍生物质(biomass)和石油、煤炭等一样，是一类重要的能源物质。在我国的广大农村地区，生物质能源(如秸秆、薪柴以及牲畜粪便等)占据重要地位，在1979年前，更是占到整个农村能源消费总量的70%以上[^1][1]。同时，由于生物质燃烧过程中，排放大量气态以及颗粒态的污染物，对大气能见度造成影响，危害人体健康，同时具有着一定的气候效益。 环境遥感技术依据中红外波段对高温热源的敏感特性，可对全球陆面的火点信息进行检测，识别生物质开放燃烧事件的具体位置。目前较为成熟的火点数据库主要有两类：MODIS以及VIIRS 375m，以多种数据格式为公众提供历史火点信息。 特定时期燃烧事件数据的提取此处介绍MODIS数据集中应用最为广泛的__MCD14ML__数据，对基础数据的打理和分析做以基本的介绍。__MCD14ML__数据的单个文件包含某个月的全球火点信息，具体获取方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#### 下载网址及用户名 ### #ftp://fuoco.geog.umd.edu/modis/C6/mcd14ml/#用户名:fire#密码: burnt #----------------------#### 1. 文件的下载下载# 此处参考https://stackoverflow.com/questions/7006574/how-to-download-file-from-ftp中的方法进行下载，亦可直接在浏览器中手动下载 import mimetypes,os,urllib2,urlparsedef filename_from_url(url): return os.path.basename(urlparse.urlsplit(url)[2])def download_file(url): \"\"\"Create an urllib2 request and return the request plus some useful info\"\"\" name = filename_from_url(url) r = urllib2.urlopen(urllib2.Request(url)) info = r.info() if 'Content-Disposition' in info: # If the response has Content-Disposition, we take filename from it name = info['Content-Disposition'].split('filename=')[1] if name[0] == '\"' or name[0] == \"'\": name = name[1:-1] elif r.geturl() != url: # if we were redirected, take the filename from the final url name = filename_from_url(r.geturl()) content_type = None if 'Content-Type' in info: content_type = info['Content-Type'].split(';')[0] # Try to guess missing info if not name and not content_type: name = 'unknown' elif not name: name = 'unknown' + mimetypes.guess_extension(content_type) or '' elif not content_type: content_type = mimetypes.guess_type(name)[0] return r, name, content_typeimport shutildef download_file_locally(url, dest): req, filename, content_type = download_file(url) if dest.endswith('/'): dest = os.path.join(dest, filename) with open(dest, 'wb') as f: shutil.copyfileobj(req, f) req.close() time = \"201609\" ## 此处以2016年09月为例filename = 'MCD14ML.'+time+'.006.01.txt.gz'download_file_locally('ftp://fire:burnt@fuoco.geog.umd.edu/modis/C6/mcd14ml/',filename) #### 2. 文件预处理# 原始文件中存在多类分隔符，需进行预先处理，统一替换为“ ”，便于后续读取。with open(filename, 'r') as file : filedata = file.read()# Replace the target stringfiledata = filedata.replace(' ', ' ')filedata = filedata.replace(' ', ' ')#Write the file out againwith open(filename, 'w') as file: file.write(filedata)#### 3. 文件读取df = pd.read_csv(filename, sep='\\s+')df.head() 特定区域内燃烧事件的提取原始数据中包含该段时期火点位置的有关信息，对于研究生物质燃烧对于局地污染事件发生的影响，应对于特定地理位置内的火点信息进行提取。该工作同样可以借助fiona包，利用简单的判断语句实现。这里我们以四川省为例，对2016年9月发生在该省境内的逐日火点信息进行提取。 12345678910111213141516171819import fionafrom shapely.geometry import shape,Polygon, Point# read the shapefilesc_area = fiona.open(\"/Users/HYF/Documents/NanChong/GIS/sichuan.shp\")pol = sc_area.next()from shapely.geometry import shapegeom = shape(pol['geometry'])poly_data = pol[\"geometry\"][\"coordinates\"][0]poly = Polygon(poly_data)### zip the x coordidate and y coordidate of each hotspotpoints = zip(*np.array([df.lon.values, df.lat.values]))### to identify whether the hotspot is inside the boundary or notmask = np.array([poly.contains(Point(x, y)) for x, y in points]) df[\"MASK\"] = maskselect = df[df[\"MASK\"] == True]select['datetime'] = pd.to_datetime(select.YYYYMMDD,format = '%Y%m%d') 简单的图形绘制 火点分布图例如，此处我们专门考察2016年9月5日四川省境内的火点分布情况： 1234567891011121314####Only select the biomass events in 2016-09-05select[select.datetime == '2016-09-05']m = Basemap(projection='cyl', resolution='l',llcrnrlon = 97.2,llcrnrlat=26.0,urcrnrlon = 109.0,urcrnrlat=34.5)m.readshapefile('/Users/HYF/Documents/NanChong/GIS/sichuan', 'sichuan', color='b', zorder=3,linewidth=1.5)m.drawcoastlines(color = '0.15')lon, lat = m(select.lon,select.lat)plt.scatter(lon,lat, color = 'red')parallels = np.arange(26.,34,3.)m.drawparallels(parallels,labels=[False,True,True,False])meridians = np.arange(97,109.,3.)m.drawmeridians(meridians,labels=[True,False,False,True])plt.show()","link":"/2017/06/22/6.%5BCode%5DPython%E7%81%AB%E7%82%B9%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%86%E6%9E%90%E5%A4%84%E7%90%86/"},{"title":"中国空气质量历史数据整理(HDF5格式)","text":"全国空气质量历史数据是大气环境研究的重要基础资料，而我国官方平台仅提供实时监测信息，而且还只支持IE浏览器(╮(╯_╰)╭)。 我曾尝试采用爬虫工具获取并存储逐时信息，但限于权限，未能在实验室服务器上连续长时间运行。互联网上直接提供数据或API端口的网站有很多，如环境云，PM25.in, 青悦开放环境数据中心等。其中，beijingair最具分享精神,提供了2013年至今的详尽历史数据，且完全免费。 由衷感谢@王_晓磊的出色工作和无私分享，极大地推动了我国环境数据的公开与透明。其数据格式为每日一份csv文件存储当日所有站点/城市的逐时信息。在长时间尺度的数据分析时，需逐一阅读各原始文件。此处，我考虑将全年数据文件整合为多维度数据存储格式(HDF5)文件，便于调用和处理。 1. 全站点数据整合 截至2018年，我国367座城市共设置有1437处国控空气质量监测站点。下文为全年数据打理流程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#libraryimport pandas as pdfrom pandas import HDFStore, DataFramefrom pandas import read_hdfimport os,sys,stringimport numpy as np### CREAT A EMPTY HDF5 FILEhdf = HDFStore(\"site_2017_whole_year.h5\")### READ THE CSV FILES AND SAVE IT INTO HDF5 FORMATos.chdir(\"./site_2017/\")files = os.listdir(\"./\")files.sort()print files[0]test_file= \"./../china_sites_20170101.csv\"test_f = pd.read_csv(test_file,encoding='utf_8')site_columns = list(test_f.columns[3:])print site_columns[1]### AFTER 2017_2 PUTUO SITE IN SHANGHAI (1141A) is in.### 注意，2017年2月后上海市增加了普陀站点(1141A),需在site_columns中补充上#site_columns = site_columns.append(u'1141A')feature = ['pm25','pm10','O3','O3_8h','CO',\"NO2\",'SO2',\"aqi\"]fe_dict = {\"pm25\":1,\"aqi\":0, 'pm10':3, 'SO2':5,'NO2':7, 'O3':9,\"O3_8h\":11, \"CO\": 13}for k in range(0,len(feature),1): data_2017 = {\"date\":[],'hour':[],} for i in range(0,len(site_columns),1): data_2017[site_columns[i]] = [] for file in files[0:]: print file filename,extname = os.path.splitext(file) if (extname == \".csv\"): datafile =file f_day = pd.read_csv(datafile,encoding='utf_8') #site_columns = list(f_day.columns[3:]) for i in range(0,len(f_day),15): datetime = str(f_day[\"date\"].iloc[i]) hour = \"%02d\" % ((f_day[\"hour\"].iloc[i])) data_2017[\"date\"].append(datetime) data_2017[\"hour\"].append(hour) for t in range(0,len(site_columns),1): data_2017[site_columns[t]].append(f_day[site_columns[t]].iloc[i+fe_dict[feature[k]]]) print feature[k] data_2017 = pd.DataFrame(data_2017) hdf.put(feature[k], data_2017, format='table', encoding=\"utf-8\") 2. 全城市数据整合 方法类似，每次写入367座城市某一类污染物信息 123456789101112131415161718192021222324252627282930313233343536### CREAT A EMPTY HDF5 FILEhdf = HDFStore(\"city_2017_whole_year.h5\")### READ THE CSV FILES AND SAVE IT INTO HDF5 FORMATos.chdir(\"./city_2017/\")files = os.listdir(\"./\")files.sort()print files[0]test_file= \"china_cities_20170101.csv\"test_f = pd.read_csv(test_file,encoding='utf_8')city_columns = list(test_f.columns[3:])print city_columns[1]feature = ['pm25','pm10','O3','O3_8h','CO',\"NO2\",'SO2',\"aqi\"]fe_dict = {\"pm25\":1,\"aqi\":0, 'pm10':3, 'SO2':5,'NO2':7, 'O3':9,\"O3_8h\":11, \"CO\": 13}for k in range(0,len(feature),1): data_2017 = {\"date\":[],'hour':[],} for i in range(0,len(city_columns),1): data_2017[city_columns[i]] = [] for file in files[0:]: filename,extname = os.path.splitext(file) if (extname == \".csv\"): datafile =file f_day = pd.read_csv(datafile,encoding='utf_8') city_columns = list(f_day.columns[3:]) for i in range(0,len(f_day),15): datetime = str(f_day[\"date\"].iloc[i]) hour = \"%02d\" % ((f_day[\"hour\"].iloc[i])) data_2017[\"date\"].append(datetime) data_2017[\"hour\"].append(hour) for t in range(0,len(city_columns),1): data_2017[city_columns[t]].append(f_day[city_columns[t]].iloc[i+fe_dict[feature[k]]]) print feature[k] data_2017 = pd.DataFrame(data_2017) hdf.put(feature[k], data_2017, format='table', encoding=\"utf-8\") 在此处分享我已转制的数据文件，欢迎大家学习、使用： 2017年 所有站点/所有城市 2016年 所有站点/所有城市 2015年 所有站点/所有城市 全国国控环境监测点列表及经纬度 3. HDF读取应用例举 3.1 提取某个城市的站点浓度信息12345678910111213import pandas as pdfrom pandas import ExcelWriterimport sys,csv,os#### SELECT THE SPECIES ####feature = ['pm25', 'pm10', 'O3', 'O3_8h', 'CO', 'NO2', 'SO2', 'aqi']writer = ExcelWriter('./city_2017.xlsx')for i in range(0,len(feature),1): df = pd.read_hdf(\"./site_2017_whole_year.h5\",\\ feature[i], encoding = 'utf-8') df = df[['date','hour','1744A','1745A','1746A','1747A']] df.to_excel(writer,feature[i], index = False)writer.save() 3.2 全国PM2.5年均浓度分布123456789101112131415161718192021222324252627282930313233343536#libraryfrom mpl_toolkits.axes_grid1 import make_axes_locatableimport cartopyfrom cartopy.io.shapereader import Readerfrom cartopy.feature import ShapelyFeaturefrom mpl_toolkits.basemap import cm as base_cm#Load the data select the variabledata_city = {\"city\":[],'pm25':[],'pm10':[]}pm25 = pd.read_hdf(\"./../data/city_2017_whole_year.h5\",'pm25', encoding = 'utf-8')pm10 = pd.read_hdf(\"./../data/city_2017_whole_year.h5\",'pm10', encoding = 'utf-8')for i,t in enumerate(pm25.columns[2:]): data_city['city'].append(t) data_city['pm25'].append(pm25[t].mean()) data_city['pm10'].append(pm10[t].mean()) data_city = pd.DataFrame(data_city) ## Add the lon,lat for each city## 加载所有国控站点名称，位置的xls文件，已上传至google drive，下载链接见上文Df = pd.ExcelFile(\"./站点列表含经纬度-1497个.xlsx\", )df = Df.parse(u'Sheet1')data_city['lon'] = [df[df[u'城市'].str[0:2]==k[0:2]][u'经度'].mean() for k in data_city['city']]data_city['lat'] = [df[df[u'城市'].str[0:2]==k[:2]][u'纬度'].mean() for k in data_city['city']]#Plotting fig = plt.figure(figsize=(6, 5))ax = plt.subplot(projection=ccrs.PlateCarree())ax.set_extent([73, 135, 17, 55])ax.coastlines(linewidth = 0.2)ax.add_feature(cartopy.feature.OCEAN)ax.add_feature(cartopy.feature.BORDERS,linewidth = 0.4)pm25_plot = ax.scatter(data_city['lon'],data_city['lat'],s = 45,zorder = 2,c = data_city['pm25'],\\ cmap=base_cm.GMT_seis_r, lw = 0,alpha = 0.9) 例图","link":"/2018/04/04/8.%5BDataset%5DChina%20air%20quality%20data%20in%20HDF5%20format/"},{"title":"利用Python获取地理信息并可视化","text":"网络中有丰富的地理信息数据资源，此处介绍我采用Python工具实现地形地貌、行政区划、城市交通路网等数据获取及可视化的部分实例，供大家参考学习。 1. 简易地形底图绘制 1.1 WMS数据库针对较大范围的研究区域，我采用WMS数据库的地形底图，依四角坐标下载特定尺寸的图片，并展示。此处以东亚地区为例，逐步展示。 1234567891011121314151617181920# 1. Set the left, bottom, right, top locationsllx,lly, urx,ury = [104.211,29.150,105.523,30.150]# 2. Download the original filetarget = 'http://www2.demis.nl/wms/wms.asp?Service=WMS&amp;WMS=worldmap&amp;Version=1.1.0&amp;Request=GetMap&amp;BBox=%s,%s,%s,%s'%(llx,lly, urx,ury) +\\ '&amp;SRS=EPSG:4326&amp;Width=800&amp;Height=600&amp;Layers=Bathymetry,Countries,Topography,Ocean%20features,Hillshading,Borders,Waterbodies,Coastlines&amp;Format=image/gif'path_name = './Terrain.gif'urllib.urlretrieve(target, path_name) # 3. Plot the terrain map of East Asiafrom scipy.misc import imreadfig = plt.figure(figsize = (8,6))ax = plt.subplot()m = Basemap(projection='cyl', resolution='l',llcrnrlon = llx,llcrnrlat=lly,urcrnrlon = urx,urcrnrlat=ury)img = imread(path_name)plt.imshow(img, zorder=1,extent=[llx,urx,lly,ury ] )m.drawparallels(np.arange(-90., 120., 10.), labels=[1, 0, 0, 0],fontsize =18,linewidth= 0.02)m.drawmeridians(np.arange(45,180,10), labels=[0, 0, 0, 1],fontsize = 18,linewidth= 0.02)plt.subplots_adjust(left=0.015, bottom = 0.05, top = 0.99,right=0.99, wspace = 0.1,hspace=None) 出图的样例，如下所示。 1.2 SRTM geotiff数据对于较小的地理环境，直接采用WMS格式图片清晰度不够。此处介绍下载并绘制原始DEM地形文件的方法。通过elevation package进行tif格式地形文件的下载 12345678910111213141516171819202122232425262728293031# 以四川某市及周边地区为例## 1. Download the geotiff file!pip install elevation !eio --product SRTM3 clip -o DEM.tif --bounds 104.211 29.150 105.523 30.150 # left bottom right top locations ## 2. Read the tif file as 2-d arraypathToRaster = r'./DEM.tif'raster = gdal.Open(pathToRaster, gdal.GA_ReadOnly)dem = raster.GetRasterBand(1).ReadAsArray()dem = dem[::-1] ## 3. Generate a terrain coloarmap import matplotlib.colors as colorsdef truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100): new_cmap = colors.LinearSegmentedColormap.from_list( 'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n))) return new_cmapcmap = plt.get_cmap('terrain')terrain_cmap = truncate_colormap(cmap, 0.2, 0.7) ## 4. Plot it!llx,lly, urx,ury = [104.211,29.150,105.523,30.150]fig = plt.figure(figsize = (8,5))ax = plt.subplot()m = Basemap(projection='cyl', resolution='l',llcrnrlon = llx,llcrnrlat=lly,urcrnrlon = urx,urcrnrlat=ury) dem_plot = map.imshow(dem, extent=[llx,urx,lly,ury ], cmap =plt.get_cmap('terrain'),vmin = 0 ,vmax = 900)# terrain_cmapmap.drawparallels(np.arange(-90.0, 120., 0.5), labels=[1, 0, 0, 0],fontsize = 12,linewidth = 0.1,color = '#FFFFFF', zorder = 10)map.drawmeridians(np.arange(-180.0, 180., 0.5), labels=[0, 0, 0, 1],fontsize = 12,linewidth = 0.1,color = '#FFFFFF',zorder=10) 结果如下图所示： 1.3 Cartopy STAMEN 图形绘制数据源stamen是开源的地理信息服务网站，提供道路、地形等多种数据信息。在其官方网页采用手动下方式，仅能以确定中心点和缩放比例的方式进行图片下载，不能实现任意区域内图形获取。此处参照Geology and Python博客中的相关教程，以中国全域为例，展示底图获取和绘制流程。 cartopy是用以分析并可视化地理数据的Python库。在安装时候，与系统原有的matplotlib.basemap发生冲突，原因在于二者所依赖的GEOS库不一致。在此处花费了不少时间来解决这一问题(网络上也有很多人遇到了类似的问题，如无法同时安装成功，安装后无法import以及执行cartopy有关指令，kernel会自动restart. )。 在此处，我列出了自己的安装步骤: 123456789101112131415161718192021222324252627282930313233343536373839404142434445## 卸载原有的cartopy及相关库shapely，保留原有的basemapconda uninstall cartopy conda uninstall shaeply ## 升级gdal库 brew upgrade gdal ## 采用pip方式安装cartopy及shapelypip install shapely cartopy --no-binary shapely --no-binary cartopy## 结果仍未能成功import cartopy,原因在于shapely库未安装成功。用conda方式再安装conda install shapely ## 全部安装完毕!! d(`･∀･)b## 开始绘图import matplotlib.pyplot as pltimport cartopy.crs as ccrsfrom cartopy.feature import BORDERSfrom cartopy.io.img_tiles import StamenTerrainfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatterdef basemap_terrain(extent): fig = plt.figure(figsize=(8, 6)) ax = fig.add_subplot(111, projection=ccrs.PlateCarree()) ax.set_extent(extents=extent, crs=ccrs.Geodetic()) ax.coastlines(resolution='10m') BORDERS.scale = '10m' ax.add_feature(BORDERS) ax.gridlines(color='.5') ax.set_xticks(np.arange(extent[0]+2, extent[1], 15), crs=ccrs.PlateCarree()) ax.set_yticks(np.arange(extent[2]+2, extent[3], 15), crs=ccrs.PlateCarree()) ax.xaxis.set_major_formatter(LongitudeFormatter()) ax.yaxis.set_major_formatter(LatitudeFormatter()) return fig, ax# 地图四至x1,x2,y1,y2 = 74,134, 18,51extent = x1 -1, x2+1,y1-1, y2+1 fig, ax = basemap_terrain(extent)st = StamenTerrain()ax.add_image(st, 4) 2. 行政区划及城市路网信息此处以四川内江市为例，利用osmnx库，shp文件自动下载于工作文件夹内。 1234567# 行政辖区下载import osmnx as oxcity = ox.gdf_from_place('Neijiang City, Sichuan, China')ox.save_gdf_shapefile(city)city = ox.project_gdf(city)fig, ax = ox.plot_shape(city, figsize=(3,3)) 1234# 内江市东兴区公路路网下载Road = ox.graph_from_place('Dongxing, Neijiang, Sichuan, China', network_type='drive')Road_projected = ox.project_graph(Road)fig, ax = ox.plot_graph(Road_projected) 3. 一些有用的小指令12345678910111213141516171819202122232425262728293031323334353637# A. 任意地点的海拔查询!pip install geocoderimport geocoderprint geocoder.elevation ([lat,lng]).meters# B. 两例获取某地区及周边的google地图静态图像## B1. 卫星图http://maps.googleapis.com/maps/api/staticmap?center=42.2341289,118.9790778&amp;zoom=12&amp;format=png&amp;sensor=false&amp;size=2400x2000&amp;maptype=satellite&amp;style=feature:administrative|weight:0.1|invert_lightneºss:true|visibility:off&amp;style=feature:water|element:labels.text|visibility:off&amp;style=feature:landscape.natural|visibility:on## B2. 道路地图http://maps.google.com/maps/api/staticmap?sensor=false&amp;size=512x512&amp;center=Brooklyn&amp;zoom=12&amp;style=feature:all|element:labels|visibility:off# C. google地图下载工具http://www.chengfolio.com/google_map_customizer#satellitemap# D. 地形图在线下载工具https://maps-for-free.com/ # E.测量两点间距离# source: https://stackoverflow.com/questions/15736995/how-can-i-quickly-estimate-the-distance-between-two-latitude-longitude-pointsfrom math import radians, cos, sin, asin, sqrtdef haversine(lon1, lat1, lon2, lat2): \"\"\" Calculate the great circle distance between two points on the earth (specified in decimal degrees) \"\"\" # convert decimal degrees to radians lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2 c = 2 * asin(sqrt(a)) # Radius of earth in kilometers is 6371 km = 6371* c return km","link":"/2017/11/06/7.%5BCode%5D%E5%9C%B0%E7%90%86%E5%9C%B0%E5%BD%A2%E6%95%B0%E6%8D%AE%E7%9A%84%E8%8E%B7%E5%8F%96%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"title":"Useful python scripts for daily uses","text":"Welcome to my collection of Python scripts and code snippets for everyday use. This document contains a variety of scripts and code snippets that can be used to automate repetitive tasks, process data, and perform various other operations. Whether you are a beginner or an experienced Python programmer, this collection of scripts and code snippets will provide you with useful tools and shortcuts for your everyday work. In this document, you will find code snippets and scripts for tasks such as: Data processing File manipulation Web scraping Automating tasks And more! This document is organized into sections for easy navigation, so you can quickly find the script or code snippet that you need. Many of them were collected from stackoverflow. On 2022.03.31, I re-organize all the scripts into Python 3.x version and post again.On 2023.02.12, I edited the whole content with help from ChatGPT 1.Data processing 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368#1. Find the duplicated values in one columnfrom collections import Counter print ([item for item, count in Counter(df[\"Sample\"].values).items() if count &gt;1] )#2. Build new dataframe based on the user-defined column names### N: lengthdef new_df(N, LIST): df = pd.DataFrame(np.nan, index=np.arange(0,N,1), columns=['A']) for i,t in enumerate(LIST): df[LIST[i]] = np.nan df = df.drop(['A'],1) #3. Change the integer to N digits ## method 1# Method 1df.MONTH.apply(\"{0:0=2d}\".format)# Method 2\"%02d\" &amp; (string)#4. Replace the values in specific columnspd.options.mode.chained_assignment = Nonedf.loc[condition function, 'column'] = xxx #5. Find the index of the nearest value to a pre-defined inputdef find_nearest(array,value): idx = (np.abs(array-value)).argmin() return array[idx] #6. Test a value if it is NaN: def isNaN(num): return num != num#7. Test dataframe if it is emptydf.empty==True#8. Zip two pandas columns into 2-d arrayfrom itertools import product c = list(product(df.lon.values, df.lat.values)) #9. Manipulate the string## Add an elementlist.insert(0, 'A')## Delete an element by indexlist.pop(2)## Delete an element by its valuelist.remove('A')#10. Assign value to specific location of a dataframe## Method 1 - the fastestdf.set_value('row index', 'column name', value)## Method 2 - moderate speeddf['column name']['row index'] = value## Method 3 - the slowestdf.at('column name', 'row index') = value#11. Group the dataframe with user-defined range settingbins = [0,50, 60, 70, 80, 90,100]group_names = ['&lt;50', '50-60', '60-70', '70-80','80-90','&gt;90']df['categories'] = pd.cut(df['RH'], bins, labels=group_names)## If we want to plot them group-name wisedfig = plt.figure()for i in range(0,len(categories.unique())-1,1): ddf = df[df['categories'] == categories.unique()[i]] plt.scatter(ddf['PM2.5'], ddf['VISIBILITY'], alpha = 0.45, label ='RH: '+ roup_names[i]) popt, pcov = curve_fit(func, ddf[\"PM2.5\"], ddf[\"VISIBILITY\"]) xdata = np.arange(0, 300, 4) plt.plot(xdata, func(xdata, *popt), zorder =3)plt.legend(ncol = 2) plt.show() #12. Transform the list to stringimport astx = u'[ \"A\",\"B\",\"C\",\"D\"]'x = ast.literal_eval(x)print x # Output: ABCD#13. Subset of pandas dataframe by datetime rangemask = (met['Date'] &lt; '2016-03-01')|(met['Date'] &gt;= '2016-12-01')met_sub = met.locmask#14. Sort one column by values of another column (ranking species by their concentrations)SE_COL = ['Cr', 'Co', 'Ni', 'Cu', 'Zn', 'As', 'Se', 'Mo', 'Cd', 'Pb', 'V', 'Mg', 'Ca', 'K', 'Ti', 'Mn', 'Fe', 'Ba', 'Sr']SE_COL = [SE_COL[t] for t in np.argsort(np.array([vals.dropna().mean() for col, vals in pm25_ef.items()]))]#15. Drop np.nan and then find the 10th percentilev = df['value'].notna()ra_ = df.loc[v, 'value'].valuesse_index = ra_.argsort().argsort()[:int(0.1len(df))]sorted(ra_)[int(0.1len(ra_))]#16. Rename the columns of dataframedf.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)#17. Delete specific columnscolumns = ['Col1', 'Col2', ...]df.drop(columns=columns, inplace=True, axis = 1)#18. Add new columnsdf[new_column_name] = pd.Series(data, index=df.index)#19. Delete the rows with np.nan## Method 1df = df[~np.isfinite('col1')]## Method 2df.dropna(subset=['col1'], inplace=True)#20. Sort the values from the biggestvar_exp = sorted(eigen_vals, reverse=True)#21. Saving a CSV file containing Chinese characters##To save a CSV file containing Chinese characters, you need to make sure that the file is encoded in UTF-8 format.import sysreload(sys)sys.setdefaultencoding('utf-8') df.to_csv('file.csv', encoding='utf-8-sig', index=False)#22. Adding a day to a datetime object## To add one day to a datetime object, you can use the pd.to_timedelta() function. For example:date = pd.to_datetime('2017-11-01')new_date = date + pd.to_timedelta(1, 'D')#23. Getting the keys and values of a dictionarykeys = [key for key, value in dict_.items()]values = [value for key, value in dict_.items()]#24. Binning data with natural breakfrom pysal.esda.mapclassify import Natural_Breaks as nbbreaks = nb(df.value,k=5)class_ = pd.DataFrame({'class': breaks.yb}, index=df[df['value'].notnull()].index)df = df.join(class_)df.class.fillna(-1, inplace=True)#25. Printing a pandas DataFrame in Markdown formatfrom tabulate import tabulateprint(tabulate(response_df, tablefmt=\"pipe\", headers=\"keys\", showindex=False))#26. saving the 2-d array in txt and read itnp.savetxt(\"mask_PM_0.1x0.1.txt\",mask)mask = np.loadtxt('./mask_PM_0.1x0.1.txt')#27. np.array dropna for the NaN of ratios t = np.array([(a/b) for a, b in zip(dyn_w[spec].values,dyn_s[spec].values)])t = t[~np.isnan(t)]# 28. How to select the data points within specific area### 长三角yrd_area = fiona.open(\"./data/shp/YRD/yrd_b.shp\")pol = yrd_area.next()from shapely.geometry import shapegeom = shape(pol['geometry'])poly_data = pol[\"geometry\"][\"coordinates\"][0][0]#[0:12]poly = Polygon(poly_data)yrd_mask = np.array([poly.contains(Point(x,y)) for x,y in zip(data_city['lon'],data_city['lat'],)])data_yrd = data_city[yrd_mask]data_yrd = data_yrd.reset_index()# 29. 存储中文xls报错问题处理df = pd.read_csv('./xxx.txt', delimiter = \"\\t\",encoding='utf-8')with pd.ExcelWriter('xxx.xls',options={'encoding':'utf-8'}) as writer: # df.to_excel(writer, sheet_name=u'sheet1')# 30. split the list by specfic string (e.g., quotes)[s for s in l.split('\"') if s.strip() != ''][1]# 31. replace specific values by condition df.loc[df['COLA']!='XXX','COLB']=yyy# 32. Select subset of netcdf file by specific attributesimport netCDF4 as ncval_ = ['HGT_M', 'XLONG_C','XLAT_C']with netCDF4.Dataset(\"geo_em.d01.nc\") as src, netCDF4.Dataset(\"out.nc\", \"w\") as dst: # copy global attributes all at once via dictionary dst.setncatts(src.__dict__) # copy dimensions for name, dimension in src.dimensions.items(): dst.createDimension( name, (len(dimension) if not dimension.isunlimited() else None)) # copy all file data except for the excluded for name, variable in src.variables.items(): if name in val_: x = dst.createVariable(name, variable.datatype, variable.dimensions) dst[name][:] = src[name][:] # copy variable attributes all at once via dictionary dst[name].setncatts(src[name].__dict__)# 33. combining the listscode_ = []for pr in pc_code: code_ = list(itertools.chain.from_iterable([code_,[t['code'] for t in pr['children']]])) # 34. 2-D数据插值#向新网格进行插值from scipy import interpolatehfunc = interpolate.interp2d(lon_dust_sub,lat_dust_sub,PM_dust)new_dust = np.zeros(len(lat_anthr_sub)*len(lon_anthr_sub))t = 0 for i in range(0,len(lat_anthr_sub),1): for j in range(0,len(lon_anthr_sub),1): new_dust[t] = hfunc(lon_anthr_sub[j],lat_anthr_sub[i]) t+=1dust_int = new_dust.reshape(len(lat_anthr_sub),len(lon_anthr_sub))# 35. 排序获得其序列indexsorted(range(len(s)), key=lambda k: s[k])# 39. Sort时注意排序sorter = source_df_contr_cr.source = df_contr_cr.source.astype(\"category\")df_contr_cr.source.cat.set_categories(sorter, inplace=True)df_contr_cr = df_contr_cr.sort_values([\"source\"]) df['categories'].value_counts().reindex(group_names)# 39. 分钟数据转化为小时数据df2_re = df2.groupby(np.arange(len(df2))//60.0).mean()# 40. 有效位数输出to_clipboard(float_format = '%.4f')# 41.自然基金报告下载import osfor i in range(0,100,1): os.system(\"wget http://output.nsfc.gov.cn/report/41/41205122_%i.png\" %i) import osimport natsortfrom fpdf import FPDFfiles = os.listdir('./')files = natsort.natsorted(files, reverse = False)files = files[0:]pdf = FPDF()for image in files[0:]: if image[-3:]=='png': pdf.add_page() pdf.image(image,0,0,210,297) pdf.output(\"../hh.pdf\", \"F\") # 43. 缺失值监测pd.isnull()dataframe[xxx].values.isnull()# 44. If Else in list comprehension[x for x in list(df.columns[2:]) if x not in ['重庆','山西','西藏']]# 45. Two variables in list comprehension (double iteration)# 46. assign two value to pandas columns with if condition.contr_df['type'] = contr_df['sample'].apply(lambda x:10 if x[0:7] ==\"IITK-20\" else \"x\")# 46. txt file reading with multiple delimiter and skiplinesdf = pd.read_csv('./data/210717_Lucknow_Day1.txt',delimiter='\\t',skiprows=[i for i in range(0,10)])# 47. add multiple lists as the columns of new dataframedpara = pd.DataFrame(list(zip(t_series,rn_series,minErr1ion)),columns=['t_series','rn_series','minErr1ion'])# 48. replace all the quotation mark in the columnspecies_name = [str(c.rstrip().split(\"\\t\")[1].replace('\"', '')) for c in species_info1]# 49. Kruskal-Wallis testfrom scipy import statsstats.kruskal(dd['Glass vial'],dd['Plastical vial'],dd['Dilution water'])# 50. list substractionlist(set( list(dafr_no3.columns)) - set(spike_cols)-set(['time'])# 51. Assign 2-d matrix into pandas dataframe with column name dafr_org = pd.DataFrame(org_, columns=df_org['Parameter'].attrs['ExactMassText'].astype(str))# 52. divide string based on chemical formula import rec = 'C2H3O'splitted = list(re.split(\"([A-Z][a-z]*)\",c))keyss = list(filter(lambda a: a[0].isupper() if a else False, splitted))values = list(filter(lambda a: a[0].isdigit() if a else False, splitted))spec_dict = dict(zip(keyss,values)) # 53. check string type in columnsna_em_power.applymap(type)==str# 54. Sorting files by specific strings witin the filenamedef last_4chars(x): return(x[-25:])sorted(imgs, key = last_4chars) # 55. split string by more than one delimitersimport rere.split('; |, ', string_to_split)# 56. Pandas rename multiple columnsdf_area.rename(columns=dict(zip(df_area.columns[:], mz_list)),inplace=True)# 57. Pandas check whether the whole column is np.nanfor c in mz_list[0:]: if df_area[c].isnull().all(): print(\"All values in the column 'B' are NaN\")# 58. Rename part of column names by conditiondef renaming_fun(a): if \"_unc_new_toc_scaled\" in a: return a.split('_')[0] +\"_unc\" # or None return aauto_df_fa.columns = [renaming_fun(col) for col in auto_df_fa.columns]# 58. Append 2-d array into 3-d array### https://stackoverflow.com/questions/43363641/building-a-3d-array-from-a-number-of-2d-arrays-with-numpyOC_emissions_daily = np.empty((1,720, 1440), int)daily_em_array = month_OC_emissions * daily_fractionOC_emissions_daily = np.vstack([OC_emissions_daily, daily_em_array[None]])# 59. processing pandas multiple columns df[df.columns[2:]] = df[df.columns[2:]].multiply(spi_conc/RIE/c, axis=\"index\")# 60. Lambda function### Apply function NumPy.square() to square the values of two rows 'A'and'Bdf2 = df.apply(lambda x: np.square(x) if x.name in ['A','B'] else x)# 61. Saving n-d array and reload via pickle methodimport numpy as npimport pickle# 62. create a large numpy arraydata = np.random.rand(1000, 1000, 100)# 63. save the numpy array to a file using picklewith open('data.pkl', 'wb') as f: pickle.dump(data, f)# 64. load the numpy array from the filewith open('data.pkl', 'rb') as f: loaded_data = pickle.load(f)# 65. check if the original and loaded numpy arrays are the sameprint(np.array_equal(data, loaded_data))# 66. Daily averagedf.set_index('datetime',inplace=True)df = df.rename(columns=new_column_names)daily_ave = df.resample('D').mean()# 67. Create a Pandas Excel writer using 'xlsxwriter' enginewith pd.ExcelWriter('my_excel_file.xlsx', engine='xlsxwriter') as writer: # Write dataframes df1 and df2 to two separate sheets in the same Excel file df1.to_excel(writer, sheet_name='Sheet1') df2.to_excel(writer, sheet_name='Sheet2') # 68. Simple way to classify chemical formuladef classify_chemicals(formula): if pd.isnull(formula): return np.nan else: elements = ['C', 'H', 'O', 'N', 'Na'\\] components = re.findall('\\[A-Z\\]\\[a-z\\]*', formula) # Split formula into components category = ''.join(\\[element for element in elements if element in components]) return category ds['Ion-rule1-class'] = ds['ion-rule1'].apply(classify_chemicals) 2. Data statistics 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#1. Curve fitting (y = ax + b)from scipy import statsimport numpy as npslope, intercept, r_value, p_value, std_err = stats.linregress(x,y)##1.1 remove the nan value first!mask = ~np.isnan(x_) &amp; ~np.isnan(y_)slope, intercept, r_value, p_value, std_err = stats.linregress(x_[mask], y_[mask])#2. Fitting by user-defined function from scipy.optimize import curve_fit def func(x, a,b): return ax**2+bx#+0.55def get_r(f,x,y,popt): residuals = y- f(x, *popt) ss_res = np.sum(residuals**2) ss_tot = np.sum((y-np.mean(y))**2) r_squared = 1 - (ss_res / ss_tot) return r_squaredpopt, pcov = curve_fit(func, df['x'], df['y']) print \"## f(x) = ax*2 +bx ##\"print \"## a##\"print poptprint \"## r-square ##\"print get_r(func,df['x'], df['y'], popt )#3. pivot table df.groupby(['site']).get_group(df.site.unique()[i])['value'].mean()#4. Summarize the frequencyclu_d['Cluster'].value_counts().to_frame()clu_d['Cluster'].value_counts().index.tolist() #5. one-way ANOVA test# 如何统计 seasonal anova differencespec_ = ['crustal','trace','OM',\"EC\",'SO42-','NO3-','Cl-','NH4+','n_K',]val_annual,val_sp,val_su,val_au,val_wi = [],[],[],[],[]for t in sp_pie: val_sp=df[df.season=='Spring'][t].dropna().values val_su=df[df.season=='Summer'][t].dropna().values val_au=df[df.season=='Autumn'][t].dropna().values val_wi=df[df.season=='Winter'][t].dropna().values print t print stats.f_oneway(val_sp,val_su, val_au,val_wi)# pearson correlation mask = ~np.isnan(df['Ma_re'])&amp;~np.isnan(df['PM25'])stats.pearsonr(df['Ma_re'][mask],df['PM25'][mask])# 6. D-N averagedsample_id = df_op[df_op['Sample ID'].str[-1] !='C']['Sample ID'].values#IITK-2018-02-12-N #IITK-2018-01-21-Nsample_id = [c for c in sample_id if c not in ['IITK-2018-02-12-N' ,'IITK-2018-01-21-N',]]df_op_c = df_op[~(df_op['Sample ID'].isin(sample_id))]df_op_c = df_op_c[df_op_c['Sample ID']!='IITK-2018-11-01-C']df_op_dn = pd.DataFrame()df_op_dn['Sample ID'] = [c[:-2]+'-C' for c in sample_id]df_op_dn['Date'] = [c[5:-2] for c in sample_id]for co in df_op.columns[2:]: val_ = [] for c in sample_id: val_.append(df_op[df_op['Sample ID'].str[:-2] == c[:-2]][co].mean()) df_op_dn[co] = val_ df_op_new = pd.concat([df_op_dn,df_op_c]).reset_index(drop=True) ## OM:OCfrom pyvalem.formula import Formuladef cal_omoc(ion): f = Formula(ion) atom_dict = f.atom_stoich n_c = atom_dict['C'] omoc_ratio = f.mass/(n_c*12.0) return omoc_ratiopro_pr['OM:OC'] = pro_pr['Ion'].apply(lambda ion: cal_omoc(ion)) 3. Data visualization 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537#1. Temperature symbolax.set_ylabel(u'Temperature (\\N{DEGREE SIGN}C)', fontweight='bold')#2. Match the length of colorbar with the figurefrom mpl_toolkits.axes_grid1 import make_axes_locatabledivider = make_axes_locatable(ax1)cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)cbar = plt.colorbar(cf, cax=cax) cbarlabel = \"xxxx\"cbar.ax.set_xlabel(cbarlabel,size = 8,labelpad=-35)#3. User-defined colorbar location and ticklabel edittingcbaxes = fig.add_axes([0.83, 0.125, 0.1, 0.01]) cbar = plt.colorbar(ss,cax=cbaxes,orientation='horizontal')loc_ = np.array([5,10,15,20,25])cbar.set_ticks(loc_)cbar.set_ticklabels(loc_)## two methods for ticklabel fontsize(1) cbar.ax.tick_params(labelsize=8)(2) cbar.ax.set_xticklabels(cbar.ax.get_xticklabels(), fontsize=8)cbar.ax.set_xlabel(r'$\\mathregular{OC/EC}$',fontsize = 8,labelpad = -28)#4. Chinese settingfrom matplotlib.font_manager import FontProperties mpl.rcParams['font.sans-serif'] = ['Microsoft YaHei']# 微软雅黑的中英文混排效果较好plt.rcParams['axes.unicode_minus'] = False #5. User-defined axes settingdef stylize_axes(ax): ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.xaxis.set_tick_params(top='off', direction='out', width=1) ax.yaxis.set_tick_params(right='off', direction='out', width=1)#6. Plotting the first-order regression line## (x,y) is the original datasetsfrom scipy import statsslope, intercept, r_value, p_value, slope_std_error = stats.linregress(x, y)xx = np.arange(x.min(),x,max(),0.01)yy = slope*xx+interceptclabel = # make the font itatilc styleplt.plot(xx,yy,color ='k', lw =1.2, linestyle = \"-\", label = clabel)#7. Setting the two subplots shared the same aspect ratio## Noted that when one subplot using basemap or cartopy (geo-based coordinate), and the other subplot is in basic matplotlib style, there aspect ratio would not be the same. def get_aspect(ax): xlim = ax.get_xlim() ylim = ax.get_ylim() aspect_ratio = abs((ylim[0]-ylim[1]) / (xlim[0]-xlim[1])) return aspect_ratiofig = plt.figure(figsize=(12,8))ax1=plt.subplot(121, projection=ccrs.PlateCarree())ax2=plt.subplot(122)ax2.set_aspect(get_aspect(ax1) / get_aspect(ax2))#8. Fake legend markers#参考 https://jakevdp.github.io/PythonDataScienceHandbook/04.06-customizing-legends.htmlfor area in [100, 300, 500]: plt.scatter([], [], c='k', alpha=0.3, s=area, label=str(area) + ' km$^2$')plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='City Area')#9. Remove the plotting items and retain the label#10. Remove the tick lines while retain the ticklabelsax.tick_params(axis=u'both', which=u'both',length=0)#11. plotting two subplots with different sizefig = plt.figure(figsize=(7,4))##definitions for the axesleft, width = 0.07, 0.6bottom, height = 0.1, .8bottom_h = left_h = left+width+0.05rect_cones = [left, bottom, width, height]rect_box = [left_h, bottom, 0.5, height]ax1 = plt.axes(rect_cones,projection=ccrs.PlateCarree())ax2 = plt.axes(rect_box)ax2.set_aspect(get_aspect(ax1) / get_aspect(ax2))#12. fake circle legenddef plot_legend(title): s = [1,3,5,7,9] c = plt.cm.binary(np.arange(5)/5.0) labels =['A',\"B\",'C',\"D\",\"E\"] cir1 = plt.scatter([1], [2], c=c[0], alpha=0.8, s=s[0],label=r'&lt;0.5') cir2 = plt.scatter([1], [2], c=c[1], alpha=0.8, s=s[1],label=labels[1]) cir3 = plt.scatter([1], [2], c=c[2], alpha=0.8, s=s[2],label=labels[2]) cir4 = plt.scatter([1], [2], c=c[3], alpha=0.8, s=s[3],label=labels[3]) cir5 = plt.scatter([1], [2], c=c[4], alpha=0.8, s=s[4],label=labels[4]) leg = plt.legend([cir1,cir2,cir3,cir4,cir5],labels,scatterpoints = 1, \\ frameon=False, labelspacing=0.9, ncol =2, fontsize=8,title=title , loc = [0.05,0.1]) #13. seperated colors from colormapc_list = plt.cm.rainbow(np.arange(6)/6.0)# 14. set twin axis with different coloraxu = ax.twinx()axu.spines[\"right\"].set_position((\"axes\", 1.25))so_,=plt.plot(xxx)axu.yaxis.label.set_color(so_.get_color())axu.spines['right'].set_color(so_.get_color())axu.spines[\"right\"].set_edgecolor(so_.get_color())axu.tick_params(axis='y', colors=so_.get_color())# 15. add a rectanglefrom matplotlib.patches import Rectangleimport matplotlib.patches as mpatchesrec = mpatches.Rectangle((position[i]- width/2.0,bot_[i]),width,hei_[i],linewidth=1,edgecolor='b',facecolor='none',zorder=12) ax.add_patch(rec)# 16. output the color by pre-defined alphadef make_rgb_transparent(rgb, bg_rgb, alpha): return [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)]from matplotlib import colorsimport matplotlib.pyplot as pltalpha = 0.5kwargs = dict(edgecolors='none', s=3900, marker='s')for i, color in enumerate(['red', 'blue', 'green']): rgb = colors.colorConverter.to_rgb(color) rgb_new = make_rgb_transparent(rgb, (1, 1, 1), alpha) print(color, rgb, rgb_new) plt.scatter([i], [0], color=color, **kwargs) plt.scatter([i], [1], color=color, alpha=alpha, **kwargs) plt.scatter([i], [2], color=rgb_new, **kwargs)# 17. terrain mapdef truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100): new_cmap = colors.LinearSegmentedColormap.from_list( 'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n))) return new_cmapcmap = plt.get_cmap('terrain')terrain_cmap = truncate_colormap(cmap, 0.15, 0.9) # 18. 不等距subplotsimport matplotlib.gridspec as gridspecfig = plt.figure(figsize=(9,6))gs = gridspec.GridSpec(46,1)row_xx = 23# pm25_pm10ax1 = plt.subplot(gs[0:row_xx, 0])# 19. two axis one legend# ask matplotlib for the plotted objects and their labelslines, labels = ax.get_legend_handles_labels()lines2, labels2 = ax2.get_legend_handles_labels()ax2.legend(lines + lines2, labels + labels2, loc=0)# 20. Cartopy longitude## 方法1ax.set_xticks([0, 60, 120, 180, 240, 300, 360], crs=ccrs.PlateCarree())dax.set_yticks([-90, -60, -30, 0, 30, 60, 90], crs=ccrs.PlateCarree())lon_formatter = LongitudeFormatter(zero_direction_label=True)lat_formatter = LatitudeFormatter()ax.xaxis.set_major_formatter(lon_formatter)ax.yaxis.set_major_formatter(lat_formatter)## 方法2ax.set_xticks(np.arange(extent[0]+2, extent[1]+2, 15), crs=ccrs.PlateCarree())ax.set_yticks(np.arange(extent[2]+3, extent[3], 15), crs=ccrs.PlateCarree())ax.set_xticklabels([r'$\\mathrm{75^o E}$',r'$\\mathrm{90^o E}$',r'$\\mathrm{105^o E}$',\\ r'$\\mathrm{120^o E}$',r'$\\mathrm{135^o E}$',])ax.set_yticklabels([r'$\\mathrm{20^o N}$',r'$\\mathrm{35^o N}$',r'$\\mathrm{50^o N}$'])# 21 'GeoAxesSubplot' object has no attribute '_hold'from matplotlib.axes import Axesfrom cartopy.mpl.geoaxes import GeoAxesGeoAxes._pcolormesh_patched = Axes.pcolormesh# 22.中英文混排# # -*- coding: utf-8 -*-import matplotlib.pyplot as pltfrom matplotlib.font_manager import FontPropertiessong_ti = FontProperties(fname=r'/Library/Fonts/Songti.ttc', size=20)times_new_roman = FontProperties(fname=r'/Library/Fonts/Arial Black.ttf', size=15)ax = plt.gca()ax.set_title(u'能量随时间的变化', fontproperties=song_ti)ax.set_xlabel('Time (s)', fontproperties=times_new_roman)ax.set_ylabel('Energy (J)', fontproperties=times_new_roman)plt.show()# 23. no-legend(label='_nolegend_')# 25. plot with grey backgroundax.set_facecolor(\"#F5F5F5\")# 26. Midpoint-normalizedimport matplotlib.colors as colors#https://stackoverflow.com/questions/25500541/matplotlib-bwr-colormap-always-centered-on-zeroclass MidpointNormalize(colors.Normalize): def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False): self.midpoint = midpoint colors.Normalize.__init__(self, vmin, vmax, clip) def __call__(self, value, clip=None): # I'm ignoring masked values and all kinds of edge cases to make a # simple example... x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1] return np.ma.masked_array(np.interp(value, x, y))# 27. 月份colorbarcbaxes = fig.add_axes([0.29, 0.27, 0.18, 0.02]) cbar = plt.colorbar(ss,cax=cbaxes, orientation='horizontal')n = 6st_po = []for i in range(0,n,1): st_po.append(np.array_split(pd.to_datetime(sorted(date_point)),n)[i].values[0])cb_ticks = [float(c) for c in st_po]cbar.ax.set_xticklabels(pd.to_datetime(cb_ticks).strftime('%b')) #B cbar.ax.tick_params(labelsize=8.5) # 28. nclcmapsimport nclcmapsnclcmaps.cmaps(\"precip2_17lev\")# 29. 创建新的colormap cm = LinearSegmentedColormap.from_list('test',plt.cm.BuPu(np.arange(6)/6.0)[1:], N=5) # 30. 最好的terrain maphttp://chris35wills.github.io/discrete_colourbar/# 31. 对数坐标系统显示实际数值from matplotlib.ticker import StrMethodFormatter, NullFormatterax.yaxis.set_major_formatter(StrMethodFormatter('{x:.3f}'))ax.yaxis.set_minor_formatter(NullFormatter()) # 32. loop figure for subplotfig, axs = plt.subplots(2,4, figsize=(15, 6), facecolor='w', edgecolor='k')#subplot_kw={'projection': ccrs.PlateCarree()}fig.subplots_adjust(hspace = .5, wspace=.001)axs = axs.ravel()for i in range(8): axs[i].contourf(np.random.rand(10,10),5,cmap=plt.cm.Oranges) axs[i].set_title(str(250+i))# patch_box from matplotlib.collections import PatchCollectionfrom matplotlib.patches import Rectangledef PATCH_BOX(ax,pos,wid): for x in pos[::2]: width = wid/2.0 facecolor = \"#F0F0F0\" rect = Rectangle((x-width, 0.00), width*4.0,1000000,facecolor=facecolor,linewidth = 0)# clip_on=False, ax.add_patch(rect) # 33. cartopy extent 报错问题https://github.com/SciTools/cartopy/issues/837pip uninstall shapely &amp;&amp; pip install --no-binary :all: shapely# 34. shapely的向量化总是报错# https://github.com/Toblerity/Shapely/issues/810pip uninstall shapely &amp; pip install shapely --no-binary shapely==1.7a2 # 35. scatter图的外环linewidths=1# 36. discrete color from continuous colormapfrom matplotlib import cmcs=cm.Set2(np.arange(4)/4.)# 36. how to plot circle on unequal axes ax.plot(0.3,-0.1 , 'ro', fillstyle='full', markersize=5, transform=ax.transAxes,clip_on =False)# 37. Plotting multi-polygons with pre-defined color lists.# 38. Plotting scatter plimport matplotlib.dates as mdates# Method# In method 1, the color for each data point is determined by the date value in date_point. The date values are converted to numerical values using mdates.date2num() before passing to ax.scatter(). The colorbar ticks and labels are set using mdates.AutoDateLocator() and mdates.ConciseDateFormatter(). 1fig1 = plt.figure(figsize=(5, 5))ax1 = fig1.add_subplot()date_point = df['Date']ss1 = ax1.scatter(df['WSOC'], df['OC'], c=mdates.date2num(date_point), cmap=plt.cm.coolwarm, s=100)cbaxes1 = fig1.add_axes([0.29, 0.27, 0.48, 0.02]) cb1 = plt.colorbar(ss1, cax=cbaxes1, orientation='horizontal')loc1 = mdates.AutoDateLocator(maxticks=12)cb1.ax.xaxis.set_major_locator(loc1)cb1.ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(loc1))# Method 2# Method 2 uses the datetime values directly as color values, which means the colorbar ticks will match the dates exactly. It manually divides the range of dates into 5 segments and creates tick labels with the abbreviated month names.fig2 = plt.figure(figsize=(5, 5))ax2 = fig2.add_subplot()date_point = df['Date']ss2 = ax2.scatter(df['WSOC'], df['OC'], c=date_point, cmap=plt.cm.coolwarm, s=100)cbaxes2 = fig2.add_axes([0.29, 0.27, 0.48, 0.02]) cbar2 = plt.colorbar(ss2, cax=cbaxes2, orientation='horizontal')n = 5st_po = []for i in range(0, n, 1): st_po.append(np.array_split(pd.to_datetime(sorted(date_point)), n)[i].values[0])cb_ticks = [float(c) for c in st_po]cbar2.ax.set_xticklabels(pd.to_datetime(cb_ticks).strftime('%b')) #B cbar2.ax.tick_params(labelsize=8.5)#39. plotting color palette based on user-defined color listfrom matplotlib.colors import ListedColormapimport matplotlib as mplfig = plt.subplots(figsize=(10,2))ax = plt.subplot(111)color_list = ['#cccccc', '#da6701','#00994d','#00cccc', '#0000ff','#000099','#c9ace6','#9039e6','#000000']cmap_diy = ListedColormap(color_list, color_list)col_map = cmap_diynew_val = []ticks = np.linspace(0.0,1, len(color_list)+1)for i in range(0,len(ticks)-1,1): new_val.append((ticks[i]+ticks[i+1])/2.0)cbar = mpl.colorbar.ColorbarBase(ax, cmap=col_map, orientation = 'horizontal', ticks =new_val, alpha = 0.75)cbar.ax.set_xticklabels(color_list, fontsize =12, rotation = 30) ttl = plt.title('EESI Color Palette 1',fontweight=\"bold\",fontsize =18,)ttl.set_position([.5, 1.15])plt.tight_layout()#40. Boxplotdef box_plot(ax, data, pos, color): bp = ax.boxplot(data, sym='', whis=[5, 95], widths=(len(data) + 4) / (2 * len(data)) * 0.7, positions=pos, boxprops=dict(facecolor=color, edgecolor=color), medianprops=dict(color='k'), whiskerprops=dict(color='k'), capprops=dict(color='k'), patch_artist=True) for box in bp['boxes']: box.set(facecolor=color) return bp#41. Linear fitting in short# Remove NaN values from the dataimport statsmask = ~np.isnan(x) &amp; ~np.isnan(y)x_clean = x[mask]y_clean = y[mask]# Perform linear regression and calculate the slope and interceptslope, intercept, r_value, p_value, std_err = stats.linregress(x_clean, y_clean)# Create a scatter plot with error bars using matplotlibfig, ax = plt.subplots()ax.scatter(x_clean, y_clean, s=50, edgecolors='black', alpha=0.8)# Add the linear fit lineax.plot(x_clean, slope*x_clean + intercept, color='red', linestyle='-', label='Linear Fit')# Annotate the fitted equationax.text(0.05, 0.95, 'y = {:.2f}x + {:.2f}'.format(slope, intercept), transform=ax.transAxes, fontsize=14,horizontalalignment='left',\\ verticalalignment='top', bbox=dict(facecolor='white',edgecolor='white', alpha=0.8))#42. Color-coded with datesfig = plt.figure(figsize=(12,4))c_time = mdates.date2num(auto_df_dl['Date'])ss = plt.scatter(pd.to_datetime(auto_df_dl['sample_starting_time']), auto_df_dl['130.15903'], c=c_time,cmap = plt.cm.Spectral_r)cbaxes = fig.add_axes([0.61, 0.75, 0.25,0.02]) cb = plt.colorbar(ss, cax=cbaxes, orientation='horizontal')loc = mdates.AutoDateLocator(maxticks=12)cb.ax.xaxis.set_major_locator(loc)cb.ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(loc))# 43. Add tooltips to the scatter plotslabels = [\"%.2f\" % (c) + tof_ion[i] for i, c in enumerate(tof_mz.values)]tooltip = mpld3.plugins.PointLabelTooltip(ax.collections, labels=labels)mpld3.plugins.connect(fig, tooltip)# 44. Horizontial and vertical color barcbaxes = fig.add_axes([0.775, 0.2, 0.11, 0.015]) ticks = [50,100, 250,500] cbar = plt.colorbar(ss_,ticks =ticks, cax=cbaxes,orientation='horizontal', )cbar.ax.set_xticks(ticks) # set the tick locationscbar.ax.set_xticklabels(tick_labels, fontsize=7) # set the tick labelscbar.ax.set_xlabel('FRP (MW)',fontsize = 8, labelpad = -25)cbar.ax.tick_params(color='#FFFFFF', direction='in')# 45. Dendragramdef plot_matrix_dendrogram_cn(corr_,va): D = corr_ ax1 = fig.add_axes([0.9,0.15,0.11,0.7*7/6.0]) Y = sch.linkage(D, method='ward') #method='centroid')ward set_link_color_palette(line_color) # Temporarily override the default line width: with plt.rc_context({'lines.linewidth': 1.25}): Z1 = sch.dendrogram(Y, orientation='right',above_threshold_color='grey', color_threshold=va, ) ax1.set_xticks([]) ax1.set_yticks([]) ax1.axis('off') axmatrix = fig.add_axes([0.05,0.15,0.70,0.7*7/6.0]) idx1 = Z1['leaves'] idx2 = Z1['leaves'] D = D[idx1,:] D = D[:,idx1] mask = np.tri(D.shape[1], k=-1) A = np.ma.array(D, mask=mask) cmap = plt.cm.YlOrRd# cmap = cm.get_cmap('YlGnBu') #YlOrRd cmap.set_over('#C0C1C0') im = axmatrix.matshow(A, aspect='auto', origin='lower', cmap=cmap,vmax = 0.999, )#Spectral_r# im = axmatrix.pcolormesh(A, cmap=cmap,vmax = 0.999, edgecolors ='grey',lw = 0.0)#Spectral_r # axmatrix.set_xticks(range(len(ele_name))-0.5) axmatrix.set_xticks(np.arange(0,len(ele_name),1) - 0.5) site_X = [ele_name[i] for i in idx1] axmatrix.set_xticklabels(site_X, minor=False, va=\"baseline\") axmatrix.xaxis.set_label_position('bottom') axmatrix.xaxis.tick_bottom() pylab.xticks(fontsize=8,) axmatrix.tick_params(axis='x', pad=60,length=0,rotation = 45) axmatrix.tick_params(axis='y', pad=2,length=0) ind_array = np.arange(0,len(ele_name),1) x, y = np.meshgrid(ind_array, ind_array) for i in range(0,len(ele_name),1): for j in range(0,len(ele_name),1): if math.isnan(A[j,i]) == 0: axmatrix.text(i, j, str(round(A[j,i], 2)), color = 'black',va='center', ha='center', fontsize = 12) axcolor = fig.add_axes([0.05,0.7,0.2,0.015]) cbar = pylab.colorbar(im, cax=axcolor,orientation='horizontal' ) cbar.set_label(r'$\\mathregular{{Pearson^{\\prime}}s\\ r}$', labelpad = -35, fontsize = 10) loc_ = np.array([0.2,0.5,0.8]) cbar.set_ticks(loc_) ttt = [0.2,0.5,0.8] cbar.set_ticklabels(ttt,) cbar.ax.tick_params(color=\"w\", direction='in',labelsize=10)# length axmatrix.set_yticks(range(len(ele_name))) site_Y = [ ele_name[i] for i in idx1] axmatrix.set_yticklabels(site_Y, minor=False, fontsize =8) axmatrix.yaxis.set_label_position('right') axmatrix.yaxis.tick_right() pylab.yticks( fontsize=8) axmatrix.spines['left'].set_visible(False) axmatrix .spines['top'].set_visible(False) def fitting_line(x,y): mask = ~np.isnan(x) &amp; ~np.isnan(y) x_clean = x[mask] y_clean = y[mask] slope, intercept, r_value, p_value, std_err = stats.linregress(x_clean, y_clean) annotation = f\"y = {slope:.2f}x + {intercept:.2f}\\n\\n\" + f\"r = {r_value:.2f}\" ax.annotate(annotation, xy=(0.05, 0.7), xycoords='axes fraction', fontsize = 10) # the version without slope # x_clean = x_clean[:,np.newaxis] # a, _, _, _ = np.linalg.lstsq(x_clean, y_clean) # print (a,) # annotation = f\"y = {a[0]:.2f}x\" # ax.annotate(annotation, xy=(0.15, 0.7), xycoords='axes fraction', fontsize = 10)## 46. Venn plot for 2 elementslist_a,list_b = owb_poa,owb_soaions_intersection = list_a[list_a['Ion'].isin(list_b['Ion'])]['Ion'].unique()ions_only_a = list_a[~list_a['Ion'].isin(list_b['Ion'])]['Ion'].unique()ions_only_b = list_b[~list_b['Ion'].isin(list_a['Ion'])]['Ion'].unique()from matplotlib_venn import venn2# Determine the countsnum_ions_in_a_not_b = len(ions_only_a)num_ions_in_b_not_a = len(ions_only_b)num_ions_intersection = len(ions_intersection)# Plot Venn Diagramplt.figure(figsize=(6, 5))venn2(subsets=(num_ions_in_a_not_b,num_ions_in_b_not_a, num_ions_intersection), set_labels=('Open wood burning POA','Open wood burning SOA'), set_colors=('#5477b1', '#f5a34f'))# plt.title(\"Orbitrap peak list\")plt.savefig('./Figures/20240423/Orbitrap_fitted_ions_OWB_POA+SOA.png', dpi = 400)plt.tight_layout()plt.show()## 47. Venn plot for 3 elementsdef only_in_target(tar,ref1,ref2): list1= (tar[tar['Ion'].isin(ref1['Ion'])]['Ion'].unique()) list2= (tar[tar['Ion'].isin(ref2['Ion'])]['Ion'].unique()) list_tot = np.unique(np.append(list1,list2)) return tar[~tar['Ion'].isin(list_tot)]['Ion'].unique()# Calculate intersectionsions_intersection_ab = list_a[list_a['Ion'].isin(list_b['Ion'])]['Ion'].unique()ions_intersection_ac = list_a[list_a['Ion'].isin(list_c['Ion'])]['Ion'].unique()ions_intersection_bc = list_b[list_b['Ion'].isin(list_c['Ion'])]['Ion'].unique()# Intersection of all three listsions_intersection_abc = list_a[list_a['Ion'].isin(ions_intersection_bc)]['Ion'].unique()# Calculate unique ionsions_only_a = only_in_target(list_a,list_b,list_c)ions_only_b = only_in_target(list_b,list_a,list_c)ions_only_c = only_in_target(list_c,list_a,list_b)# Unique in a, b and c but not in the intersection of all threeions_unique_a = np.setdiff1d(ions_only_a, ions_intersection_abc)ions_unique_b = np.setdiff1d(ions_only_b, ions_intersection_abc)ions_unique_c = np.setdiff1d(ions_only_c, ions_intersection_abc)# Calculate lengthsnum_ions_in_a_not_bc = len(ions_unique_a)num_ions_in_b_not_ac = len(ions_unique_b)num_ions_in_c_not_ab = len(ions_unique_c)num_ions_intersection_ab = len(ions_intersection_ab) - len(ions_intersection_abc)num_ions_intersection_ac = len(ions_intersection_ac) - len(ions_intersection_abc)num_ions_intersection_bc = len(ions_intersection_bc) - len(ions_intersection_abc)num_ions_intersection_abc = len(ions_intersection_abc)from matplotlib_venn import venn3plt.figure(figsize=(10, 6))v = venn3(subsets=(num_ions_in_a_not_bc, num_ions_in_b_not_ac, num_ions_intersection_ab, num_ions_in_c_not_ab, num_ions_intersection_ac, num_ions_intersection_bc, num_ions_intersection_abc), set_colors=('#a6cee3', '#d83f3f', '#405f3a'), set_labels=('All samples Merged (2371 ions)', ' Coal burning SOA (616 ions)', ' Open burning SOA (858 ions)'))# change the edgecolor to blackfor area in v.patches: if area: area.set_edgecolor('k')plt.tight_layout()plt.savefig(\"./Figures/20240423/Combined_coal_POA_owb_SOA_All.png\", dpi = 400)plt.show() 4. Jupyter and Python setting 123456789101112131415161718192021222324252627282930313233343536#1. Full width displaying all the timefrom IPython.core.display import display, HTMLdisplay(HTML(\"&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;\"))#2. Import script.py filesimport os,sysscriptpath = \"/Users/HYF/Downloads/stackoverflow_tag_cloud-master/\"##Add the directory containing your module to the Python path (wants absolute paths)sys.path.append(os.path.abspath(scriptpath))## Do the importimport stackoverflow_users_taginfofrom stackoverflow_users_taginfo import taginfoinfo = taginfo(link = 4918632, num_tags = 400)WordCloud().generate_from_frequencies(info).to_image().save('TagCloud.pdf')#3. Check the installing pathimport timetime.file#4. Get the pathname of the filescript_dir = os.path.dirname(os.path.realpath(filename)) + os.sep #5. Change the size of the uploaded figure&lt;img src=\"https://i.stack.imgur.com/kK1LC.png\" width=\"100\"&gt;#6. Inquire the information of computer, Python and its package using watermark packagepip install watermark%load_ext watermark ## Compiler, system, and CPU%watermark ## numpy version%watermark -p numpy# 7. 取消Warnings显示import warningswarnings.filterwarnings('ignore') &nbsp;5. Some tex characters 1\\leq &lt; 6. Spatial analysis 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# 1. intersection of two linesimport fionafrom shapely.geometry import shapefrom shapely.geometry import LineStringline1 = fiona.open(fname)# fname is the path to a specific polyline-format shapefile.line1 = shape(line1.next()['geometry']) start_point, end_point = (111,22),(33,33)line2 = LineString([start_point, end_point])print line1.intersection(line2)print line1.intersection(line2).is_empty# 2. Read .tifimport warningsimport rasteriowarnings.filterwarnings('ignore')import skimage.transform as st pathToRaster =r'/Users/HYF/Downloads/2014/per2014/中国年均降水.tif'xs = np.array([100.5, 12.0])ys = np.array([2.5, 42.0])# src.boundswith rasterio.open(pathToRaster) as src: arr = src.read() arr[arr&lt;0] = np.nan arr = arr[0,:,:] rows, cols = rasterio.transform.rowcol(src.transform, xs, ys) # 3. mask point seriesmask_= []for i in range(0,len(df_fire_cn),1): xy_point = gpd.geoseries.Point(df_fire_cn.lon.iloc[i],df_fire_cn.lat.iloc[i]) res = geom.contains(xy_point) mask_.append(res)df_fire_cn = df_fire_cn[mask_] # 4. Classify scatter point based on polylinesimport warningswarnings.filterwarnings('ignore')from shapely.geometry import shapefrom shapely.geometry import LineString# loading the boundary layerimport fionafname = '/Users/HYF/Dropbox/data/geo/中国地图/供暖分界/N-S_boundary.shp'line1 = fiona.open(fname)line1 = shape(line1.next()['geometry']) # set a end point which is the southernmost for all stations. end_point = (dy[dy['lat']==dy['lat'].min()]['lon'].values[0],dy[dy['lat']==dy['lat'].min()]['lat'].values[0])# loop all monitoring stations for classificationdy['NS']= np.nanfor i in range(0,len(dy),1): start_point = (dy['lon'].iloc[i],dy['lat'].iloc[i]) line2 = LineString([start_point, end_point]) if line1.intersection(line2).is_empty: dy[\"NS\"].iloc[i]='S' else: dy[\"NS\"].iloc[i]='N' # 5. transform shp file to geojsonimport fionaimport json# Open the shapefile using fionawith fiona.open(\"shapefile.shp\") as src: # Create a new GeoJSON file with open(\"shapefile.geojson\", \"w\") as output: # Write the GeoJSON representation of the shapefile to the file output.write(json.dumps(list(src)))# referencing https://gist.github.com/pelson/9785576import fionaimport shapely.vectorizedfrom shapely.geometry import shapefname = r'xxx/china.shp'cn_area = fiona.open(fname)pol = cn_area.next()geom = shape(pol['geometry'])## four corner x0,x1 = geom.bounds[0],geom.bounds[2]y0,y1 = geom.bounds[1],geom.bounds[3]## creating the mask arrayx = np.linspace(x0,x1,30)y = np.linspace(y0,y1,20)xx,yy = np.meshgrid(x,y)import timestart = time.time()mask_ = shapely.vectorized.contains(geom, xx,yy)print \"Process time: \" + str(time.time() - start)## 出现错误: 'Polygon' object is not iterable shapeFeature([c.geometry]) # when nothing showed up in Plotly import plotly.io as piopio.renderers.default = \"notebook\" # or \"jupyterlab\" 7. Other functions 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# 1.Crop PDF file# https://pypi.org/project/pdfcropper/#filespdf-crop-margins -v -s -u /Users/HYF/Dropbox/thesis//5_全国时空分析/output/Sec3/As_空间差异图.pdf# 2. When the self-installed files were unable to be importedpython setup.py buildpython setup.py install## Then, install the package and egg into&gt;/Users/HYF/anaconda3/lib/python3.7/site-packages#3. Enabling code folding in Jupyter labsetting/notebook{ \"codeCellConfig\": { \"codeFolding\": true }}#4. How to search for content in any notebook on local PC by keywordsimport globpattern = './../../../../**/*.ipynb'query = 'EESI_inforamtion_example'for filepath in glob.iglob(pattern, recursive=True): with open(filepath) as file: s = file.read() if (s.find(query) &gt; -1): print(filepath) # 5. pandas function for reading markdown table# Paste in your table and assign it to a variablemd_table_string = \"\"\"| Risk Factor | Estimated Annual Number of Deaths ||---|---|| Air pollution (outdoor &amp; indoor) | 1,670,000 || High blood pressure | 1,470,000 || High blood sugar | 1,120,000 || Smoking | 1,010,000 || Outdoor particulate matter pollution | 979,682 || Indoor air pollution | 630,093 || Obesity | 606,890 || Unsafe water source | 508,290 || Low birth weight | 579,108 || Alcohol use | 343,695 || Unsafe sanitation | 258,257 || Diet low in whole grains | 256,232 || Diet low in fruits | 240,098 || Secondhand smoke | 174,797 || Diet high in sodium | 173,939 || No access to handwashing facility | 164,172 || Diet low in nuts and seeds | 134,007 || Diet low in vegetables | 118,682 || Low bone mineral density | 118,283 || Low physical activity | 96,012 || Drug use | 72,903 || Child stunting | 17,973 || Iron deficiency | 10,098 || Non-exclusive breastfeeding | 1,501 |\"\"\"from io import StringIOdeath_factor = pd.read_csv( StringIO(md_table_string.replace(' ', '')), # Get rid of whitespaces sep='|', index_col=1).dropna( axis=1, how='all').iloc[1:]## mount the external drivesudo umount /mnt/nsudo mount -t drvfs -o metadata,uid=1000,gid=1000 N: /mnt/n## unzip all files within the folder#!/bin/bash \\# Navigate to the directory containing the zip files cd /path/to/your/directory # Loop through all files with .z* extensions and unzip them for file in 248650.z*; do echo \"Unzipping $file...\" unzip \"$file\" -d ./unzip/ \\# Unzip to a subdirectory called 'unzipped_files' done echo \"All files have been unzipped.\"","link":"/2018/07/22/9.%5BCode%5DUseful%20python%20scripts%20for%20daily%20use/"},{"title":"WRF post processing 2: Xarray tricks","text":"Xarray provides pandas-level convenience for working with ultidimensional data. Xarray has two fundamental data structure: a DataArray, which holds a single multi-dimensional variable and its coordinates;a Dataset, which holds multiple variables that potentially share the same coordinates. Moreover, a DataArray has four attributes: values: a numpy.ndarray holding the array’s values (矩阵数值，例如地表温度具体数值) dims: dimension names for each axis (e.g., (‘x’, ‘y’, ‘z’)) (维度名称，如经度、纬度、垂直分层、时间等) coords: a dict-like container of arrays (coordinates) that label each * point (e.g., 1-dimensional arrays of numbers, datetime objects or strings) (各维度坐标体系，如时间序列) attrs: an OrderedDict to hold arbitrary metadata (attributes) (对各属性的描述) 1. Subset and Diurnal profiles As an illustration, we firstly extract the original file generated from WRF-Chem by specific time range and spatial coverage. Then, the daily averages for those selected grids are calculated and saved as another netCDF file. 123456789101112131415161718192021222324import xarray as xrfilename = \"./../wrfout_allBVOCs_d01_2018-06-01_00:00:00\"df = xr.open_dataset(filename)# 1. add dimsdf = df.assign_coords(south_north=('south_north',df.XLAT[0,:,0].values))df = df.assign_coords(west_east=('west_east',df.XLONG[0,0,:].values))df = df.assign_coords(Time=('Time',df.XTIME.values))# 2. selcting the grids with lon in the range of 100-120，lat in the range of 40-50 in the bottom vertical level. The simulation period was chosen in the range of 2018-06-01 to 2018-06-05da = df.sel(bottom_top = slice(0,1), south_north = slice(40,50),west_east = slice(100,120), Time = slice('2018-06-01T00:00:00.000000000',\\ '2018-06-08T00:00:00.000000000'))# 3. selecting one/multi variable(s)var = 'PM2_5_DRY' var_list = ['asoa1j','asoa1i','asoa2j','asoa2i']# 3.1 Summarizing multiple variablesda['SUM'] = 0for c in var_list: da['SUM'] = da['SUM']+da[c] # 4. Calculating the diurnal averagesdm = da[\"SUM\"].groupby('Time.hour').mean()dm.to_netcdf(\"diurnal.nc\") 2. Rolling windowIn the second example, we use resample trick to group the original data, and use rolling trick to calculate the daily maximum of 8-hour ozone concentrations. 123da['8h_o3_averege'] = da['o3'].rolling(Time=8, center=False).mean() # center = False-&gt;time=end point of the rolling window.dt = da['8h_o3_averege'].resample(Time ='1D').max()dt.to_netcdf(\"./8h_o3_max.nc\")","link":"/2020/11/01/WRF-technique-xarray-tricks/"},{"title":"Linear Fitting with Matplotlib and Plotly","text":"Linear regression is a fundamental method in data analysis to understand the relationship between two variables. Here, I summarize four reusable Python functions for performing and visualizing linear fitting: Matplotlib: with and without intercept Plotly: with and without intercept All methods: Drop NaN values in x and y Plot a scatter graph Fit a linear line Annotate the equation and correlation coefficient (R) 1. Matplotlib — Linear Fit with Intercept123456789101112131415161718192021222324import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import linregressdef add_mpl_subplot_with_intercept(ax, df, x_col, y_col, color='blue'): x = df[x_col] y = df[y_col] valid = ~(x.isna() | y.isna()) x = x[valid] y = y[valid] ax.scatter(x, y, color=color, alpha=0.6) # Linear regression with intercept slope, intercept, r_value, _, _ = linregress(x, y) line_x = np.linspace(x.min(), x.max(), 100) line_y = slope * line_x + intercept ax.plot(line_x, line_y, 'r--') # Annotate equation and R eq_text = f\"y = {slope:.2f}x + {intercept:.2f}\\nR = {r_value:.2f}\" ax.annotate(eq_text, xy=(0.05, 0.95), xycoords='axes fraction', ha='left', va='top', fontsize=10, color='black') 2. Matplotlib — Linear Fit without Intercept1234567891011121314151617181920def add_mpl_subplot_no_intercept(ax, df, x_col, y_col, color='green'): x = df[x_col] y = df[y_col] valid = ~(x.isna() | y.isna()) x = x[valid] y = y[valid] ax.scatter(x, y, color=color, alpha=0.6) # Linear regression without intercept slope = np.sum(x * y) / np.sum(x ** 2) r_value = x.corr(y) line_x = np.linspace(x.min(), x.max(), 100) line_y = slope * line_x ax.plot(line_x, line_y, 'r--') eq_text = f\"y = {slope:.2f}x\\nR = {r_value:.2f}\" ax.annotate(eq_text, xy=(0.05, 0.95), xycoords='axes fraction', ha='left', va='top', fontsize=10, color='black') 3. Plotly — Linear Fit with Intercept1234567891011121314151617181920212223242526272829303132333435363738import plotly.graph_objects as gofrom scipy.stats import linregressdef add_plotly_subplot_with_intercept(fig, df, x_col, y_col, row, col, color='blue', show_eq=True): x = df[x_col] y = df[y_col] valid = ~(x.isna() | y.isna()) x = x[valid] y = y[valid] fig.add_trace(go.Scatter( x=x, y=y, mode='markers', marker=dict(color=color), showlegend=False ), row=row, col=col) slope, intercept, r_value, _, _ = linregress(x, y) line_x = np.linspace(x.min(), x.max(), 100) line_y = slope * line_x + intercept fig.add_trace(go.Scatter( x=line_x, y=line_y, mode='lines', line=dict(color='red', dash='dash'), showlegend=False ), row=row, col=col) if show_eq: eq_text = f\"y = {slope:.2f}x + {intercept:.2f}&lt;br&gt;R = {r_value:.2f}\" fig.add_annotation( text=eq_text, xref=f'x{col}', yref=f'y{row}', x=x.min() + 0.05 * (x.max() - x.min()), y=y.max() - 0.05 * (y.max() - y.min()), showarrow=False, font=dict(size=11, color=\"black\"), align=\"left\", row=row, col=col ) 4. Plotly — Linear Fit without Intercept123456789101112131415161718192021222324252627282930313233343536def add_plotly_subplot_no_intercept(fig, df, x_col, y_col, row, col, color='green', show_eq=True): x = df[x_col] y = df[y_col] valid = ~(x.isna() | y.isna()) x = x[valid] y = y[valid] fig.add_trace(go.Scatter( x=x, y=y, mode='markers', marker=dict(color=color), showlegend=False ), row=row, col=col) slope = np.sum(x * y) / np.sum(x ** 2) r_value = x.corr(y) line_x = np.linspace(x.min(), x.max(), 100) line_y = slope * line_x fig.add_trace(go.Scatter( x=line_x, y=line_y, mode='lines', line=dict(color='red', dash='dash'), showlegend=False ), row=row, col=col) if show_eq: eq_text = f\"y = {slope:.2f}x&lt;br&gt;R = {r_value:.2f}\" fig.add_annotation( text=eq_text, xref=f'x{col}', yref=f'y{row}', x=x.min() + 0.05 * (x.max() - x.min()), y=y.max() - 0.05 * (y.max() - y.min()), showarrow=False, font=dict(size=11, color=\"black\"), align=\"left\", row=row, col=col )","link":"/2024/02/10/%5BCode-library%5DLinear_fitting_plotting_code_summary/"},{"title":"Installing FLEXPART v11 with ECMWF Support","text":"This post walks through a full FLEXPART v11 installation for ECMWF meteorological data with NetCDF and Fortran support, including resolving common compilation issues. All components were built from source on a Linux HPC system with user-level access. ⚙️ Installation Structure12export DIR=/data/user/hao_y/libexport INSTALL_DIR=$DIR All components (GCC, HDF5, NetCDF, ecCodes, etc.) will be installed under this directory tree. 🔧 Step-by-Step Build Pipeline1. GCC 13.2.0 (with Fortran Support)12345678wget https://ftp.gnu.org/gnu/gcc/gcc-13.2.0/gcc-13.2.0.tar.gztar -xzf gcc-13.2.0.tar.gzcd gcc-13.2.0./contrib/download_prerequisitesmkdir build &amp;&amp; cd build../configure --prefix=$DIR/gcc-13.2.0 --enable-languages=c,c++,fortran,go --disable-multilibmake -j$(nproc)make install Export compilers: 123456789export PATH=$DIR/gcc-13.2.0/bin:$PATHexport LD_LIBRARY_PATH=$DIR/gcc-13.2.0/lib64:$LD_LIBRARY_PATHexport CC=gccexport CXX=g++export FC=gfortranexport F77=gfortranexport CFLAGS='-gdwarf-2 -gstrict-dwarf'export FCFLAGS=-m64export FFLAGS=-m64 2. HDF5 1.14.612345678wget https://support.hdfgroup.org/releases/hdf5/v1_14/v1_14_6/downloads/hdf5-1.14.6.tar.gztar -xzf hdf5-1.14.6.tar.gzcd hdf5-1.14.6./configure --prefix=$DIR/hdf5 --enable-cxx --enable-fortran --enable-sharedmake -j$(nproc)make installh5cc -showconfig ## check whether it is installed properly 3. NetCDF (C + Fortran)3.1 NetCDF-C1234567891011wget https://github.com/Unidata/netcdf-c/archive/refs/tags/v4.9.3.tar.gz -O netcdf-c-4.9.3.tar.gztar -xzf netcdf-c-4.9.3.tar.gzcd netcdf-c-4.9.3./configure --prefix=$DIR/netcdf \\ CPPFLAGS=\"-I$DIR/hdf5/include\" \\ LDFLAGS=\"-L$DIR/hdf5/lib\" \\ --enable-netcdf-4 --disable-dapmake -j$(nproc)make installnc-config --all ## check whether it is installed properly 3.2 NetCDF-Fortran123456wget https://github.com/Unidata/netcdf-fortran/archive/refs/tags/v4.6.1.tar.gz -O netcdf-fortran-4.6.1.tar.gztar -xzf netcdf-fortran-4.6.1.tar.gzcd netcdf-fortran-4.6.1./configure --prefix=$DIR/netcdfmake -j$(nproc)make install 4. OpenMPI (Optional for MPI mode)12345678wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.6.tar.gztar -xzf openmpi-4.1.6.tar.gzcd openmpi-4.1.6./configure --prefix=$DIR/libs/openmpimake -j$(nproc)make installexport PATH=$DIR/libs/openmpi/bin:$PATHexport LD_LIBRARY_PATH=$DIR/libs/openmpi/lib:$LD_LIBRARY_PATH 5. AEC Library (Required by ecCodes)123456wget https://gitlab.dkrz.de/k202009/libaec/-/archive/v1.0.6/libaec-v1.0.6.tar.gztar -xzf libaec-v1.0.6.tar.gzcd libaec-v1.0.6 &amp;&amp; mkdir build &amp;&amp; cd buildcmake .. -DCMAKE_INSTALL_PREFIX=$DIR/libaecmake -j$(nproc)make install 6. ecCodes (with Fortran and NetCDF)1234567wget https://confluence.ecmwf.int/download/attachments/45757960/eccodes-2.33.0-Source.tar.gztar -xzf libaec-v1.0.6.tar.gz# Unpack and buildcd eccodes-2.33.0-source &amp;&amp; mkdir build &amp;&amp; cd buildcmake .. -DCMAKE_INSTALL_PREFIX=$DIR/eccodes -DCMAKE_PREFIX_PATH=\"$DIR/netcdf;$DIR/hdf5;$DIR/libaec\" -DENABLE_NETCDF=ON -DENABLE_FORTRAN=ON -DCMAKE_BUILD_TYPE=Releasemake -j$(nproc)make install 📃 Environment Exports SummarySet these variables in your .bashrc or shell: 1234567891011121314151617181920212223242526272829303132export PATH=$DIR/gcc-13.2.0/bin:$PATHexport LD_LIBRARY_PATH=$DIR/gcc-13.2.0/lib64:$LD_LIBRARY_PATHexport CC=gccexport CXX=g++export FC=gfortranexport F77=gfortranexport FFLAGS=-m64# HDF5export HDF5_DIR=$DIR/hdf5export CPATH=$HDF5_DIR/include:$CPATHexport LIBRARY_PATH=$HDF5_DIR/lib:$LIBRARY_PATHexport LD_LIBRARY_PATH=$HDF5_DIR/lib:$LD_LIBRARY_PATH# NetCDFexport NETCDF_DIR=$DIR/netcdfexport CPATH=$NETCDF_DIR/include:$CPATHexport LIBRARY_PATH=$NETCDF_DIR/lib:$LIBRARY_PATHexport LD_LIBRARY_PATH=$NETCDF_DIR/lib:$LD_LIBRARY_PATHexport PKG_CONFIG_PATH=$NETCDF_DIR/lib/pkgconfig:$PKG_CONFIG_PATH# AECexport CPATH=$DIR/libaec/include:$CPATHexport LIBRARY_PATH=$DIR/libaec/lib:$LIBRARY_PATHexport LD_LIBRARY_PATH=$DIR/libaec/lib:$LD_LIBRARY_PATH# ecCodesexport CPATH=$DIR/eccodes/include:$CPATHexport LIBRARY_PATH=$DIR/eccodes/lib:$LIBRARY_PATHexport LD_LIBRARY_PATH=$DIR/eccodes/lib:$LD_LIBRARY_PATHexport PATH=$DIR/eccodes/bin:$PATHexport PKG_CONFIG_PATH=$DIR/eccodes/lib/pkgconfig:$PKG_CONFIG_PATH 🌐 FLEXPART v11 Compilation (ECMWF Mode)12345wget https://gitlab.phaidra.org/flexpart/flexpart/-/archive/v11/flexpart-v11.tar.gztar -xzf flexpart-v11.tar.gzcd flexpart-v11# the place to change in the ./src/makefile_gfortran is the F90 = /data/user/hao_y/lib/gcc-13.2.0/bin/gfortran make -f makefile_gfortran -j4 eta=no ncf=yes eta=no: Use hybrid ECMWF coordinate system ncf=yes: Enable NetCDF support ❌ Common Errors and Fixes Error Cause Solution cannot find -leccodes_f90 ecCodes built without Fortran Add -DENABLE_FORTRAN=ON AEC library not found libaec missing Install libaec and add to CMAKE_PREFIX_PATH GLIBCXX_3.4.30 not found Old libstdc++ in system Build newer GCC (&gt;=13) and use its lib path gfortran: command not found GCC built without Fortran Rebuild with --enable-languages=c,c++,fortran 📅 Final Notes Think of ./configure as the blueprint, make as construction, and make install as moving into your house and wiring the electricity.You now have a clean, custom-built FLEXPART environment with all dependencies resolved, ready to simulate ECMWF-based transport at scale. when copy the setting from another PC into the release folder, you need to change the permission of the folder by typing chmod -R 755 ./options and chmod 644./options/RELEASES or any other files. For multi-core processing, you can use the export OMP_NUM_THREADS=4 to set 4 cores (can set for more) and then with ./FLEXPART command to run FLEXPART with multiple processes. Sometimes, we will find error of “Program received signal SIGSEGV: Segmentation fault - invalid memory reference.” By typing ulimit -s unlimted, we can solve this problem. Reference FLEXPART GitLab repositoryhttps://gitlab.phaidra.org/flexpart/flexpart(Main source code and Makefiles for FLEXPART v10/v11) FLEXPART Installation Guide (official wiki)https://www.flexpart.eu/wiki/FpInstallation(Overview of FLEXPART compilation across versions and systems) ECMWF ecCodes documentationhttps://confluence.ecmwf.int/display/ECC/ecCodes+Home(Build instructions, environment variables, and sample readers)","link":"/2025/06/20/%5BModel%5DFLEXPART.V11_ECMWF_supported_installation/"},{"title":"GEOS-Chem Installation and Run log","text":"In this post, I document the complete process of installing and configuring the GEOS-Chem Classic model, including environmental setup, compilation, and runtime configuration. The log is based on my experience running GEOS-Chem on a Linux HPC system, and covers both the challenges I encountered and the solutions I implemented. 1. Environmental SetupGEOS-Chem requires a proper build environment. 12345export DIR=/data/user/hao_y/libexport HDF5_DIR=$DIR/hdf5export NETCDF_DIR=$DIR/netcdfexport PATH=$DIR/gcc-13.2.0/bin:$HDF5_DIR/bin:$NETCDF_DIR/bin:$PATHexport LD_LIBRARY_PATH=$DIR/gcc-13.2.0/lib64:$HDF5_DI 2. GEOS-Chem Classic initiiation2.1 Clone and Initialize123git clone --recurse-submodules https://github.com/geoschem/GCClassic.gitcd GCClassic/run./createRunDir.sh 2.2 Run Directory ConfigurationMy setting is as follows: Simulation: Full chemistry Complex SOA with semivolatile POA Meteorology: GEOS-FP Resolution: 0.25° × 0.3125° Domain: Asia (manually adjusted to South Asia later) Vertical Levels: 72 (native) 2.3 CompilationGo into the run directory and install 12345cd runs/gc_025_SAsia_2018/gc_025x03125_AS_geosfp_fullchem_complexSOA/cd buildcmake ../CodeDir -DRUNDIR=..make -j 16make install After building, adjust these configuration files:geoschem_config.yml,HISTORY.rc,HEMCO_Config.rc 3. Input Data Handling3.1 Dry Run and AWS SetupTo determine required meteorological and restart data: 123456789./gcclassic --dryrun | tee log.dryrun# install the AWS service command line service tool.curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"unzip awscliv2.zip./aws/install -i ~/aws-cli -b ~/binaws s3 ls --no-sign-request s3://geos-chem/./download_data.py log.dryrun geoschem+aws 3.2 Download the data1./download_data.py log.dryrun geoschem+aws 3.3 Fixing restart file timestamp errorsSometimes AWS pulls a misaligned restart file. Manually download the correct one, for example: 1wget http://geoschemdata.wustl.edu/ExternalShare/geos-chem/10yr_benchmarks/14.0.0/GCClassic/Restarts/2018/GEOSChem.Restart.20181001_0000z.nc4 Place it into the Restarts/ directory of your run folder. 4. SLURM Run Script (run_gcclassic.slurm)Simply run by sbatch run_gcclassic.slurm to start the simulation, and there will be email to remind when the simulation was done. 1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash#SBATCH -c 256 # Request 16 cores#SBATCH -N 1 # Use 1 node#SBATCH -t 6-00:00 # Max runtime of 12 hours#SBATCH -p general # &lt;-- Replace iwith actual partition name#SBATCH --mem=400000 # Request 400 GB memory#SBATCH --job-name=gcclassic_run#SBATCH --output=gcclassic_%j.out#SBATCH --error=gcclassic_%j.err#SBATCH --mail-type=END#SBATCH --mail-user=yufang.hao@psi.ch # &lt;-- Add this if you want email alerts################################################################################## GEOS-Chem Classic run script for SLURM (MERLIN cluster)################################################################################ Load your GCC 13.2.0 environmentsource ~/.bashrc# Optionally load other environment modules if needed (HDF5, NetCDF, etc.)# Remove stack size limit to prevent segfaultsulimit -s unlimited# (Optional) log current limitsulimit -a# Set OpenMP threadsexport OMP_STACKSIZE=1Gexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK# Run GEOS-Chem Classicsrun -c $OMP_NUM_THREADS time -p ./gcclassic &gt;&gt; GC.log# Exitexit 0 5. Debugging and Errors5.1 Segmentation FaultIf encoutering:Investigated via gdb, pinpointed to cldj_fjx_sub_mod.F90, line 1457: 1DD(I,J,L,K) = -E(I,J)*C(J,L,K) Likely due to unallocated or incorrectly dimensioned array. I found increase the memory could be helpful, so in the Increased --mem=64000 (64 GB) to --mem=400000. Ensured ulimit -s unlimited applied 6. Output Control and Optimization6.1. Optmization of exectuable speed In HISTORY.rc, I disabled species output to save time and space. In geoschem_config.yml, 12autoreduce_solver: activate: true This can help reduce runtime by simplifying the chemical solver dynamically. 7. Job MonitoringSubmit: 1sbatch run_gcclassic.slurm Monitor: 12squeue -u $USERscontrol show job &lt;JOBID&gt; Stop 1scancel &lt;JOBID&gt; Check logs: 12cat gcclassic_&lt;JOBID&gt;.outcat gcclassic_&lt;JOBID&gt;.err A simple example of the results References GEOS-Chem User Guides and DocumentationGEOS-Chem Official Website: https://geos-chem.orgUser Documentation: https://geos-chem.readthedocs.io GEOS-Chem GitHub RepositorySource code and run directory setup: https://github.com/geoschem/GCClassic GEOS-Chem Input Data on AWShttps://registry.opendata.aws/geos-chem AWS CLI Installation Guidehttps://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html GEOS-Chem Error Troubleshooting Guidehttps://geos-chem.readthedocs.io/en/14.2.2/geos-chem-shared-docs/supplemental-guides/error-guide.html","link":"/2025/07/22/%5BModel%5DGEOS_Chem_Classic_South_Asia_Simulation_Tutorial/"},{"title":"Plotting topography map with hillshade 绘制带阴影高光的地形图","text":"Hillshade is the representation of the earth’s surface under the radiation of sun. A terrain raster data can be better visualized by adding the information of hillshades. This blog will introduce some procrdures to overlay the hillshade with terrain for a nice picture. First, let’s import the required packages. 12345from osgeo import gdal import numpy as np import matplotlib.pyplot as plt import elevation%matplotlib inline Second, we download and read a sample data 123456!eio --product SRTM3 clip -o DEM.tif --bounds -122.4 41.2 -122.1 41.3from osgeo import gdalraster = gdal.Open(\"./DEM.tif\", gdal.GA_ReadOnly)dem = raster.GetRasterBand(1).ReadAsArray()dem = dem[::-1] Then we calculated the hillshade using the function I referenced from Download and Process DEMs in Python 1234567891011def hillshade(array, azimuth, angle_altitude): # Source: http://geoexamples.blogspot.com.br/2014/03/shaded-relief-images-using-gdal-python.html x, y = np.gradient(array) slope = np.pi/2. - np.arctan(np.sqrt(x*x + y*y)) aspect = np.arctan2(-x, y) azimuthrad = azimuth*np.pi / 180. altituderad = angle_altitude*np.pi / 180. shaded = np.sin(altituderad) * np.sin(slope) \\ + np.cos(altituderad) * np.cos(slope) \\ * np.cos(azimuthrad - aspect) return 255*(shaded + 1)/2 Finally, we plotted three figures, one is the simple dem plot, the other two are the topography overlayed by hillshade using different alpha. 123456789101112131415161718fig = plt.figure(figsize=(12, 4))extent =[-122.4 ,41.2, -122.1, 41.3]ax = fig.add_subplot(131,projection=ccrs.Mercator())cax = ax.contourf(dem, np.arange(vmin, vmax, 10),extent=extent, cmap=terrain_cmap, vmin=vmin, vmax=vmax, origin='image')plt.title(r'$Original$',fontsize =12)ax = fig.add_subplot(132,projection=ccrs.Mercator())ax.matshow(hillshade(dem, 30, 30), extent=extent, cmap='Greys', alpha=.3, zorder=10)cax = ax.contourf(dem, np.arange(vmin, vmax, 10),extent=extent, cmap=terrain_cmap, vmin=vmin, vmax=vmax, origin='image')plt.title(r'$\\alpha=0.3$',fontsize =12)ax = fig.add_subplot(133,projection=ccrs.Mercator())ax.matshow(hillshade(dem, 30, 30), extent=extent, cmap='Greys', alpha=.8, zorder=10)cax = ax.contourf(dem, np.arange(vmin, vmax, 10),extent=extent, cmap=terrain_cmap, vmin=vmin, vmax=vmax, origin='image')plt.title(r'$\\alpha=0.8$',fontsize =12) References Create a Hillshade from a Terrain Raster in Python Earth analytics python Download and Process DEMs in Python","link":"/2018/11/09/13.%5BCode%5DPlotting%20topography%20map%20with%20using%20Python/"},{"title":"Post processing of FLEXPART-WRF output","text":"In this post, I present some simple programs written in Python for post-processing the flexpart-wrf output. It mainly contains several aspects, data merging, data processing and data visualization. I will also show some tips tp creat self-defined colormaps for nice plots. PS: All th codes are also uploaded in my GitHub respority PyFlex Merging datasetsIn the output path, the result (e.g., flxout_d01_20160304_030000.nc ) for each run was saved as an independent file. It would be inconvenient to loop them for every post-processing function. Therefore, I zip them into a hdf5 file. Here is the code and some instructions. 1234567891011121314151617181920212223242526272829303132333435363738394041\"\"\"pre-reading an template file for capture the geogrid information\"\"\"test_file_path = output_path+\"flxout_d01_20160304_030000.nc\"test_file = Dataset(test_file_path)pes = np.zeros_like(test_file['/CONC'][:,0,0,0,:,:].sum(axis = 0))pos=Dataset(output_path+\"header_d01.nc\")xx = pos['XLONG'][:]yy = pos['XLAT'][:]\"\"\"Creat HDF FILES\"\"\"hdf_filename ='Chifeng_PES_72hour.hdf'os.system(\"rm \"+hdf_filename)with h5py.File(hdf_filename, \"w\") as nf: dset = nf.create_dataset(\"test\", (100,), dtype='i')nf = h5py.File(hdf_filename, 'r+')grp = nf.create_group(\"PES\")dset1 = grp.create_dataset(\"lon\", data= xx)dset2 = grp.create_dataset(\"lat\", data= yy)\"\"\"Loop the datasets for each month\"\"\"\"\"\"Take the winter months for example\"\"\"print \"## WINTER ##\"for mo in ['01','02','12']: ## The arrays include 6 dimensions, time, ageclass, releases (species), Z, Y, X ## Since most sources are located within the footprint layer (~300 m above the ground), we only calculated the residence time of bottom vertical layer. pes = np.zeros_like(test_file['/CONC'][:,0,0,0,:,:].sum(axis = 0)) t = 0 Dir = output_path+\"winter/\"+mo+\"/\" files = os.listdir(Dir) files = sorted(files) for file in files: filename,extname = os.path.splitext(file) if filename[0:4] == 'flxo': fc = Dataset(Dir+file) if fc['/CONC'].shape[0]&gt;70.0: pes+=fc['/CONC'][:,0,0,0,:,:].sum(axis = 0) t+=1 print mo+\" \"+str(t) pes = pes/t dset = grp.create_dataset(mo, data= pes) print 'Done' nf.close() Then, we can download the “.hdf5” file for further analysis. Colormap settingA good colormap can really improve the representability for the figure. Here, I recommend and generate three colormaps using different approaches. cmap1: generated by user-defined RGB values cmap2: subsets of an existing 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455## CMAP1 from pflexible.py on https://git.nilu.no/from matplotlib.colors import ListedColormapcolor_list = [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 9.9607843e-01, 9.1372549e-01, 1.0000000e+00, 9.8431373e-01, 8.2352941e-01, 1.0000000e+00, 9.6470588e-01, 7.1764706e-01, 1.0000000e+00, 9.3333333e-01, 6.0000000e-01, 1.0000000e+00, 8.9019608e-01, 4.4705882e-01, 1.0000000e+00, 8.3137255e-01, 2.0000000e-01, 1.0000000e+00, 7.5686275e-01, 0.0000000e+00, 1.0000000e+00, 6.6274510e-01, 0.0000000e+00, 1.0000000e+00, 5.4901961e-01, 0.0000000e+00, 1.0000000e+00, 4.0784314e-01, 0.0000000e+00, 1.0000000e+00, 2.4705882e-01, 0.0000000e+00, 1.0000000e+00, 7.4509804e-02, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 2.8235294e-01, 1.0000000e+00, 0.0000000e+00, 4.8627451e-01, 1.0000000e+00, 0.0000000e+00, 6.3137255e-01, 1.0000000e+00, 0.0000000e+00, 7.4509804e-01, 1.0000000e+00, 0.0000000e+00, 8.4705882e-01, 1.0000000e+00, 0.0000000e+00, 9.3725490e-01, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 9.7647059e-01, 0.0000000e+00, 1.0000000e+00, 8.9411765e-01, 0.0000000e+00, 1.0000000e+00, 8.0000000e-01, 0.0000000e+00, 1.0000000e+00, 6.9019608e-01, 0.0000000e+00, 1.0000000e+00, 5.6470588e-01, 0.0000000e+00, 1.0000000e+00, 4.0000000e-01, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 3.9607843e-01, 1.0000000e+00, 0.0000000e+00, 5.6470588e-01, 1.0000000e+00, 0.0000000e+00, 6.9019608e-01, 1.0000000e+00, 0.0000000e+00, 7.9607843e-01, 1.0000000e+00, 0.0000000e+00, 8.9411765e-01, 1.0000000e+00, 0.0000000e+00, 9.7647059e-01, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 9.4509804e-01, 0.0000000e+00, 1.0000000e+00, 8.7450980e-01, 0.0000000e+00, 1.0000000e+00, 7.9215686e-01, 0.0000000e+00, 1.0000000e+00, 7.0588235e-01, 0.0000000e+00, 1.0000000e+00, 6.0392157e-01, 0.0000000e+00, 1.0000000e+00, 4.8235294e-01, 0.0000000e+00, 1.0000000e+00, 3.1372549e-01, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.4901961e-01, 1.0000000e+00, 0.0000000e+00, 3.3333333e-01, 1.0000000e+00, 0.0000000e+00, 4.4705882e-01, 1.0000000e+00, 0.0000000e+00, 5.3725490e-01, 1.0000000e+00, 0.0000000e+00, 6.1176471e-01, 9.7647059e-01, 0.0000000e+00, 6.6666667e-01, 8.9411765e-01, 0.0000000e+00, 6.6666667e-01, 7.9607843e-01, 0.0000000e+00, 6.3921569e-01, 6.9019608e-01, 0.0000000e+00, 5.9215686e-01, 5.6470588e-01, 0.0000000e+00, 5.0980392e-01, 3.9607843e-01, 0.0000000e+00, 3.8039216e-01]color_list = np.reshape(color_list, (-1, 3))name = 'flexpart_cmap'cmap1 = ListedColormap(color_list, name) 12345678910## CMAP2 extracted from an existing colormapimport matplotlib.pyplot as pltimport matplotlib.colors as colorsimport numpy as npdef truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100): new_cmap = colors.LinearSegmentedColormap.from_list( 'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n))) return new_cmapcmap2 = truncate_colormap(plt.cm.gist_ncar, 0.3,0.9) cmap3 is an interesting colormap which I clipped from an existing figure. I cut the colorbar from the above figure, read its RGB values, and generate a new colormap 12345678910111213## CMAP3 from a current colorbarfrom PIL import Imageimport pandas as pdim = Image.open('./colorbar_3.png')rgb_im = im.convert('RGB')r, g, b = rgb_im.getpixel((1, 1))k = []for i in range(18,321,1): k.append(rgb_im.getpixel((i,20)))color_list = np.array(k) color_list = np.array([(i/255.0,j/255.0,k/255.0) for i,j,k in color_list])name = 'copy_cmap'cmap3 = ListedColormap(color_list, name) 1234567891011121314# plotting themfrom mpl_toolkits.axes_grid1 import make_axes_locatabledef sample_plot(ax,arr,cmap): s = ax.imshow(arr, interpolation='nearest', cmap=cmap) divider = make_axes_locatable(ax) cax = divider.new_vertical(size=\"5%\", pad=0.3, pack_start=True) fig.add_axes(cax) fig.colorbar(s, cax=cax, orientation=\"horizontal\") arr = np.linspace(0, 50, 100).reshape((10, 10))fig, ax = plt.subplots(ncols=3)sample_plot(ax[0],arr, cmap1)sample_plot(ax[1],arr, cmap2)sample_plot(ax[2],arr, cmap3) Finally, we can read and visualize the datasets. 123456789101112131415161718192021222324252627282930313233343536373839404142# reading the hdf5 data PES_data = h5py.File(\"/Users/HYF/Downloads/Chifeng_PES_pbl_72hour.hdf\")lon = PES_data['/PES/lon'][:]lat = PES_data['/PES/lat'][:]# sample arrray of the results in January.PES_1 = PES_data['/PES/12'][:]PES_2 = PES_data['/PES/01'][:]PES_3 = PES_data['/PES/02'][:]PES_winter = (PES_1+PES_2+PES_12)/3.0 # plot the winter retroplume with colormap1fig = plt.figure(figsize=(4,3), frameon=True)proj = ccrs.LambertConformal(central_latitude = 42.715, central_longitude = 118.79, standard_parallels = (30, 60))ax =plt.subplot(111, projection = proj)mask_v = np.ma.masked_less_equal(PES_winter,0)cs = ax.pcolormesh(lon,lat,mask_v,transform=ccrs.PlateCarree(),cmap =cmap1,alpha = 0.85,zorder=1,norm=matplotlib.colors.LogNorm(), vmin = 1E-9) ax.set_extent([105,133,31,52], crs=ccrs.PlateCarree())ax.coastlines(linewidth = 0.5,resolution='50m')ax.add_feature(cfeature.BORDERS, linewidth=0.5)## the xticks, yticks setting is referenced from ## https://github.com/ARM-DOE/pyart/blob/master/pyart/graph/radarmapdisplay_cartopy.py## I have not uploaded the original function here to make the code more tight.fig.canvas.draw()ax.gridlines(xlocs=xticks, ylocs=yticks, color='gray', alpha=0.5, linestyle='--',linewidth = 0.75)ax.xaxis.set_major_formatter(LONGITUDE_FORMATTER) ax.yaxis.set_major_formatter(LATITUDE_FORMATTER)lambert_xticks(ax, xticks)lambert_yticks(ax, yticks) pos1 = ax.get_position()tax = fig.add_axes([pos1.x0,pos1.y1,pos1.x1-pos1.x0,0.03])#x0,y0,long,widthtax.get_xaxis().set_visible(False)tax.get_yaxis().set_visible(False)tax.set_facecolor('#FFE5CE')tax.text(0.4,0.3,\"Winter\",color = 'k',fontsize =9,fontweight = 'bold',transform=tax.transAxes)","link":"/2018/11/06/12.%5BModel%5DFLEXPART-WRF%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C%E5%90%8E%E5%A4%84%E7%90%86/"},{"title":"OrbiTrack Dev Log 6: TOF missing peaks adding","text":"","link":"/2024/11/24/38.%5BMS%5DOrbiTrack_Dev_Log_6_Using_TOF_Fitting_Refiner/"},{"title":"🍵非靶向质谱方法探索各类茶饮的分子组成","text":"我一直想对茶叶中复杂的化学成分充满兴趣。实验室有同事需要测量生物气溶胶样品（如孢子、真菌等），借助协助其调试维护质谱仪器的机会，开展了一项课外探索：使用高分辨率质谱方法，探索不同类型茶叶在分子层面是否具有可辨识的特征。 本次尝试使用非靶向直接注入分析，结合Orbitrap与TOF两台质谱，评估我构建的分析流程在实际复杂样品中的表现。 1. 实验设计我总共设计了两组测量任务： 任务 A：不同国家与产区的茶叶对比包括中国绿茶、日本绿茶、印度红茶、斯里兰卡红茶等，卢旺达红茶等，探究不同类型/产地茶叶能否通过质谱分析呈现差异。 任务 B：西湖龙井的“档次分辨”挑战对比杭州核心产区与非核心产区龙井（价格不同）以及其他地区的”挂牌”龙井的分子组成差异。该部分实验已完成，尚未分析其数据，未来会专文介绍。 茶叶样品相比大气污染物自然采集更有意思。我个人从老家带来几种安徽产的绿茶，为了扩大样品量，向同事们借茶，并前往Lenzburg的茶叶商店It’s tea time购买红茶。最终我收集到的茶叶样品包括： 中国绿茶：安徽池州绿茶、四川雅安绿茶 日本绿茶：两款产地不同的煎茶 斯里兰卡绿茶红茶系列： 中国祁门红茶 印度大吉岭红茶（Margaret’s Hope） 越南红茶（Golden Tippy） 斯里兰卡红茶（Nuwara Eliya） 卢旺达红茶（Rukeri） 在国内朋友的帮助下，我收到了多个价位（从 200 元至 1000 元不等）的西湖龙井样品，并由一位回国探亲的同事帮忙携带至瑞士，具见下图： 通过预实验，尽可能模拟真实饮茶状态——即将茶叶浸泡于热水中提取成分，并直接进行质谱测量。 2. 数据处理RDKit Python library can reproduce the structurs based on the smiles code. 3. 结果分析 m/z Formula 中文名称 English Name Note 140.06853 C5H11NO2-Na⁺ 缬氨酸 Valine 氨基酸 158.08109 C7H11NO3-H⁺ 甘氨酸 Tiglylglycine 氨基酸衍生物 175.10800 C7H14N2O3-H⁺ 茶氨酸 Theanine 茶叶中重要氨基酸成分之一 197.09180 C9H12N2O3-H⁺ 3-氨基-L-酪氨酸 3-Amino-L-tyrosine 酪氨酸衍生物 215.05261 C7H12O6-Na⁺ 奎宁酸 Quinic acid 植物代谢产物 217.06955 C8H10N4O2-Na⁺ 咖啡因 Caffeine 茶叶中的典型生物碱成分 219.07156 C7H13N2O3-Na⁺ L-茶氨酸 Theanine 又称谷氨酰乙胺，是茶鲜味的主要来源，占游离氨基酸50%以上 313.06851 C15H14O6-Na⁺ 儿茶素 Catechin 多酚类抗氧化成分 335.09497 C11H20O10-Na⁺ 接骨木二糖 Sambubiose 多糖类 365.10501 C12H22O11-Na⁺ 蔗糖 Sucrose 常见双糖 203.05258 C6H12O6-Na⁺ 葡萄糖 Glucose 单糖 329.09976 C16H18O6-Na⁺ 升麻素 Cimifugin 中草药活性成分，具有抗炎活性","link":"/2024/05/29/39.%5BMS%5DUntargeted_Analysis_on_Tea/"},{"title":"Common Function for Organics Visualization 有机物分子组成绘图代码分享","text":"","link":"/2025/05/01/42.%5BCode%5DCommon_Functions_in_Chemical_Composition_Visualization%20copy/"},{"title":"Organic Emission Profiles in Interactive Plots 不同污染源的排放图谱分享","text":"Load Plotly JS once (in the theme’s head.ejs or _partial/footer.ejs)In themes/minos/layout/_partial/head.ejs, before Interactive Spectrum: Fire vs Vehicular Emissions","link":"/2025/05/01/43.%5BCode%5DInteractive_plot_for_organics_spectra/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"Data processing","slug":"Data-processing","link":"/tags/Data-processing/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Model","slug":"Model","link":"/tags/Model/"},{"name":"Air quality","slug":"Air-quality","link":"/tags/Air-quality/"},{"name":"Visualization","slug":"Visualization","link":"/tags/Visualization/"},{"name":"GIS","slug":"GIS","link":"/tags/GIS/"},{"name":"WRF","slug":"WRF","link":"/tags/WRF/"},{"name":"Knowledge Base","slug":"Knowledge-Base","link":"/tags/Knowledge-Base/"},{"name":"Visulaization","slug":"Visulaization","link":"/tags/Visulaization/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"Windows","slug":"Windows","link":"/tags/Windows/"},{"name":"Atmosphere","slug":"Atmosphere","link":"/tags/Atmosphere/"},{"name":"Dataset","slug":"Dataset","link":"/tags/Dataset/"},{"name":"Geospatial","slug":"Geospatial","link":"/tags/Geospatial/"},{"name":"Package","slug":"Package","link":"/tags/Package/"},{"name":"Orbitrap","slug":"Orbitrap","link":"/tags/Orbitrap/"},{"name":"Mass Spectrometry","slug":"Mass-Spectrometry","link":"/tags/Mass-Spectrometry/"},{"name":"人文阅读","slug":"人文阅读","link":"/tags/%E4%BA%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"Chemistry","slug":"Chemistry","link":"/tags/Chemistry/"},{"name":"Food","slug":"Food","link":"/tags/Food/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"Plotly","slug":"Plotly","link":"/tags/Plotly/"}],"categories":[{"name":"Tech","slug":"Tech","link":"/categories/Tech/"},{"name":"Research","slug":"Research","link":"/categories/Research/"},{"name":"Life","slug":"Life","link":"/categories/Life/"}]}
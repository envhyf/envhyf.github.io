{"pages":[{"title":"About Me","text":"Yufang Hao / éƒå®‡æ”¾I am an environmental scientist at the Paul Scherrer Institute (PSI) in Switzerland. My research focuses on understanding the sources, transformation, and impacts of air pollution by integrating advanced analytical chemistry and atmospheric modeling. Currently, I develop and apply high-resolution mass spectrometry techniques to investigate the molecular composition of organic aerosols, and combine these observations with regional chemical transport models such as GEOS-Chem to assess pollution sources and associated health risks in regions including South Asia and Africa. æˆ‘åœ¨ç‘å£«ä¿ç½—è°¢å°”ç ”ç©¶æ‰€ï¼ˆPSIï¼‰ä»äº‹ç¯å¢ƒç§‘å­¦æ–¹é¢çš„ç ”ç©¶å·¥ä½œï¼Œç ”ç©¶èšç„¦äºé€šè¿‡å…ˆè¿›çš„åˆ†æåŒ–å­¦æ‰‹æ®µä¸å¤§æ°”æ¨¡å¼æ¨¡æ‹Ÿï¼Œæ·±å…¥ç†è§£ç©ºæ°”æ±¡æŸ“çš„æ¥æºã€è½¬åŒ–è¿‡ç¨‹åŠå…¶å½±å“æœºåˆ¶ã€‚ ç›®å‰ï¼Œæˆ‘è‡´åŠ›äºå¼€å‘å¹¶åº”ç”¨é«˜åˆ†è¾¨ç‡è´¨è°±æŠ€æœ¯ï¼Œä»¥åˆ†å­æ°´å¹³è§£ææœ‰æœºæ°”æº¶èƒ¶çš„ç»„æˆç‰¹å¾ï¼Œå¹¶ç»“åˆ GEOS-Chem ç­‰åŒºåŸŸåŒ–å­¦ä¼ è¾“æ¨¡å‹ï¼Œè¯„ä¼°å—äºšã€éæ´²ç­‰åŒºåŸŸçš„æ±¡æŸ“æ¥æºåŠå…¶å¥åº·é£é™©ã€‚ ğŸ“„ Personal CV/ä¸ªäººç®€å† ğŸ“§ yufang.hao@psi.chğŸŒ WebsiteğŸ“š Google ScholarğŸ§ª ORCID ğŸ“ Education / æ•™è‚²èƒŒæ™¯Peking University åŒ—äº¬å¤§å­¦Ph.D. in Environmental Engineering ï¼ˆç¯å¢ƒå·¥ç¨‹åšå£«ï¼‰2014 â€“ 2020 Dalian University of Technology å¤§è¿ç†å·¥å¤§å­¦B.Sc. in Environmental Engineering ï¼ˆç¯å¢ƒå·¥ç¨‹å­¦å£«ï¼‰2010 â€“ 2014 ğŸ”¬ Skills / ä¸“ä¸šæŠ€èƒ½ Mass Spectrometry è´¨è°±åˆ†æ: Orbitrap, EESI-TOF, Aerosol Mass Spectrometer (AMS) Atmospheric Modeling å¤§æ°”æ¨¡æ‹Ÿ: GEOS-Chem, WRF, FLEXPART, CALPUFF Coding &amp; Data Analysis ç¼–ç¨‹ä¸æ•°æ®å¤„ç†: Python, R","link":"/about/index.html"},{"title":"","text":"window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"8067a867-0e45-4414-8f89-30b2e0d6291e\")) { Plotly.newPlot( \"8067a867-0e45-4414-8f89-30b2e0d6291e\", [{\"x\":[100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140,50,60,70,80,90,100,110,120,130,140],\"type\":\"scatter\"}], {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Fake Spectrum for Coal-like Emissions\"},\"xaxis\":{\"title\":{\"text\":\"mz\"}},\"yaxis\":{\"title\":{\"text\":\"Raw signal\"}}}, {\"responsive\": true} ) };","link":"/html/test_spectra.html"},{"title":"","text":"window.PlotlyConfig = {MathJaxConfig: 'local'}; /** * plotly.js v2.12.1 * Copyright 2012-2022, Plotly, Inc. * All rights reserved. * Licensed under the MIT license */ !function(t){if(\"object\"==typeof exports&&\"undefined\"!=typeof module)module.exports=t();else if(\"function\"==typeof define&&define.amd)define([],t);else{(\"undefined\"!=typeof window?window:\"undefined\"!=typeof global?global:\"undefined\"!=typeof self?self:this).Plotly=t()}}((function(){return function t(e,r,n){function i(o,s){if(!r[o]){if(!e[o]){var l=\"function\"==typeof require&&require;if(!s&&l)return l(o,!0);if(a)return a(o,!0);var c=new Error(\"Cannot find module '\"+o+\"'\");throw c.code=\"MODULE_NOT_FOUND\",c}var u=r[o]={exports:{}};e[o][0].call(u.exports,(function(t){return i(e[o][1][t]||t)}),u,u.exports,t,e,r,n)}return r[o].exports}for(var a=\"function\"==typeof require&&require,o=0;oe.target.column){var r=R(e,t);return t.y1-r}if(e.target.column>t.target.column)return R(t,e)-e.y1}return t.circular&&!e.circular?\"top\"==t.circularLinkType?-1:1:e.circular&&!t.circular?\"top\"==e.circularLinkType?1:-1:t.circular&&e.circular?t.circularLinkType===e.circularLinkType&&\"top\"==t.circularLinkType?t.target.column===e.target.column?t.target.y1-e.target.y1:e.target.column-t.target.column:t.circularLinkType===e.circularLinkType&&\"bottom\"==t.circularLinkType?t.target.column===e.target.column?e.target.y1-t.target.y1:t.target.column-e.target.column:\"top\"==t.circularLinkType?-1:1:void 0}));var s=i.y0;a.forEach((function(t){t.y0=s+t.width/2,s+=t.width})),a.forEach((function(t,e){if(\"bottom\"==t.circularLinkType){for(var r=e+1,n=0;r1&&n.sort((function(t,e){if(!t.circular&&!e.circular){if(t.source.column==e.source.column)return t.y0-e.y0;if(!V(t,e))return t.y0-e.y0;if(e.source.column0);else if(0==o&&1==a)s=i.y1-i.y0,i.y0=T/2-s/2,i.y1=T/2+s/2;else if(o==n-1&&1==a)s=i.y1-i.y0,i.y0=T/2-s/2,i.y1=T/2+s/2;else{var l=e.mean(i.sourceLinks,m),c=e.mean(i.targetLinks,d),u=((l&&c?(l+c)/2:l||c)-p(i))*t;i.y0+=u,i.y1+=u}}))}))}function y(){c.forEach((function(e){var r,n,i,o=a,s=e.length;for(e.sort(f),i=0;i0&&(r.y0+=n,r.y1+=n),o=r.y1+t;if((n=o-t-T)>0)for(o=r.y0-=n,r.y1-=n,i=s-2;i>=0;--i)(n=(r=e[i]).y1+t-o)>0&&(r.y0-=n,r.y1-=n),o=r.y0}))}}function V(t){t.nodes.forEach((function(t){t.sourceLinks.sort(u),t.targetLinks.sort(c)})),t.nodes.forEach((function(t){var e=t.y0,r=e,n=t.y1,i=n;t.sourceLinks.forEach((function(t){t.circular?(t.y0=n-t.width/2,n-=t.width):(t.y0=e+t.width/2,e+=t.width)})),t.targetLinks.forEach((function(t){t.circular?(t.y1=i-t.width/2,i-=t.width):(t.y1=r+t.width/2,r+=t.width)}))}))}return z.nodeId=function(t){return arguments.length?(M=\"function\"==typeof t?t:s(t),z):M},z.nodeAlign=function(t){return arguments.length?(E=\"function\"==typeof t?t:s(t),z):E},z.nodeWidth=function(t){return arguments.length?(A=+t,z):A},z.nodePadding=function(e){return arguments.length?(t=+e,z):t},z.nodes=function(t){return arguments.length?(L=\"function\"==typeof t?t:s(t),z):L},z.links=function(t){return arguments.length?(C=\"function\"==typeof t?t:s(t),z):C},z.size=function(t){return arguments.length?(i=a=0,b=+t[0],T=+t[1],z):[b-i,T-a]},z.extent=function(t){return arguments.length?(i=+t[0][0],b=+t[1][0],a=+t[0][1],T=+t[1][1],z):[[i,a],[b,T]]},z.iterations=function(t){return arguments.length?(P=+t,z):P},z.circularLinkGap=function(t){return arguments.length?(I=+t,z):I},z.nodePaddingRatio=function(t){return arguments.length?(n=+t,z):n},z.sortNodes=function(t){return arguments.length?(O=t,z):O},z.update=function(t){return w(t,M),V(t),t.links.forEach((function(t){t.circular&&(t.circularLinkType=t.y0+t.y10)for(a=e.y0-=r,e.y1-=r,i=o-2;i>=0;--i)(r=(e=t[i]).y1+b-a)>0&&(e.y0-=r,e.y1-=r),a=e.y0}))}}function P(t){t.nodes.forEach((function(t){t.sourceLinks.sort(l),t.targetLinks.sort(s)})),t.nodes.forEach((function(t){var e=t.y0,r=e;t.sourceLinks.forEach((function(t){t.y0=e+t.width/2,e+=t.width})),t.targetLinks.forEach((function(t){t.y1=r+t.width/2,r+=t.width}))}))}return M.update=function(t){return P(t),t},M.nodeId=function(t){return arguments.length?(_=\"function\"==typeof t?t:o(t),M):_},M.nodeAlign=function(t){return arguments.length?(w=\"function\"==typeof t?t:o(t),M):w},M.nodeWidth=function(t){return arguments.length?(x=+t,M):x},M.nodePadding=function(t){return arguments.length?(b=+t,M):b},M.nodes=function(t){return arguments.length?(T=\"function\"==typeof t?t:o(t),M):T},M.links=function(t){return arguments.length?(k=\"function\"==typeof t?t:o(t),M):k},M.size=function(e){return arguments.length?(t=n=0,i=+e[0],y=+e[1],M):[i-t,y-n]},M.extent=function(e){return arguments.length?(t=+e[0][0],i=+e[1][0],n=+e[0][1],y=+e[1][1],M):[[t,n],[i,y]]},M.iterations=function(t){return arguments.length?(A=+t,M):A},M},t.sankeyCenter=function(t){return t.targetLinks.length?t.depth:t.sourceLinks.length?e.min(t.sourceLinks,i)-1:0},t.sankeyLeft=function(t){return t.depth},t.sankeyRight=function(t,e){return e-1-t.height},t.sankeyJustify=a,t.sankeyLinkHorizontal=function(){return n.linkHorizontal().source(y).target(x)},Object.defineProperty(t,\"__esModule\",{value:!0})}))},{\"d3-array\":107,\"d3-collection\":108,\"d3-shape\":119}],58:[function(t,e,r){(function(){var t={version:\"3.8.0\"},r=[].slice,n=function(t){return r.call(t)},i=self.document;function a(t){return t&&(t.ownerDocument||t.document||t).documentElement}function o(t){return t&&(t.ownerDocument&&t.ownerDocument.defaultView||t.document&&t||t.defaultView)}if(i)try{n(i.documentElement.childNodes)[0].nodeType}catch(t){n=function(t){for(var e=t.length,r=new Array(e);e--;)r[e]=t[e];return r}}if(Date.now||(Date.now=function(){return+new Date}),i)try{i.createElement(\"DIV\").style.setProperty(\"opacity\",0,\"\")}catch(t){var s=this.Element.prototype,l=s.setAttribute,c=s.setAttributeNS,u=this.CSSStyleDeclaration.prototype,f=u.setProperty;s.setAttribute=function(t,e){l.call(this,t,e+\"\")},s.setAttributeNS=function(t,e,r){c.call(this,t,e,r+\"\")},u.setProperty=function(t,e,r){f.call(this,t,e+\"\",r)}}function h(t,e){return te?1:t>=e?0:NaN}function p(t){return null===t?NaN:+t}function d(t){return!isNaN(t)}function m(t){return{left:function(e,r,n,i){for(arguments.length1;t(e[a],r)0?i=a:n=a+1}return n}}}t.ascending=h,t.descending=function(t,e){return et?1:e>=t?0:NaN},t.min=function(t,e){var r,n,i=-1,a=t.length;if(1===arguments.length){for(;++i=n){r=n;break}for(;++in&&(r=n)}else{for(;++i=n){r=n;break}for(;++in&&(r=n)}return r},t.max=function(t,e){var r,n,i=-1,a=t.length;if(1===arguments.length){for(;++i=n){r=n;break}for(;++ir&&(r=n)}else{for(;++i=n){r=n;break}for(;++ir&&(r=n)}return r},t.extent=function(t,e){var r,n,i,a=-1,o=t.length;if(1===arguments.length){for(;++a=n){r=i=n;break}for(;++an&&(r=n),i","link":"/html/Coal_POA_SOA_spectrum_comparsion+new.html"}],"posts":[{"title":"åœ¨ä½ç‰ˆæœ¬Linuxå®‰è£…ç§‘å­¦è®¡ç®—åº“çš„æ›²æŠ˜ä¹‹è·¯","text":"ä»¥å‰å¤„ç†WRFç­‰æ°”è±¡æ¨¡å‹çš„è¾“å‡ºæ–‡ä»¶ï¼Œæ€»æ˜¯ä¸‹è½½åˆ°æœ¬åœ°ç”µè„‘åšåå¤„ç†åˆ†æã€‚ç”±äºè®¡ç®—é‡ä¸æ–­å¢åŠ ï¼Œæ¨¡æ‹Ÿç”Ÿæˆçš„æ–‡ä»¶å¾€å¾€ä¼šå¾ˆå¤§ã€‚å› è€Œï¼Œæˆ‘è€ƒè™‘ç›´æ¥åœ¨æœåŠ¡å™¨ä¸­å¤„ç†æ•°æ®ã€‚æœ¬æ¥æ˜¯å¾ˆå®¹æ˜“çš„äº‹æƒ…ï¼Œå´å› ä¸ºè¯¾é¢˜ç»„æœåŠ¡å™¨çš„ç³»ç»Ÿç‰ˆæœ¬è¾ƒæ—§ï¼Œåœ¨å®‰è£…æœ‰å…³å·¥å…·æ—¶è€—å»äº†ä¸å°‘æ—¶é—´ã€‚åœ¨æ­¤è®°å½•æˆ‘çš„æ¢ç´¢è¿‡ç¨‹ã€‚ æœåŠ¡å™¨çš„åŸºæœ¬ä¿¡æ¯ 123456$cat /etc/redhat-releaseCentOS release 5.9 (Final)$gcc --versiongcc (GCC) 4.9.4$ncdump --versionnetcdf library version 4.6.1 1. gdalåº“å®‰è£…gdalæ˜¯ä¸€ä¸ªå¸¸ç”¨çš„åœ°å­¦æ•°æ®åº“ï¼Œå¯ç”¨äºè¯»å–ã€ç¼–å†™netCDFç­‰æ ¼å¼çš„æ•°å€¼æ–‡ä»¶ã€‚å°è¯•åˆ©ç”¨condaå·¥å…·å®‰è£…ï¼š 1$conda install -c conda-forge gdal å®‰è£…å®Œæ¯•åï¼Œæ— æ³•å¯åŠ¨pythonï¼Œé”™è¯¯ä¿¡æ¯å¦‚ä¸‹ï¼š 1python: /lib64/libc.so.6: version `GLIBC_2.7' not found (required by /home/hyf/anaconda2/bin/../lib/libpython2.7.so.1.0) ä¸Šæ–‡å·²è¿°ï¼ŒæœåŠ¡å™¨ç³»ç»Ÿä¸ºCent OS 5.9ç‰ˆæœ¬ï¼Œå¹¶ä¸æ”¯æŒglibc_2.7ã€‚å¯ç”¨ä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œæ£€æŸ¥ï¼š 12345678910$strings /lib64/libc.so.6 |grep GLIBC_ GLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_PRIVATE åŒæ—¶ï¼Œgdalå®‰è£…è¿‡ç¨‹condaç¯å¢ƒä¹Ÿå‡çº§è‡³Anaconda5ï¼Œä¸å†é€‚é…äºCentOS 5.X/glibc_2.5çš„ç³»ç»Ÿç¯å¢ƒï¼Œå¯¼è‡´Pythonç¯å¢ƒå´©åï¼Œæ— æ³•å¯åŠ¨ã€‚å› è€Œï¼Œæˆ‘åªå¾—é‡æ–°å®‰è£…Anacondaï¼Œå¹¶è®¾ç½®å®‰è£…packageé˜¶æ®µä¸å‡çº§condaç¯å¢ƒè‡³5.xç‰ˆæœ¬ï¼Œä»¥é¿å…è¯¥é”™è¯¯çš„å†æ¬¡å‡ºç°ã€‚ 1234567# å·²å®‰è£…çš„åº“æ–‡ä»¶åç§°å­˜ä¸€ä¸‹printf '%s\\n' ~/anaconda2/pkgs/* | paste -sd \",\" - &gt; log.csv# å¸è½½anancondarm -rf ~/anaconda2rm -rf ~/.condarc ~/.conda ~/.continuum#é‡æ–°å®‰è£…Anacondaï¼Œåœ¨å…¶ä¸‹è½½æ–‡ä»¶å¤¹å†…è¿›è¡Œ./Anaconda2-4.0.0-Linux-x86_64.sh å‚è€ƒconda github issue#6041ï¼ŒæŒ‰å¦‚ä¸‹æŒ‡ä»¤å¯¹condaç¯å¢ƒè¿›è¡Œé™åˆ¶ï¼Œé˜²æ­¢å…¶å‡çº§ 123$conda config --set auto_update_conda false# æ–°çš„é”™è¯¯å†æ¬¡å‡ºç°Error: Error key must be one of always_yes, root_dir, channel_alias, add_anaconda_token, add_pip_as_python_dependency, show_channel_urls, update_dependencies, changeps1, allow_softlinks, anaconda_upload, binstar_upload, use_pip, offline, allow_other_channels, add_binstar_token, ssl_verify, always_copy, not auto_update_conda æŸ¥è¯¢ç½‘ç»œèµ„æ–™å¾—çŸ¥ï¼Œåœ¨conda 4.2ç‰ˆæœ¬åï¼Œè¯¥bugå¾—ä»¥ä¿®å¤ã€‚å› è€Œconda install conda =4.2æŒ‡ä»¤åå†å¼ºåˆ¶å…¶ä¸å‡çº§ï¼Œä¸å†å‡ºç°ä¸Šè¿°é”™è¯¯ã€‚ é‡æ–°å®‰è£…gdalåº“æ–‡ä»¶ï¼Œpythonå¯æ­£å¸¸å¯ç”¨ï¼Œä½†åœ¨è°ƒç”¨gdalåº“å‡½æ•°æ—¶ï¼Œåˆå‡ºç°äº†æ–°çš„é”™è¯¯ï¼ˆà¼¼ Í àºˆ Ä¹Ì¯ Í àºˆ à¼½ï¼‰ï¼š 12from osgeo import gdal&gt;ImportError: /home/hyf/anaconda2/lib/python2.7/site-packages/osgeo/../../../libgdal.so.20: ELF file OS ABI invalid å‚è€ƒè¿™ç¯‡æ–‡ç« çš„è§£é‡Šï¼Œè¯¥é”™è¯¯ä¸Šæ–‡â€œglibc_2.7â€æŠ¥é”™æ˜¯ç±»ä¼¼çš„ã€‚ç³»ç»Ÿçš„ldd versionåŒæ ·é™ˆæ—§(2.5)ï¼Œæ— æ³•æ”¯æŒgdalåº“æ­£å¸¸ä½¿ç”¨ã€‚å°†Cent OSå‡çº§æˆ6.xç‰ˆæœ¬å¯ä¸€åŠ³æ°¸é€¸åœ°è§£å†³è¿™ç±»é—®é¢˜ï¼Œä½†æˆ‘å¹¶æ— æƒé™ï¼Œä¹Ÿæ— å†³å¿ƒé‡æ–°é…ç½®æ•´ä¸ªç³»ç»Ÿã€‚ åœ¨åå¤å®‰è£…æµ‹è¯•æœªæœåï¼Œæˆ‘å†³å®šæ”¾å¼ƒâ•®(â•¯_â•°)â•­ï¼Œè½¬å‘å®‰è£…å…¶ä»–æ”¯æŒè¯»å–netcdfæ ¼å¼çš„pythonåº“ã€‚åœ¨æ­¤æœŸé—´ï¼Œæˆ‘è¿˜å°è¯•åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…äº†Rè¯­è¨€ç¯å¢ƒå’Œncdf packageï¼Œä»¥åæœ‰æ—¶é—´ä¸“æ–‡è®°è¿°ã€‚ 2. netCDF4-pythonåº“å®‰è£…é¦–å…ˆå°è¯•Anacondaç½‘ç«™æä¾›çš„æ–¹æ³•ï¼Œç›´æ¥åˆ©ç”¨condaå·¥å…·å®‰è£… 1$conda install -c anaconda netcdf4 å®‰è£…åï¼Œå‡ºç°pythonæ— æ³•å¯åŠ¨çš„é”™è¯¯ï¼Œå…·ä½“ä¸ºâ€œimportError: cannot import name md5â€ã€‚Stackoverflowä¸Šç±»ä¼¼é¢˜çš„å›ç­”è¡¨æ˜ï¼Œè¯¥é”™è¯¯æ˜¯åœ¨anaocnda5.0.1ç‰ˆæœ¬ä¸­å¾—ä»¥ä¿®å¤ã€‚ä½†æ˜¯ï¼Œæˆ‘ä¸èƒ½å®‰5.xç‰ˆæœ¬çš„condaå•Šà®‡Ğ´à®‡ï¼Œäºæ˜¯å†æƒ³å…¶ä»–åŠæ³•ã€‚ é‡æ–°å®‰è£…anacondaç¯å¢ƒåï¼Œæˆ‘å°è¯•ç”¨æ‰‹åŠ¨install from sourceçš„æ–¹æ³•æ¥å®‰è£…ï¼Œå…·ä½“netcdf4-pythonçš„githubä¸»é¡µä¸­çš„ä»‹ç»ï¼š 1234567891011121314$git clone https://github.com/Unidata/netcdf4-python.gitcd netcdf4-python# ä¿®æ”¹ setup.cfgä¸­çš„netcdf, hdf5å­˜æ”¾è·¯å¾„sed -i '/\\[directories\\]/a \\HDF5_dir = $DIR/hdf5 \\netCDF4_dir = $DIR/netcdf' setup.cfg# run setup.py$python setup.py build$python setup.py install# run test.py$cd test$python run_all.py ä¸å‡ºæ„å¤–ï¼Œå®‰è£…è¿‡ç¨‹è‚¯å®šæ˜¯è¦æŠ¥é”™çš„ï¼Œå…·ä½“çš„é”™è¯¯ä¿¡æ¯å¦‚ä¸‹ï¼š 1234/usr/bin/ld: /disk2/hyf/lib/netcdf/lib/libnetcdf.a(libdispatch_la-att.o): relocation R_X86_64_32 against `a local symbol' can not be used when making a shared object; recompile with -fPIC/disk2/hyf/lib/netcdf/lib/libnetcdf.a: could not read symbols: Bad valuecollect2: error: ld returned 1 exit statuserror: command 'gcc' failed with exit status 1 è¯¥ç»“æœè¡¨æ˜ï¼Œnetcdfåº“åœ¨å®‰è£…è¿‡ç¨‹ï¼Œå…³é—­äº†enable-sharedï¼Œå…±äº«åº“æœªè¿›è¡Œç¼–è¯‘ï¼Œå¹¶è¿›ä¸€æ­¥å¯¼è‡´äº†\"a local symbol\" can not be used â€¦çš„é—®é¢˜ã€‚æ­£å¥½åœ¨flexpartå®‰è£…è¿‡ç¨‹(å‚è§ä¹‹å‰çš„åšæ–‡FLEXPART installation notes)ä¸­ï¼Œæˆ‘åˆå®‰è£…äº†ä¸€å¥—netCDFåº“ï¼Œåœ¨è¯¥ç¯å¢ƒä¸‹é‡æ–°æ‰‹åŠ¨å®‰è£…ï¼Œè¯¥é”™è¯¯ä¸å†æ˜¾ç¤ºã€‚ 123456789101112# change netCDF$source ~/.bashrc_forFlex$ncdump --versionnetcdf library version 4.6.1# run setup.py$python setup.py build$python setup.py install# run test.py$cd test$python run_all.py æˆåŠŸå®‰è£…åï¼Œåœ¨/netcdf4-python/test/æ–‡ä»¶å¤¹ä¸‹å¯è¿è¡Œï¼Œå¹¶è°ƒç”¨netCDF4åº“å‡½æ•°(from netCDF4 import Dataset)ã€‚æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œå†™netCDFæ ¼å¼æ–‡ä»¶(mode=â€™wâ€™)ä¸€åˆ‡æ­£å¸¸ï¼Œä½†æœ‰ä¸¤ä¸ªè‡´å‘½é—®é¢˜ï¼š1. ä»…èƒ½åœ¨è¯¥æ–‡ä»¶å¤¹å†…ï¼Œæ–¹å¯æˆåŠŸimport netCDF4 ï¼Œ2. æ— æ³•è¯»å–ç°æœ‰çš„ncæ•°æ®ï¼ŒæŠ¥é”™å¦‚ä¸‹ï¼š 1filepath method not enabled. To enable, install Cython, make sure you have version 4.1.2 or higher of the netcdf C lib, and rebuild netcdf4-python. Cythonæˆ‘å·²ç»å®‰è£…äº†ï¼Œnetcdf C ç‰ˆæœ¬ä¹Ÿé«˜äº4.1.2ï¼Œåˆ°åº•æ˜¯å“ªé‡Œå‡ºé—®é¢˜äº†å‘¢ï¼Ÿå‚è€ƒgithubä¸Šnetcdf-python issue#263ï¼Œæˆ‘æ£€æŸ¥äº†python setup.py buildæ—¶çš„outputï¼Œå…¶æ˜ç¡®æ˜¾ç¤º 123456netcdf lib does not have group rename capabilitynetcdf lib does not have nc_inq_path functionnetcdf lib does not have nc_inq_format_extended functionnetcdf lib does not have nc_open_mem functionnetcdf lib does not have cdf-5 format capabilitynetcdf lib does not have netcdf4 parallel functions åˆæ­¥æ–­å®šâ€nc_inq_path functionâ€ çš„ç¼ºå¤±ï¼Œæ˜¯å¯¼è‡´â€filepath method not enabledâ€çš„æ ¹æœ¬åŸå› ã€‚åå¤æœç´¢ç›¸å…³èµ„æ–™ï¼Œä¹ŸæœªæŸ¥è¯¢åˆ°ç›¸ä¼¼çš„é—®é¢˜ä»¥åŠè§£å†³æ–¹æ³•ã€‚ å®Œå…¨æ”¾å¼ƒå‰ï¼Œæˆ‘å°è¯•conda install netcdf4 ï¼Œç«Ÿç„¶æ„å¤–æˆåŠŸï¼Œå¯ä»¥æ­£å¸¸è¯»å–ncæ ¼å¼æ–‡ä»¶äº†ã€‚ âœ§ï½¡Ù©(ËŠá—œË‹)Ùˆâœ§*ï½¡ âœ§ï½¡Ù©(ËŠá—œË‹)Ùˆâœ§*ï½¡ âœ§ï½¡Ù©(ËŠá—œË‹)Ùˆâœ§*ï½¡ å‚è€ƒèµ„æ–™1. GCC install 2. NetCDF in R 3. Installing NetCDF and R â€˜ncdfâ€™ 4. Aqua-Duct installation guide","link":"/2018/09/04/11.%5BModel%5D%E6%B0%94%E8%B1%A1%E6%A8%A1%E6%8B%9FNetCDF%E8%BE%93%E5%87%BA%E6%96%87%E4%BB%B6%E5%90%8E%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85/"},{"title":"Post processing of FLEXPART-WRF output","text":"In this post, I present some simple programs written in Python for post-processing the flexpart-wrf output. It mainly contains several aspects, data merging, data processing and data visualization. I will also show some tips tp creat self-defined colormaps for nice plots. PS: All th codes are also uploaded in my GitHub respority PyFlex Merging datasetsIn the output path, the result (e.g., flxout_d01_20160304_030000.nc ) for each run was saved as an independent file. It would be inconvenient to loop them for every post-processing function. Therefore, I zip them into a hdf5 file. Here is the code and some instructions. 1234567891011121314151617181920212223242526272829303132333435363738394041\"\"\"pre-reading an template file for capture the geogrid information\"\"\"test_file_path = output_path+\"flxout_d01_20160304_030000.nc\"test_file = Dataset(test_file_path)pes = np.zeros_like(test_file['/CONC'][:,0,0,0,:,:].sum(axis = 0))pos=Dataset(output_path+\"header_d01.nc\")xx = pos['XLONG'][:]yy = pos['XLAT'][:]\"\"\"Creat HDF FILES\"\"\"hdf_filename ='Chifeng_PES_72hour.hdf'os.system(\"rm \"+hdf_filename)with h5py.File(hdf_filename, \"w\") as nf: dset = nf.create_dataset(\"test\", (100,), dtype='i')nf = h5py.File(hdf_filename, 'r+')grp = nf.create_group(\"PES\")dset1 = grp.create_dataset(\"lon\", data= xx)dset2 = grp.create_dataset(\"lat\", data= yy)\"\"\"Loop the datasets for each month\"\"\"\"\"\"Take the winter months for example\"\"\"print \"## WINTER ##\"for mo in ['01','02','12']: ## The arrays include 6 dimensions, time, ageclass, releases (species), Z, Y, X ## Since most sources are located within the footprint layer (~300 m above the ground), we only calculated the residence time of bottom vertical layer. pes = np.zeros_like(test_file['/CONC'][:,0,0,0,:,:].sum(axis = 0)) t = 0 Dir = output_path+\"winter/\"+mo+\"/\" files = os.listdir(Dir) files = sorted(files) for file in files: filename,extname = os.path.splitext(file) if filename[0:4] == 'flxo': fc = Dataset(Dir+file) if fc['/CONC'].shape[0]&gt;70.0: pes+=fc['/CONC'][:,0,0,0,:,:].sum(axis = 0) t+=1 print mo+\" \"+str(t) pes = pes/t dset = grp.create_dataset(mo, data= pes) print 'Done' nf.close() Then, we can download the â€œ.hdf5â€ file for further analysis. Colormap settingA good colormap can really improve the representability for the figure. Here, I recommend and generate three colormaps using different approaches. cmap1: generated by user-defined RGB values cmap2: subsets of an existing 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455## CMAP1 from pflexible.py on https://git.nilu.no/from matplotlib.colors import ListedColormapcolor_list = [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 9.9607843e-01, 9.1372549e-01, 1.0000000e+00, 9.8431373e-01, 8.2352941e-01, 1.0000000e+00, 9.6470588e-01, 7.1764706e-01, 1.0000000e+00, 9.3333333e-01, 6.0000000e-01, 1.0000000e+00, 8.9019608e-01, 4.4705882e-01, 1.0000000e+00, 8.3137255e-01, 2.0000000e-01, 1.0000000e+00, 7.5686275e-01, 0.0000000e+00, 1.0000000e+00, 6.6274510e-01, 0.0000000e+00, 1.0000000e+00, 5.4901961e-01, 0.0000000e+00, 1.0000000e+00, 4.0784314e-01, 0.0000000e+00, 1.0000000e+00, 2.4705882e-01, 0.0000000e+00, 1.0000000e+00, 7.4509804e-02, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 2.8235294e-01, 1.0000000e+00, 0.0000000e+00, 4.8627451e-01, 1.0000000e+00, 0.0000000e+00, 6.3137255e-01, 1.0000000e+00, 0.0000000e+00, 7.4509804e-01, 1.0000000e+00, 0.0000000e+00, 8.4705882e-01, 1.0000000e+00, 0.0000000e+00, 9.3725490e-01, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 9.7647059e-01, 0.0000000e+00, 1.0000000e+00, 8.9411765e-01, 0.0000000e+00, 1.0000000e+00, 8.0000000e-01, 0.0000000e+00, 1.0000000e+00, 6.9019608e-01, 0.0000000e+00, 1.0000000e+00, 5.6470588e-01, 0.0000000e+00, 1.0000000e+00, 4.0000000e-01, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 3.9607843e-01, 1.0000000e+00, 0.0000000e+00, 5.6470588e-01, 1.0000000e+00, 0.0000000e+00, 6.9019608e-01, 1.0000000e+00, 0.0000000e+00, 7.9607843e-01, 1.0000000e+00, 0.0000000e+00, 8.9411765e-01, 1.0000000e+00, 0.0000000e+00, 9.7647059e-01, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 9.4509804e-01, 0.0000000e+00, 1.0000000e+00, 8.7450980e-01, 0.0000000e+00, 1.0000000e+00, 7.9215686e-01, 0.0000000e+00, 1.0000000e+00, 7.0588235e-01, 0.0000000e+00, 1.0000000e+00, 6.0392157e-01, 0.0000000e+00, 1.0000000e+00, 4.8235294e-01, 0.0000000e+00, 1.0000000e+00, 3.1372549e-01, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.4901961e-01, 1.0000000e+00, 0.0000000e+00, 3.3333333e-01, 1.0000000e+00, 0.0000000e+00, 4.4705882e-01, 1.0000000e+00, 0.0000000e+00, 5.3725490e-01, 1.0000000e+00, 0.0000000e+00, 6.1176471e-01, 9.7647059e-01, 0.0000000e+00, 6.6666667e-01, 8.9411765e-01, 0.0000000e+00, 6.6666667e-01, 7.9607843e-01, 0.0000000e+00, 6.3921569e-01, 6.9019608e-01, 0.0000000e+00, 5.9215686e-01, 5.6470588e-01, 0.0000000e+00, 5.0980392e-01, 3.9607843e-01, 0.0000000e+00, 3.8039216e-01]color_list = np.reshape(color_list, (-1, 3))name = 'flexpart_cmap'cmap1 = ListedColormap(color_list, name) 12345678910## CMAP2 extracted from an existing colormapimport matplotlib.pyplot as pltimport matplotlib.colors as colorsimport numpy as npdef truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100): new_cmap = colors.LinearSegmentedColormap.from_list( 'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n))) return new_cmapcmap2 = truncate_colormap(plt.cm.gist_ncar, 0.3,0.9) cmap3 is an interesting colormap which I clipped from an existing figure. I cut the colorbar from the above figure, read its RGB values, and generate a new colormap 12345678910111213## CMAP3 from a current colorbarfrom PIL import Imageimport pandas as pdim = Image.open('./colorbar_3.png')rgb_im = im.convert('RGB')r, g, b = rgb_im.getpixel((1, 1))k = []for i in range(18,321,1): k.append(rgb_im.getpixel((i,20)))color_list = np.array(k) color_list = np.array([(i/255.0,j/255.0,k/255.0) for i,j,k in color_list])name = 'copy_cmap'cmap3 = ListedColormap(color_list, name) 1234567891011121314# plotting themfrom mpl_toolkits.axes_grid1 import make_axes_locatabledef sample_plot(ax,arr,cmap): s = ax.imshow(arr, interpolation='nearest', cmap=cmap) divider = make_axes_locatable(ax) cax = divider.new_vertical(size=\"5%\", pad=0.3, pack_start=True) fig.add_axes(cax) fig.colorbar(s, cax=cax, orientation=\"horizontal\") arr = np.linspace(0, 50, 100).reshape((10, 10))fig, ax = plt.subplots(ncols=3)sample_plot(ax[0],arr, cmap1)sample_plot(ax[1],arr, cmap2)sample_plot(ax[2],arr, cmap3) Finally, we can read and visualize the datasets. 123456789101112131415161718192021222324252627282930313233343536373839404142# reading the hdf5 data PES_data = h5py.File(\"/Users/HYF/Downloads/Chifeng_PES_pbl_72hour.hdf\")lon = PES_data['/PES/lon'][:]lat = PES_data['/PES/lat'][:]# sample arrray of the results in January.PES_1 = PES_data['/PES/12'][:]PES_2 = PES_data['/PES/01'][:]PES_3 = PES_data['/PES/02'][:]PES_winter = (PES_1+PES_2+PES_12)/3.0 # plot the winter retroplume with colormap1fig = plt.figure(figsize=(4,3), frameon=True)proj = ccrs.LambertConformal(central_latitude = 42.715, central_longitude = 118.79, standard_parallels = (30, 60))ax =plt.subplot(111, projection = proj)mask_v = np.ma.masked_less_equal(PES_winter,0)cs = ax.pcolormesh(lon,lat,mask_v,transform=ccrs.PlateCarree(),cmap =cmap1,alpha = 0.85,zorder=1,norm=matplotlib.colors.LogNorm(), vmin = 1E-9) ax.set_extent([105,133,31,52], crs=ccrs.PlateCarree())ax.coastlines(linewidth = 0.5,resolution='50m')ax.add_feature(cfeature.BORDERS, linewidth=0.5)## the xticks, yticks setting is referenced from ## https://github.com/ARM-DOE/pyart/blob/master/pyart/graph/radarmapdisplay_cartopy.py## I have not uploaded the original function here to make the code more tight.fig.canvas.draw()ax.gridlines(xlocs=xticks, ylocs=yticks, color='gray', alpha=0.5, linestyle='--',linewidth = 0.75)ax.xaxis.set_major_formatter(LONGITUDE_FORMATTER) ax.yaxis.set_major_formatter(LATITUDE_FORMATTER)lambert_xticks(ax, xticks)lambert_yticks(ax, yticks) pos1 = ax.get_position()tax = fig.add_axes([pos1.x0,pos1.y1,pos1.x1-pos1.x0,0.03])#x0,y0,long,widthtax.get_xaxis().set_visible(False)tax.get_yaxis().set_visible(False)tax.set_facecolor('#FFE5CE')tax.text(0.4,0.3,\"Winter\",color = 'k',fontsize =9,fontweight = 'bold',transform=tax.transAxes)","link":"/2018/11/06/12.%5BModel%5DFLEXPART-WRF%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C%E5%90%8E%E5%A4%84%E7%90%86/"},{"title":"Satellite-Based Air Quality Mapping in Target Regions å«æ˜Ÿæ•°æ®ç»˜åˆ¶ç‰¹å®šåŒºåŸŸçš„ç©ºæ°”æ±¡æŸ“åœ°å›¾","text":"Iâ€™m writing today about downloading, handling, and plotting satellite derived air pollution maps with cartopy and fiona using Python. One key task in this post is to clip a raster-like (2-d array) dataset with a polygon in pure Python environment (i.e., no need for ArcGIS or QGIS GUI-based software). The satellite sensor can offer critical supplementary data of several atmospheric species, e.g., SO2, NO2, PM2.5. Comparaing to ground-based monitoring which might be sparse in some areas (e.g., Africa, South America, oceans), the satellite observation offers a full picture for better understanding the spatiotemporal patterns of some air pollutants. Below is an excerpt of a NO2 column maps within Chengyu urabn agglomeration in China. The tropospheric NO2 concentration derived from satellite remote sensing techniques have been used on global, regional and urban scales since the mid-nineties. After the launch of the Ozone Monitoring Instrument (OMI) in 2004, the spatial resolution (up to 13 x 24 km2) ave been largely improved. The vetical column NO2 dataset have contributed to reveal the hotspots of air pollution around the world, or show the episodes with long-range transport. The TEMIS website provide the image and data for NO2 concentration maps in individual days and monthly averages derived from different satellite sensors (e.g., OMI, GOME-2). Herein, I present the general processes to obtain DOMINO version 2.0 data Obtaining the datasetsWe start by downloading the essential data files using function of wget. 1234567#Creat an loop for downloading the monthly averages from 2013-2017import osfor i in ['2013','2014','2015','2016','2017']: for t in ['01','02','03','04','05','06','07','08','09','10','11','12']: filename = 'http://www.temis.nl/airpollution/no2col/data/omi/data_v2/'+i+'/'+t+'/'+'no2_'+i+t+ '.grd.gz' os.system('wget %s' %filename)print \"DOWNLOADING COMPLETE\" Merging the datasetsWe then define a reading function and open those files to obtain essential information. In the case of 2013, there are 12 monthly averages need to read and merge for further analysis. 12345678910111213141516171819202122232425262728import numpy as npdef read_grd(filename): with open(filename) as infile: ncols = int(infile.readline().split()[1]) nrows = int(infile.readline().split()[1]) xllcorner = float(infile.readline().split()[1]) yllcorner = float(infile.readline().split()[1]) cellsize = float(infile.readline().split()[1]) nodata_value = int(infile.readline().split()[1]) version = float(infile.readline().split()[1]) longitude = xllcorner + cellsize * np.arange(ncols) latitude = yllcorner + cellsize * np.arange(nrows) value = np.loadtxt(filename, skiprows=7) return longitude, latitude, value,nodata_valueset_year = 2013for i in range(0,12,1): # setting the file path which contains those original data. file_path = './NO2_POMINO/no2_'+str(set_year)+ '%02d' %(i+1)+'.grd' lon,lat,value,nodata_value = read_grd(file_path) value = value[::-1] value[value == nodata_value] = np.nan if i ==0: value_ = value.reshape(1,value.shape[0],value.shape[1]) else: value_ = np.vstack([value_,value[None, ...]])## np.nanmean() is utilized to compute the arithmetic mean igonring NaNsno2_2013 = np.nanmean(value_,axis = 0) ##Cut 2-d array by shapefile Next, we import the necessary library and the specific shapefile to clip the global 2-d map. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#1. transform the shapefile to fiona polygon objectimport fionafrom shapely.geometry import shape,Polygon, Pointcf_area = fiona.open(\"xxx.shp\")pol = cf_area.next()from shapely.geometry import shapegeom = shape(pol['geometry'])poly_data = pol[\"geometry\"][\"coordinates\"][0]poly = Polygon(poly_data)#2. substract the dataset that can cover the polygonx1 = np.array([t for t,j in (poly_data)]).min()x2 = np.array([t for t,j in (poly_data)]).max()y1 = np.array([j for t,j in (poly_data)]).min()y2 = np.array([j for t,j in (poly_data)]).max()extent = x1 -0.5, x2+0.5,y1-0.5, y2+0.5def find_nearest(array,value): idx = (np.abs(array-value)).argmin() return array[idx]nx_st = np.where(lon == (find_nearest(lon,extent[0])))[0]nx_en = np.where(lon == (find_nearest(lon,extent[1] )))[0]ny_st = np.where(lat == (find_nearest(lat,extent[2])))[0]ny_en = np.where(lat == (find_nearest(lat,extent[3] )))[0]lon_,lat_ = lon[nx_st:nx_en+1], lat[ny_st:ny_en+1]so2_data= so2_data[ny_st:ny_en+1, nx_st:nx_en+1] #3. test the dat points whether or not within the polygonimport timestart_time = time.time()sh = (len(lat_)*len(lon_),2)points = np.zeros(len(lat_)*len(lon_)*2).reshape(*sh)k = 0for i in range(0,len(lat_),1): for j in range(0,len(lon_),1): points[k] = np.array([lon_[j],lat_[i]],dtype=float) k+=1mask = np.array([poly.contains(Point(x, y)) for x, y in points]) mask = mask.reshape(len(lat_),len(lon_))print(\"--- %s seconds ---\" % (time.time() - start_time))print mask.shapenp.savetxt(\"mask_SO2.txt\",mask)#4. mask the data points outsied the polygondef shape_mask(array, lon_,lat_,mask): arr = np.zeros_like(array) for i in range(0,len(lat_),1): for j in range(0,len( lon_),1): if mask[i,j] == 1: arr[i,j] = array[i,j] else: arr[i,j] = -1 return arrno2_mask = shape_mask(no2_data, lon_,lat_,mask) The mask array as the rastered polygon is shown like this. 1234567fig = plt.figure()ax = back_main_clean(fig,extent)for spine in plt.gca().spines.values(): spine.set_visible(False)ax.outline_patch.set_visible(False)ax.pcolormesh(mask)plt.tight_layout() Visualization1234567891011121314151617181920from mpl_toolkits.axes_grid1 import make_axes_locatablefig = plt.figure()ax = back_main_clean(fig,extent) # the background map was prior-defined by specific uses.lon_x,lat_y = np.meshgrid(lon_,lat_)value_mask = np.ma.masked_array(no2_mask, np.isnan(no2_mask))value_mask = np.ma.masked_less(value_mask, 0.0)my_cmap = plt.cm.get_cmap('Spectral_r')my_cmap.set_under('w')cc = ax.pcolormesh(lon_x,lat_y,value_mask/100.0,cmap =my_cmap,alpha = 0.7,transform=ccrs.PlateCarree(),zorder =2,vmax = 25)#cbaxes = fig.add_axes([0.58, 0.12, 0.16, 0.015]) cbar = fig.colorbar(cc, cax=cbaxes, ticks=[4,12,20],orientation='horizontal')cbar.ax.set_title(r'$\\mathregular{NO_2\\ [10^{15}molec/cm^2]}$', fontsize = 8)cbar.ax.tick_params(color='k', direction='in',labelsize=8)for spine in plt.gca().spines.values(): spine.set_visible(False)ax.outline_patch.set_visible(False)plt.tight_layout()","link":"/2018/11/25/14.%5BCode%5DPlotting-satellite-derived-air-pollution-maps-using-Python/"},{"title":"Plotting topography map with hillshade ç»˜åˆ¶å¸¦é˜´å½±é«˜å…‰çš„åœ°å½¢å›¾","text":"Hillshade is the representation of the earthâ€™s surface under the radiation of sun. A terrain raster data can be better visualized by adding the information of hillshades. This blog will introduce some procrdures to overlay the hillshade with terrain for a nice picture. First, letâ€™s import the required packages. 12345from osgeo import gdal import numpy as np import matplotlib.pyplot as plt import elevation%matplotlib inline Second, we download and read a sample data 123456!eio --product SRTM3 clip -o DEM.tif --bounds -122.4 41.2 -122.1 41.3from osgeo import gdalraster = gdal.Open(\"./DEM.tif\", gdal.GA_ReadOnly)dem = raster.GetRasterBand(1).ReadAsArray()dem = dem[::-1] Then we calculated the hillshade using the function I referenced from Download and Process DEMs in Python 1234567891011def hillshade(array, azimuth, angle_altitude): # Source: http://geoexamples.blogspot.com.br/2014/03/shaded-relief-images-using-gdal-python.html x, y = np.gradient(array) slope = np.pi/2. - np.arctan(np.sqrt(x*x + y*y)) aspect = np.arctan2(-x, y) azimuthrad = azimuth*np.pi / 180. altituderad = angle_altitude*np.pi / 180. shaded = np.sin(altituderad) * np.sin(slope) \\ + np.cos(altituderad) * np.cos(slope) \\ * np.cos(azimuthrad - aspect) return 255*(shaded + 1)/2 Finally, we plotted three figures, one is the simple dem plot, the other two are the topography overlayed by hillshade using different alpha. 123456789101112131415161718fig = plt.figure(figsize=(12, 4))extent =[-122.4 ,41.2, -122.1, 41.3]ax = fig.add_subplot(131,projection=ccrs.Mercator())cax = ax.contourf(dem, np.arange(vmin, vmax, 10),extent=extent, cmap=terrain_cmap, vmin=vmin, vmax=vmax, origin='image')plt.title(r'$Original$',fontsize =12)ax = fig.add_subplot(132,projection=ccrs.Mercator())ax.matshow(hillshade(dem, 30, 30), extent=extent, cmap='Greys', alpha=.3, zorder=10)cax = ax.contourf(dem, np.arange(vmin, vmax, 10),extent=extent, cmap=terrain_cmap, vmin=vmin, vmax=vmax, origin='image')plt.title(r'$\\alpha=0.3$',fontsize =12)ax = fig.add_subplot(133,projection=ccrs.Mercator())ax.matshow(hillshade(dem, 30, 30), extent=extent, cmap='Greys', alpha=.8, zorder=10)cax = ax.contourf(dem, np.arange(vmin, vmax, 10),extent=extent, cmap=terrain_cmap, vmin=vmin, vmax=vmax, origin='image')plt.title(r'$\\alpha=0.8$',fontsize =12) References Create a Hillshade from a Terrain Raster in Python Earth analytics python Download and Process DEMs in Python","link":"/2018/11/09/13.%5BCode%5DPlotting%20topography%20map%20with%20using%20Python/"},{"title":"Two method to mask the raster data by specific geometry","text":"Sometimes, we need to clip or extract the raster image with polygon features, e.g., only focus on the percipitation within China using global dataset. This post will introduce two methods to mask the 2-d array-like dataset by a specific geometry using Python. The first method use GeoPandas module to test those raster coordidnates within/out of the shapefile The second method which I strongly recommended with faster speed was based on shapely module. 1. Geopandas12345678910111213141516171819202122232425262728293031# referencing https://stackoverflow.com/questions/47781496/python-using-polygons-to-create-a-mask-on-a-given-2d-gridimport numpy as npimport irisimport geopandas as gpd# reading the mask shapefilefname = r'xxx/china.shp'cn =gpd.read_file(fname)# four cornersx0,x1 = cn.bounds['minx'].values,cn.bounds['maxx'].valuesy0,y1 = cn.bounds['miny'].values,cn.bounds['maxy'].values# the map background functiondef back_main_clean(fig,extent): ax = fig.add_subplot(111, projection=ccrs.PlateCarree()) ax.set_extent(extents=extent, crs=ccrs.Geodetic()) for i in ax.spines.itervalues(): i.set_linewidth(0.01) fname = r'xxx/china_provinces.shp' shape_feature = ShapelyFeature(Reader(fname).geometries(), ccrs.PlateCarree(),facecolor='#FFFFFF', linestyle='--',edgecolor ='k', linewidth = 0.25, alpha =1) ax.add_feature(shape_feature, zorder =3) # initializing the raster datax = np.linspace(x0,x1,30)y = np.linspace(y0,y1,20)fig = plt.figure(figsize=(8, 5))ax = back_main_clean(fig, extent)xx,yy = np.meshgrid(x,y)zz = np.random.random((len(x),len(y)))plt.scatter(xx.reshape(-1),yy.reshape(-1),zorder =4,transform=ccrs.PlateCarree(), c=zz.reshape(-1),s=8) 12345678910111213# creating the mask array and counting the processing timeimport timestart = time.time()lon2 = xx.reshape(-1)lat2 = yy.reshape(-1)mask = []for lat, lon in zip(lat2, lon2): this_point = gpd.geoseries.Point(lon,lat) res = cn.contains(this_point) mask.append(res.values[0])mask = np.array(mask)mask= mask.reshape(len(y),len(x))print \"Process time: \" + str(time.time() - start) Process time: 5.11 Seconds 1234567# plotting the raster data extracted by Chinazz_mask = np.ma.masked_array(zz,mask) # noted that ~mask can simply transform to clip function.fig = plt.figure(figsize=(8, 5))ax = back_main_clean(fig, extent)xx,yy = np.meshgrid(x,y)zz = np.random.random((len(x),len(y)))plt.scatter(xx.reshape(-1),yy.reshape(-1),zorder =4,transform=ccrs.PlateCarree(), c=zz_mask.reshape(-1),s=8) 2. shapely.vectorized12345678910111213141516171819202122# referencing https://gist.github.com/pelson/9785576import fionaimport shapely.vectorizedfrom shapely.geometry import shapefname = r'xxx/china.shp'cn_area = fiona.open(fname)pol = cn_area.next()geom = shape(pol['geometry'])## four corner x0,x1 = geom.bounds[0],geom.bounds[2]y0,y1 = geom.bounds[1],geom.bounds[3]## creating the mask arrayx = np.linspace(x0,x1,30)y = np.linspace(y0,y1,20)xx,yy = np.meshgrid(x,y)import timestart = time.time()mask_ = shapely.vectorized.contains(geom, xx,yy)print \"Process time: \" + str(time.time() - start) Process time: 0.01 Seconds (huge advantage here) 12345678zz = np.random.random((len(x),len(y)))zz_mask = np.ma.masked_array(zz,~mask_)fig = plt.figure(figsize=(8, 5))ax = back_main_clean(fig, extent)xx,yy = np.meshgrid(x,y)# zz = np.random.random((len(x),len(y)))# plt.scatter(xx.reshape(-1),yy.reshape(-1),zorder =4,transform=ccrs.PlateCarree(),)plt.scatter(xx.reshape(-1),yy.reshape(-1),zorder =4,transform=ccrs.PlateCarree(), c=zz_mask.reshape(-1),s=5) Some additional tips12345# 1. the error during shapefile readingimport fionacn_area = fiona.open('xxx.shp')&gt; Recode from CP936 to UTF-8 not supported, treated as ISO8859-1 to UTF-8.cn_area = fiona.open('xxx.shp', encoding ='utf-8') 12345# 2. select the polygon by attributes within geopandas modulefname = r'xxx/china_provinces.shp'cn =gpd.read_file(fname, encoding ='utf-8')test_p = cn[cn['NAME']==u'é»‘é¾™æ±Ÿ']test_p.plot() 123456# 3. select the polygon by attributes within shapely modulewith fiona.open(fname, encoding='utf-8') as src: filtered = filter(lambda f: f['properties']['NAME']==u'é»‘é¾™æ±Ÿ', src)pol = filtered[0]#.next()geom = shape(pol['geometry'])geom 12345678# 4. select and output one polygon from multiple polygons by attributesimport fionawith fiona.open(fname, encoding='utf-8') as input: meta = input.meta with fiona.open('test_heilongjiang.shp','w',**meta) as output: for feature in input: if feature['properties']['NAME']==u'é»‘é¾™æ±Ÿ': output.write(feature)","link":"/2018/12/06/15.%5BCode%5DMethods-to-mask-raster-by-polygons/"},{"title":"Altering the resolution of a raster data using Python","text":"Sometimes, we need to transform the original raster dataset to a different resolution (usually coarser). In this post, I will introduce the simple workflow for cell resampling using Python. 1. Data import / preprocessingTo get started, first load the required library and then import the original raster data. In this case, we downloaded the Gross Domestic Product (GDP) distribution of China in 2010 with the resolution of 1 km x 1 km as the input. Whatâ€™s more, a subset of China outlined by a shapefile was also used for clipping that raster. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import warningswarnings.filterwarnings('ignore')import cartopyimport matplotlib.pyplot as pltfrom osgeo import gdalimport rasteriofrom rasterio.mask import maskimport geopandas as gpdfrom shapely.geometry import mappingimport skimage.transform as st# original data importfrom osgeo import gdalpathToRaster = r\"./cngdp2010.tif\"input_raster = gdal.Open(pathToRaster)prj=input_raster.GetProjection()srs=osr.SpatialReference(wkt=prj)if srs.IsProjected: print srs.GetAttrValue('projcs')print srs.GetAttrValue('geogcs')# Albers_Conic_Equal_Area# GCS_Unknown_datum_based_upon_the_Krassowsky_1940_ellipsoid# transforming the projection of the input_rasteroutput_raster = r'./cngdp2010_adjust.tif'gdal.Warp(output_raster,input_raster,dstSRS='EPSG:4326',dstNodata = -9999)# clipping the raster by polygoninput_shape = r'./boundary.shp'shapefile = gpd.read_file(input_shape)shapefile = shapefile.to_crs(epsg=4326)input_raster = r'./cngdp2010_adjust.tif' # input rastergeoms = shapefile.geometry.values # list of shapely geometriesoutput_raster = r'./2+26city_gdp.tif' # output rastergeometry = geoms[0] # shapely geometrygeoms = [mapping(geoms[0])]with rasterio.open(input_raster) as src: out_image, out_transform = mask(src, geoms, crop=True) ## å‡ºç°äº†ä¸é…å¥—çš„æƒ…å†µï¼Œæ€ä¹ˆåŠ out_meta = src.meta.copy() out_meta.update({\"driver\": \"GTiff\", \"height\": out_image.shape[1], \"width\": out_image.shape[2], \"transform\": out_transform})with rasterio.open(output_raster, \"w\", **out_meta) as dest: dest.write(out_image) 2. Resampling the dataThere are many different approaches to adjust the resolution of the raster file. Here, I use one simple method suppoerted by skikit-image package. With the resize function, you are simply create a new 2-d array with a predefined mesh grid. 12345678pathToRaster = r'./2+26city_gdp.tif'with rasterio.open(pathToRaster) as src: arr = src.read() arr[arr ==-9999] = np.nan arr = arr[0,:,:] # In this case, the orignial resolution was adjusted to 3 km x 3 km. new_shape = (round(arr.shape[0]/3.0),round(arr.shape[1]/3.0)) newarr= st.resize(array, new_shape, mode='constant') 3. Exporting DataWhen the resampling works are done, the generated numpy array can be exported to geotiff format file for further uses. Luckily, some sufficient information for geotiff file can be easily assesed from the original one. 12345import cartopy.crs as ccrsimport rasteriopathToRaster = r'./2+26city_gdp.tif'raster = gdal.Open(pathToRaster, gdal.GA_ReadOnly)gt = raster.GetGeoTransform() Since the mesh grid has been altered, the GeoTransform information should be editted. The values in gt includes: gt[0] = top left x gt1 = w-e pixel resolution gt2 = 0 gt[3] = top left y gt[4] = 0 gt[5] = n-s pixel resolution (negative value) In this case, gt1 and gt[5] should be revised (i.e., multiplied by 3) 12345678910output = r'./2+26city_3km.tif'cols, rows =newarr.shape[1],newarr.shape[0]driver = gdal.GetDriverByName(\"GTiff\")outDs = driver.Create(output, cols, rows, 1, gdal.GDT_Float32)r, float, etc).outDs.SetGeoTransform((gt[0],gt[1]*3.0,gt[2],gt[3],gt[4],gt[5]*3.0))outDs.SetProjection(raster.GetProjection())outDs.GetRasterBand(1).WriteArray(newarr[::-1])outDs.GetRasterBand(1).SetNoDataValue(-9999)del outDs Done! PS:å‚è€ƒèµ„æ–™ Export Numpy Arrays to Geotiff Format Using Rasterio and Python","link":"/2019/04/07/18.%5BCode%5DAltering-the-resolution-raster-file-using-Pyton/"},{"title":"Resampling the orignial data to 2-d array in specific grid system","text":"In this post, I address an common problem in geoscience research: how to arrange the original geodata into pre-defined grid system. Sometimes, we need to unify the resolution of various dataset or summary the scatter data to raster one. Specifically, this brief tutorial will look at two different original data, and allow you to creat gridded data in python. For better illustration, two practial cases with detailed code are shown: Creating an emission inventory based on the emissions from point sources (e.g., power plants, cement plants) Remapping a population density map to a coarser resolution 1. Scatter data point123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inlineimport cartopy.crs as ccrsfrom matplotlib.axes import Axesfrom cartopy.mpl.geoaxes import GeoAxesGeoAxes._pcolormesh_patched = Axes.pcolormeshfrom cartopy.mpl.gridliner import LATITUDE_FORMATTER, LONGITUDE_FORMATTERfrom scipy.spatial import KDTree# 1. generating the scatter dataxc1, xc2, yc1, yc2 = 100, 110, 35, 45N = 1000x = np.random.uniform(low=xc1, high=xc2, size=(N,))y = np.random.uniform(low=yc1, high=yc2, size=(N,))value = np.ravel(np.random.rand(1000,1))df_points = pd.DataFrame({\"x\":x, \"y\":y, \"v\":value})## 2. Clustering the scatter point Using KdTree methodXSIZE,YSIZE=50,50 # set the grid resolutionlon,lat = np.linspace(xc1,xc2,XSIZE),np.linspace(yc1,yc2,YSIZE)X, Y = np.mgrid[lon.min():lon.max():XSIZE*1j, lat.min():lat.max():YSIZE*1j]grid = np.c_[X.ravel(), Y.ravel()]points = np.c_[df_points.x, df_points.y]from scipy.spatial import KDTreetree = KDTree(grid)dist, indices = tree.query(points)grid_values = df_points.groupby(indices).v.sum()df_grid = pd.DataFrame(grid, columns=[\"x\", \"y\"])df_grid[\"v\"] = grid_values## 3. Visualizationdef plot_xy_tick(ax): xticks = [100,105,110] yticks = [35,40,45] ax.set_xticks(xticks, crs=ccrs.PlateCarree()) ax.set_yticks(yticks, crs=ccrs.PlateCarree()) lon_formatter = LongitudeFormatter(number_format='.1f', degree_symbol='', dateline_direction_label=True) lat_formatter = LatitudeFormatter(number_format='.1f', degree_symbol='') ax.xaxis.set_major_formatter(lon_formatter) ax.yaxis.set_major_formatter(lat_formatter)fig = plt.figure(figsize=(8,3))ax1 = plt.subplot(121, projection=ccrs.PlateCarree())ax1.scatter(df_points.x, df_points.y, alpha=0.2,c=df_points.v,s=15,transform=ccrs.PlateCarree())ax1.set_extent([xc1, xc2, yc1, yc2], crs=ccrs.PlateCarree())plot_xy_tick(ax1)ax2 = plt.subplot(122, projection=ccrs.PlateCarree())val_ = df_grid.v.values.reshape(len(lat),len(lon))ax2.pcolormesh(X,Y, val_,transform=ccrs.PlateCarree())ax1.set_extent([xc1, xc2, yc1, yc2], crs=ccrs.PlateCarree())plot_xy_tick(ax2) plt.show() &nbsp;2. Raster dataNoted in my last post, the resampling of tif-format dataset to raster file in other resolution relied on the skimage.transform function. Here, I present the method which directly deal with np.array. 2.1 equal coarse ratio on both axis12345## creat a original data representing populaiton mapxc1, xc2, yc1, yc2 = 100, 110, 35, 45XSIZE,YSIZE=100,100lon,lat = np.linspace(xc1,xc2,XSIZE),np.linspace(yc1,yc2,YSIZE)pop = np.random.uniform(low=1000, high=50000, size=(XSIZE*YSIZE,)).reshape(YSIZE,XSIZE) 123456789101112## Method 1 reshape the original data arrayshape = np.array(pop.shape, dtype=float)coarseness = 2 # the new shape is in 50 x 50new_shape = coarseness * np.ceil(shape/coarseness).astype(int)# # Create the zero-padded array and assign it with the old densityzp_pop = np.zeros(new_shape)zp_pop[:int(shape[0]), :int(shape[1])] = poptemp = zp_pop.reshape((new_shape[0] // coarseness, coarseness, new_shape[1] // coarseness, coarseness))coarse_pop = np.sum(temp, axis=(1,3))print (pop.sum())print (coarse_pop.sum()) 2.2 inequal coarse ratio on both axisMethod.1 interpolation 1234567891011from scipy import interpolatehfunc = interpolate.interp2d(lon,lat,pop)XSIZE_n,YSIZE_n=80,60coarse_pop = np.zeros(XSIZE_n * YSIZE_n)xx,yy=np.meshgrid(lon_n,lat_n)grid = np.c_[xx.ravel(),yy.ravel()]df_grid = pd.DataFrame(grid, columns=[\"x\", \"y\"])df_grid['v'] = [hfunc(x,y)[0] for x, y in zip(df_grid['x'].values,df_grid['y'].values)]print (pop.sum())print (df_grid['v'].sum()) Method.2 Fourier transform method Thanks to help from my new roomate who is expert in math, I learned to transfrom the 2-d array using Fourier transform method 12345from scipy import fftpackpop_fft = fftpack.fft2(pop*100*100/(60*80),shape = (60,80)) # 100 x 100 -&gt; 60 x 80pop_res = fftpack.ifft2(pop_fft).realprint(pop.sum())print(pop_res.sum())","link":"/2019/06/01/19.%5BCode%5DResampling-the-orignial-data-to-2-d-array/"},{"title":"WRF post processing 1: Merging datafiles for specific spots","text":"When youâ€™ve got the all the files from WRF simulations, you might want to merge them without the spin-up frames (to reach a balanced state with the boundary conditions, i.e., 12 hours for a 5-day simulation). Meanwhile, the variables/grids which are not focused on can be ignore. Therefore, a general workflow in pythonic way is presented. I will also rewritten this function as my first Python Package. Please note the updates on my website. 1. Pretreatment In this post, the technique applied here is to extracting five meteorological variables for several grids. Those variables contain wind speed (ws), wind direction (wd), relative humidity (RH), planetal boundary layer height (PBLH), Temperature at 2 m above ground (T2) and precipitation (PREP). Noticed that ws, wd and rh are not directly derived from standard output, some essential steps should be implemented. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465\"\"\"calulating wind speed by U and V components\"\"\"def calc_ws(fc,y,x): u = fc.variables[\"U10\"][:,y,x] v = fc.variables[\"V10\"][:,y,x] return (u**2+v**2)**0.5\"\"\"calulating wind directionn by U and V components\"\"\" def calc_wd(fc,y,x): # fc: input data, y/x: the index in y/x axis of targeted grid \"\"\" Reference: http://colaweb.gmu.edu/dev/clim301/lectures/wind/wind-uv \"\"\" u = fc.variables[\"U10\"][:,y,x] v = fc.variables[\"V10\"][:,y,x] wd = [] for i in range(0,len(u),1): if u[i]==0: if v[i]==0: wd.append(np.nan) if v[i]&gt;0: wd.append(180.0) if v[i]&lt;0: wd.append(0) if u[i]&gt;0: if v[i]&gt;=0: wd.append(270-np.arctan(v[i]/u[i])/2/np.pi*360.0) if v[i]&lt;0: wd.append(270+np.arctan(v[i]/u[i])/2/np.pi*360.0) if u[i]&lt;0: if v[i]&gt;=0: wd.append(90+np.arctan(v[i]/u[i])/2/np.pi*360.0) if v[i]&lt;0: wd.append(90-np.arctan(v[i]/u[i])/2/np.pi*360.0) return wd# calculate the relative humiditydef calc_rh(fc,y,x): \"\"\" referencing: t: perturbation potential temperature. \"T\" p: perturbation pressure. \"P\" pb: base state pressure. \"PB\" qv: Water vapor mixing ratio \"QVAPOR\" noted that all these parameters from WRF have the vertical dimension. \"\"\" t = fc.variables['T'][:,0,y,x] p0 = fc.variables['P'][:,0,y,x] pb = fc.variables['PB'][:,0,y,x] qv = fc.variables['QVAPOR'][:,0,y,x] theta = t+300 p = (p0+pb)/100.0 temp = theta*(p/1000.0)**0.2854 e_0 = 6.1173 ## mb t_0 = 273.16 ## K Rv = 461.50 ## J K-1 Kg-1 Lv_0 = 2.501 * 10**6 ## J Kg-1 K1 = Lv_0 / Rv ## K K2 = 1 / t_0 ## K-1 K3 = 1 / temp ## K-1 e_s = e_0 * np.exp(K1*(K2 - K3)) w_s = (0.622 * e_s)/(p - e_s) return (qv/w_s)*100.0 Those functions were wrote in an independent file (function_lib.py) for further importing. 2. Main functionNow that we have the pre-defined tools, we can proceed to read and merge all the outputs into one hdf5 file. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from netCDF4 import Datasetimport pandas as pdfrom pandas import HDFStore, DataFramefrom pandas import read_hdfimport os,sys,stringimport numpy as np# the Pretreatment functionsimport function_lib as func# targeted girds with specific namessite_= ['YZ','TY','HD','SS','XS','HS']loc_dic = {site_[0]:[37,65],site_[1]:[38,64], site_[2]:[37,66],site_[3]:[28,75], site_[4]:[34,63],site_[5]:[38,68]}# creat an empty h5 filehdf = HDFStore(\"simulated_meteo_chifeng.h5\")feature = ['WS','WD','PBLH','RH','T2','PREP']# loop all the original filesDir = \"/disk2/hyf/wrf/WRFV3/Chifeng_output/post-process/d03/\"files = os.listdir(Dir)files.sort()files = files[0:]# write the varibales into h5 filefor k in range(0,len(feature),1): data = {'date':[],site_[0]:[],site_[1]:[],site_[2]:[], site_[3]:[],site_[4]:[],site_[5]:[],} for file in files: filename,extname = os.path.splitext(file) if filename[7:10] =='d03': ## restrict to specific nested domain fc = Dataset(Dir+file) data['date'].extend([''.join(t) for t in fc.variables['Times']]) for i in range(0,len(site_),1): x,y = loc_dic[site_[i]][1],loc_dic[site_[i]][0] if feature[k]=='WD': data[site_[i]].extend(func.calc_wd(fc,y,x)) if feature[k]=='WS': data[site_[i]].extend(func.calc_ws(fc,y,x)) if feature[k]=='PBLH': data[site_[i]].extend(fc.variables['PBLH'][:,y,x]) if feature[k]=='RH': data[site_[i]].extend(func.calc_rh(fc,y,x)) if feature[k]=='T2': data[site_[i]].extend(fc.variables['T2'][:,y,x]) if feature[k]=='PREP': #RAINC is the rain calculation by cumulus scheme and RAINNC is the rain calculation by micrphysics scheme. rain = (fc.variables['RAINC'][:,y,x]+fc.variables['RAINNC'][:,y,x])*1000.0/997.0 data[site_[i]].extend(rain) data = pd.DataFrame(data) # a efficient way to deal with those spin-up frames. data = data.drop_duplicates(subset='date', keep='first', inplace=False) print len(data) data = data.reset_index() hdf.put(feature[k], data, format='table', encoding=\"utf-8\") And after those scripts completed successfully, we have merged all the targetd variables for the spots of interest. In our furture post, the technique dealing with a 2-d griided system (i.e., a city) will be presented.","link":"/2019/03/11/17.%5BModel%5DWRF-technique-merging-datasets/"},{"title":"QGIS: ä¾ç…§ç°æœ‰å›¾ç‰‡èµ„æ–™ç»˜åˆ¶çŸ¢é‡æ–‡ä»¶","text":"æœ‰æ—¶ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è·å–æŸäº›åœ°ç†æ•°æ®çš„ç²¾ç¡®ä½ç½®ä¿¡æ¯ï¼Œä»…èƒ½ä»ç½‘ç»œä¸­ä¸‹è½½ä»–äººçš„åœ°å›¾ä½œå“ã€‚å› è€Œï¼Œæˆ‘è¯·æ•™äº†ä¸€åè§„åˆ’å¸ˆåŒå­¦ï¼Œå‚è€ƒç½‘ç»œå›¾åƒï¼Œé€šè¿‡é€‚é…åœ°ç†èŒƒå›´ï¼Œæ‰‹å·¥è·å–å…¶å®é™…åœ°ç†å±æ€§ã€‚æœ¬æ–‡ä»¥æˆ‘å›½é›†ä¸­é‡‡æš–å—åŒ—åˆ†ç•Œçº¿çš„ç»˜åˆ¶ä¸ºä¾‹ï¼Œç®€è¿°QGISè½¯ä»¶çš„å…·ä½“å®ç°æ­¥éª¤ã€‚ ä¸Šä¸–çºª50å¹´ä»£ï¼Œç”±äºèƒ½æºç¼ºä¹ï¼Œæˆ‘å›½ä»¥å—åŒ—åœ°ç†ç•Œé™ï¼ˆç§¦å²­-æ·®æ²³çº¿ï¼‰ä¸ºåˆ’åˆ†ä¾æ®ï¼Œåœ¨è‹è”çš„å¸®åŠ©ä¸‹ï¼Œåœ¨è¯¥çº¿ä»¥åŒ—å»ºç«‹äº†é›†ä¸­ä¾›æš–ç³»ç»Ÿï¼Œæ²¿ç”¨è‡³ä»Šã€‚ä¸‹å›¾å¯è§ï¼Œæˆ‘å›½çš„é›†ä¸­ä¾›æš–ç³»ç»Ÿå¹¶ä¸ä»¥çœä¸ºç•Œé™ï¼Œå¦‚å››å·çœçš„ç”˜å­œå’Œé˜¿åä½äºä¾›æš–çº¿ä»¥åŒ—ï¼Œæ±Ÿè‹çœçš„å¾å·ä½äºä¾›æš–çº¿ä»¥åŒ—ï¼Œè€Œæ²³å—çœçš„ä¿¡é˜³åˆ™æ²¡æœ‰é›†ä¸­ä¾›æš–ã€‚ è¿™æ¡åˆ†ç•Œçº¿å…·æœ‰å¤šé‡åœ°ç†å­¦æ„ä¹‰ï¼Œå®ƒè¿˜æ˜¯ æˆ‘å›½800æ¯«ç±³ç­‰é™æ°´é‡çº¿ 1æœˆä»½0â„ƒç­‰æ¸©çº¿ æ¸©å¸¦å­£é£ä¸äºšçƒ­å¸¦å­£é£æ°”å€™åˆ†ç•Œçº¿ æ¸©å¸¦è½å¶é˜”å¶æ—ä¸äºšçƒ­å¸¦å¸¸ç»¿é˜”å¶æ—å¸¦åˆ†ç•Œçº¿ æ¹¿æ¶¦ä¸åŠæ¹¿æ¶¦åŒºåˆ†ç•Œçº¿(æ‘˜å½•è‡ª å›½å®¶æ°”è±¡å±€ æ°”è±¡ç§‘æ™®å›­)ã€‚ åŒæ—¶ï¼Œè¯¥åˆ†ç•Œçº¿åŒæ—¶å…·æœ‰ç‰¹æ®Šçš„å†å²æ„ä¹‰ï¼Œå…¶ä¸œæ®µæ˜¯æˆ‘å›½å—åŒ—æœã€é‡‘æœ/è’™å…ƒ-å—å®‹æ—¶æœŸçš„å—åŒ—æ”¿æƒè¾¹ç•Œã€‚ 1. å®‰è£…Freehand raster georeferencerQGISçš„Freehandç¬¬ä¸‰æ–¹å·¥å…·åŒ…å¯è½½å…¥å›¾ç‰‡å›¾å±‚å¹¶ç®€å•å¤„ç†ï¼Œä¸»è¦ç”¨é€”åº”æ˜¯å«æ˜Ÿé¥æ„Ÿå›¾åƒçš„åƒå…ƒåˆ†æã€‚è¯¥å·¥å…·åŒ…çš„åŠŸèƒ½ç•Œé¢éå¸¸ç®€æ´ï¼Œå·¥å…·æ  ä»å·¦è‡³å³ï¼ŒåŠŸèƒ½ä¾æ¬¡ä¸ºâ€œè½½å…¥â€ï¼Œâ€œç§»åŠ¨â€ï¼Œâ€œæ—‹è½¬â€ï¼Œâ€œæ”¾ç¼©â€ï¼Œâ€œå¤åˆæ“ä½œâ€ï¼Œâ€œé€æ˜åº¦å‡å°â€ï¼Œâ€œé€æ˜åº¦å¢åŠ â€ï¼Œâ€œè¾“å‡ºâ€ ã€‚ 2. ç»˜åˆ¶Polylineæˆ‘ä»¬åˆ†åˆ«å¯¼å…¥ä¸­å›½åˆ†çœåœ°å›¾shapefileæ–‡ä»¶å’Œjpegæ ¼å¼åº•å›¾ã€‚åˆ©ç”¨Freehandä¸æ–­è°ƒæ•´åˆå›¾åƒä½ç½®ã€å¤§å°ï¼Œä½¿äºŒè€…åŸºæœ¬é‡åˆï¼ˆtip: æŒ‰ä½â€œâŒ˜â€æˆ–â€ctrlâ€å¯æŒ‰åŸå§‹æ¯”ä¾‹ç¼©æ”¾ï¼‰ï¼Œå¦‚ä¸‹å›¾ã€‚å¯ä»¥çœ‹å‡ºå—æ–¹å„çœä»½å»åˆè¾ƒå¥½ï¼Œä½†å°æ¹¾ä¸æµ·å—æœ‰è¾ƒå¤§çš„åå·®ï¼Œå¯èƒ½æ˜¯åŸå§‹å›¾ç‰‡æŠ•å½±ç³»ç»Ÿä¸Šçš„å·®å¼‚ã€‚æ­¤å¤„ä½œä¸ºæ¡ˆä¾‹ï¼Œæœªæ·±ç©¶å…¶åŸå› ã€‚ é€‰æ‹©â€Layerâ€-&gt;â€Creat Layerâ€-&gt;â€New Shapefile Layerâ€æ–°å»ºpointæ ¼å¼çš„çŸ¢é‡æ–‡ä»¶ï¼Œåˆ©ç”¨ ä¾ç…§å›¾ç‰‡å‹¾å‹’ç•Œé™ï¼Œå¦‚ä¸‹å›¾ï¼š 3. åç»­åº”ç”¨åˆ©ç”¨Pythonè¯­è¨€è¯»å–ç¼–å†™å¥½çš„çŸ¢é‡æ–‡ä»¶ï¼Œé€šè¿‡Cartopy, Fiona, Shapelyç­‰å·¥å…·åŒ…å¯åšè¿›ä¸€æ­¥åˆ†æã€‚ ä¾‹å¦‚ï¼Œä»¥è¯¥çº¿ä¸ºä¾æ®å°†æˆ‘å›½ç©ºæ°”è´¨é‡ç›‘æµ‹ç«™ç‚¹è¿›è¡Œåˆ†ç±»ï¼Œæ¢ç©¶åˆ†é‡‡æš–/éé‡‡æš–åŸå¸‚é—´å¤§æ°”æ±¡æŸ“ç‰¹å¾çš„å·®å¼‚ã€‚è¯¥å·¥ä½œå¯ç”±ç®€å•çš„ä»£ç æŒ‡ä»¤å®Œæˆï¼Œå¦‚ä¸‹ï¼š 1234567891011121314151617181920import warningswarnings.filterwarnings('ignore')from shapely.geometry import shapefrom shapely.geometry import LineString# loading the boundary layerimport fionafname = './N-S_boundary.shp'line1 = fiona.open(fname)line1 = shape(line1.next()['geometry'])# set a end point which is the southernmost for all stations.end_point = (dy[dy['lat']==dy['lat'].min()]['lon'].values[0],dy[dy['lat']==dy['lat'].min()]['lat'].values[0])# loop all monitoring stations for classificationdy['NS']= np.nanfor i in range(0,len(dy),1): start_point = (dy['lon'].iloc[i],dy['lat'].iloc[i]) line2 = LineString([start_point, end_point]) if line1.intersection(line2).is_empty: dy[\"NS\"].iloc[i]='S' else: dy[\"NS\"].iloc[i]='N' æŒ‰ä¾›æš–åˆ†ç•Œçº¿åŒºåˆ†åï¼Œå¤§æ°”é‡‡æ ·ç«™ç‚¹çš„åˆ†å¸ƒå¦‚å›¾æ‰€ç¤º","link":"/2019/02/28/16.%5BSoftware%5DQGIS%E6%93%8D%E4%BD%9C-%E4%BE%9D%E6%8D%AE%E5%9B%BE%E5%83%8F%E7%BB%98%E5%88%B6%E7%9F%A2%E9%87%8F%E6%96%87%E4%BB%B6/"},{"title":"ç¿»è¯‘è®¡åˆ’: å¤§æ°”åŒ–å­¦ç”µå­æ•™æ-ç¬¬ä¸€ç« PartA","text":"å‰æ²¿å¤§æ°”åŒ–å­¦æ˜¯ä¸€é—¨å…³äºå¤§æ°”ä¸­åŒ–å­¦ç‰©è´¨çš„æ’æ”¾ã€ç›¸äº’ä½œç”¨ã€ç‰©ç†åŠåŒ–å­¦è½¬åŒ–çš„ç»¼åˆæ€§å­¦ç§‘ã€‚åŒæ—¶ï¼Œå…¶å¯¹å¤§æ°”å…³é”®çš„ç‰©ç†è¿‡ç¨‹äº¦ä¼šæ¶‰åŠã€‚æœ¬è®¡åˆ’æ‰€ç¿»è¯‘çš„ç”µå­è¯¾æœ¬å¯¹å¤§æ°”åŒ–å­¦é¢†åŸŸè¿‘å¹´æ¥çš„ç ”ç©¶è¿›å±•(è¯¥è¯¾æœ¬ç¼–æ’°äº2013å¹´)è¿›è¡Œé™ˆè¿°ï¼Œå¦‚æ•°å­¦æ¨¡å‹çš„å¹¶è¡Œè®¡ç®—ç ”ç©¶ï¼Œç©ºæ°”æ±¡æŸ“ç‰©å¯¹å…¨çƒæ°”å€™å˜åŒ–çš„å½±å“ç­‰ã€‚åŸè‘—ä½œè€…æœŸæœ›æœ¬ä¹¦çš„å†…å®¹å°†ä¸ºæœ¬ç§‘ç”Ÿä¸ç¡•å£«ç”Ÿæä¾›å¸®åŠ©ï¼Œä»¥ä¹‹ä¸ºä¸“ä¸šè¯¾ç¨‹çš„åŸºç¡€æ€§å‚è€ƒèµ„æ–™ã€‚ ç¬¬ä¸€ç«  å¤§æ°”å±‚çš„ç»“æ„å’Œç»„æˆå¤§æ°”å±‚çš„æ€»è´¨é‡æ¥è¿‘5.3x1018kgï¼Œè€Œåœ°çƒæ°´åœˆ(åŒ…æ‹¬æµ·æ´‹ã€æ¹–æ³Šã€æ²³æµã€åœ°ä¸‹æ°´ä»¥åŠå†°é›ª)çš„æ€»è´¨é‡ä¸º1.4x1021kgï¼Œå²©çŸ³åœˆæ€»è´¨é‡åˆ™ä¸º5.98x1024kgã€‚å¤§æ°”å±‚çš„ä¸Šè¾¹ç•Œæ²¡æœ‰ä¸¥æ ¼çš„å®šä¹‰ã€‚å¤§æ°”å±‚ä¸­çš„ç‰©è´¨éšé«˜åº¦çš„ä¸Šå‡è€Œæ€¥å‰§å‡å°‘ï¼Œå¹¶æœ€ç»ˆèå…¥æ˜Ÿé™…ç©ºé—´ä¹‹ä¸­ã€‚åŒ…å«ç€å¤§é‡æ°”ä½“åŠé¢—ç²’ç»„åˆ†çš„å¤§æ°”åœˆå±‚å›´ç»•åœ¨åœ°çƒçš„å¤–éƒ¨ï¼Œéšåœ°çƒä¸€åŒç»•æ—¥æ—‹è½¬ã€‚ åœ°çƒå¤§æ°”å±‚çš„å¯†åº¦åˆ†å¸ƒæä¸å‡åŒ€ã€‚è·ç¦»åœ°é¢5.5 Kmä»¥ä¸‹çš„ç©ºæ°”å æ®ç€å¤§æ°”å±‚ä¸€åŠçš„è´¨é‡ï¼Œè€Œå¤§æ°”å±‚çº¦99%çš„è´¨é‡åˆ†å¸ƒåœ¨è·åœ°30 Kmä»¥å†…ã€‚åœ°çƒå¤§æ°”å±‚çš„èŒƒå›´å’Œåœ°çƒç£æ°”åœˆçš„èŒƒå›´ä¸€è‡´(å›¾1.1)ï¼Œè¿™ä¸€å½¢æ€ç”±åœ°çƒæœ¬ä½“ç£åœºä¸å¤ªé˜³é£[1]çš„ç›¸äº’ä½œç”¨è€Œæœ€ç»ˆå½¢æˆã€‚ å›¾1.1 åœ°çƒå¤–å±‚çš„ç”µç¦»å±‚ç”±åœ°çƒæœ¬ä½“ç£åœºä¸å¤ªé˜³é£ç›¸äº’ä½œç”¨è€Œå½¢æˆ åœ°çƒå¤§æ°”å±‚çš„å½¢æˆ45äº¿å¹´å‰çš„æ—©æœŸåœ°çƒï¼Œæ˜¯ä¸€ä¸ªç‚™çƒ­çš„å²©çŸ³æ˜Ÿä½“ã€‚æœ€åˆçš„å¤§æ°”ç»„åˆ†å¯èƒ½åŒ…æ‹¬æ°¢æ°”ï¼Œæ°¦æ°”å’Œä¸€äº›å«æœ‰æ°¢å…ƒç´ çš„ç®€å•æ°”ä½“åˆ†å­ï¼ŒåŒ…æ‹¬æ°¨æ°”(NH3)å’Œç”²çƒ·(CH4)ã€‚è¿™ä¸€æ—¶æœŸåœ°çƒçš„å¤§æ°”åœˆå±‚ä¸ä»Šå¤©çš„æœ¨æ˜ŸåŠåœŸæ˜Ÿååˆ†ç±»ä¼¼ã€‚ç”±äºåœ°ç£åœºåœ¨å½“æ—¶å¹¶æœªå½¢æˆï¼Œæ¥è‡ªå¤ªé˜³è¡¨å±‚çš„å¤ªé˜³é£ä¸æ–­åœ°ä¾µæ‰°ç€åœ°çƒå¤§æ°”ï¼Œä½¿å¾—æ—©æœŸåœ°çƒå¤§æ°”ä¸æ–­ä¸§å¤±ã€‚åŒæ—¶åœ¨4äº¿å¹´å‰å·¦å³ï¼Œéšç€åœ°çƒä¸æ–­å†·å´ï¼Œä¼´æœ‰å¼ºçƒˆç«å±±è¿åŠ¨çš„å›ºä½“å¤–å£³å¼€å§‹æˆå‹ã€‚ç«å±±çš„ä¸æ–­å–·å‘ï¼Œä½¿å¾—å¤§é‡åœ°çƒå†…éƒ¨çš„æ°”ä½“è¿›å…¥è¡¨å±‚ã€‚åŒ…æ‹¬æ°´è’¸æ°”ï¼ŒäºŒæ°§åŒ–ç¢³ä»¥åŠæ°¨æ°”ç­‰æ°”ä½“æ„æˆäº†åœ°çƒç¬¬äºŒé˜¶æ®µçš„å¤§æ°”åœˆ(æ¬¡ç”Ÿå¤§æ°”é˜¶æ®µ)ã€‚ç«æ˜Ÿå’Œé‡‘æ˜Ÿç°åœ¨çš„å¤§æ°”å±‚ä¸å½“æ—¶åœ°çƒçš„æƒ…å½¢ç±»ä¼¼ï¼Œä¸»è¦ç”±äºŒæ°§åŒ–ç¢³ç»„æˆã€‚ åŸå§‹åœ°çƒå¤§æ°”åœ¨å†ç»æ•°ç™¾ä¸‡å¹´çš„ä¸æ–­å†·å´ä¸­ï¼Œæ°´è’¸æ°”å¼€å§‹å‡ç»“æˆäº‘ã€‚äº‘æ»´ä½œç”¨å½¢æˆé™é›¨ï¼Œæµ·æ´‹ç”±æ­¤äº§ç”Ÿã€‚è€Œå¤§æ°”ä¸­çš„äºŒæ°§åŒ–ç¢³å¼€å§‹è¢«æµ·æ´‹é€æ¸å¸æ”¶ï¼Œæ°¨æ°”åˆ™åœ¨å¤ªé˜³ç…§å°„ä¸‹è¢«æ°§åŒ–ï¼Œå½¢æˆåŒ–å­¦æ€§è´¨ç¨³å®šçš„æ°®æ°”(å›¾1.2)ã€‚ å›¾1.2 åœ°çƒå¤§æ°”å„å†å²æ—¶æœŸçš„ç‰©è´¨ç»„æˆ ä¸€äº›ç†è®ºè®¤ä¸ºï¼Œåœ°çƒä¸Šæœ€æ—©çš„æ°§æ°”ç”±å¤§æ°”ä¸­çš„æ°´è’¸æ°”å…‰è§£äº§ç”Ÿ: $$\\rm 2H_2 + ç´«å¤–çº¿(UV) \\rightarrow 2H_2 + O_2 (R1.1)$$ ç„¶è€Œï¼Œç”±ä¸Šè¿°è¿‡ç¨‹äº§ç”Ÿçš„æ°§æ°”é‡æå°ã€‚æ°§æ°”æ›´ä¸»è¦çš„æ¥æºæ˜¯è“è—»ç­‰æœ‰æœºä½“çš„å…‰åˆä½œç”¨[2]ï¼Œè¿™ä¸€è¿‡ç¨‹å¼€å§‹äº25äº¿å¹´å·¦å³ã€‚é€šè¿‡å…‰åˆä½œç”¨ï¼Œæœ‰æœºä½“åˆ©ç”¨äºŒæ°§åŒ–ç¢³ï¼Œæ°´åŠå¤ªé˜³è¾å°„ç”Ÿæˆæ°§æ°”ä¸ç¢³æ°¢åŒ–åˆç‰©ï¼Œå…¶åŒ–å­¦å¼å¦‚ä¸‹: $$\\rm CO_2 + H_2O + sunlight \\rightarrow æœ‰æœºç‰© + O_2 (R1.2)$$ èµ·åˆï¼Œå¤§æ°”ä¸­çš„å°‘é‡æ°§æ°”é€šæ°§åŒ–åœ°çƒè¡¨å±‚çš„å²©çŸ³è€Œè¢«æ¶ˆè€—(å³å²©çŸ³çš„é£åŒ–è¿‡ç¨‹)ã€‚å½“è¡¨å±‚å²©çŸ³è¢«å®Œå…¨æ°§åŒ–åï¼Œå¤§æ°”æ°§å«é‡çš„å¢é•¿æ›´ä¸ºè¿…é€Ÿã€‚å½“æ°§å«é‡å¢åŠ åˆ°å¦‚ä»Šæ°´å¹³(21%)çš„1 - 2%æ—¶ï¼Œå…¶ç”Ÿæˆè‡­æ°§çš„è¿‡ç¨‹(O3)èƒ½å¤Ÿä¿æŠ¤åœ°çƒè¡¨é¢å…å—å¼ºçƒˆçš„ç´«å¤–è¾å°„: $$\\rm 2O_2 + ç´«å¤–çº¿(UV) \\rightarrow O_3 + O (R1.3)$$ ä»¥åŠ: $$\\rm O + O_2 \\rightarrow O_3 (R1.4)$$ åœ¨è¯¥ç¯å¢ƒä¸‹ï¼ŒåŸå§‹æ¤ç‰©å¾—ä»¥è¿›åŒ–äº§ç”Ÿã€‚7äº¿å¹´å‰å‰ç”±æµ·æ´‹ç™»é™†çš„åœ°çƒæ¤ç‰©å¼€å§‹å¤§é‡å¸æ”¶å¤§æ°”ä¸­çš„äºŒæ°§åŒ–ç¢³(CO2ä»1 -5%é™è‡³0.04%)ï¼Œå¹¶ä½¿å¾—å¤§æ°”ä¸­æ°§æ°”å«é‡å¢åŠ ã€‚å¤§æ°”æ°§å«é‡åœ¨è¿‡å»çš„å‡ ç™¾ä¸‡å¹´ä¸­çš„ä¸åŒæ—¶æœŸï¼Œå—æ°”å€™å˜åŒ–ï¼Œç«å±±è¿åŠ¨åŠæ¿å—è¿åŠ¨çš„å½±å“è€Œä¸æ–­æ³¢åŠ¨(Holland, 2006)ã€‚æœ€ç»ˆï¼Œå¤§æ°”æ°§å«é‡ç¨³å®šåœ¨21%å·¦å³(å›¾1.3)ã€‚ å›¾1.3 æ°§æ°”å¯èƒ½çš„å†å²æµ“åº¦åˆ†å¸ƒ å¤§æ°”åŒ–å­¦ç®€å²å¤å¸Œè…Šçš„å“²å­¦å®¶é˜¿é‚£å…‹è¥¿ç±³å°¼(585â€“528 BC)è®¤ä¸ºæ°”ä½“æ˜¯ä¸‡ç‰©ä¹‹æºã€‚åœ¨å…¶ä¹‹åï¼Œæ©åŸ¹å¤šå…‹å‹’(çº¦490-430 BC)å°†æ°”ä½“è§†ä¸ºå››ç§åŸºæœ¬å…ƒç´ ä¹‹ä¸€ã€‚ç›´è‡³18ä¸–çºªï¼Œè¿™ä¸€è§‚ç‚¹å§‹ç»ˆè¢«æ¥å—ã€‚æ­¤æ—¶ï¼Œã€‚æœ€åˆçš„ç ”ç©¶åœ¨1700å¹´å·¦å³å‘è¡¨ã€‚ä¹‹åï¼Œå¤§æ°”ä¸­çš„å„ä¸ªç»„åˆ†è¢«é€ä¸€å‘ç°ï¼Œè¿™è¯å®äº†ç©ºæ°”æ˜¯ç”±ä¸åŒç±»å‹çš„æ°”ä½“ç»„æˆçš„è§‚ç‚¹ã€‚ è¡¨1.1 å¤§æ°”ç»„åˆ†çš„é‡è¦å‘ç° æ—¶é—´ ç»„åˆ† å‘ç°è€… 1750s CO2 çº¦ç‘Ÿå¤«Â·å¸ƒæ‹‰å…‹(è‹±) 1766 H2 äº¨åˆ©Â·å¡æ–‡è¿ªè®¸(è‹±) 1772 N2 ä¸¹å°¼å°”Â·å¢ç‘Ÿç¦(è‹±) 1774; 1772(1777å¹´å‘è¡¨) O2 çº¦ç‘Ÿå¤«Â·æ™®åˆ©æ–¯ç‰¹é‡Œ; å¡å°”Â·å¨å»‰Â·èˆå‹’(ç‘å…¸) 1840, O3 å¼—é‡Œå¾·é‡Œå¸ŒÂ·å°šç­(ç‘å£«) 1894 Ar çº¦ç¿°Â·æ–¯ç‰¹æ‹‰ç‰¹(è‹±)å’Œå¨å»‰Â·æ‹‰å§†é½(è‹±) è¡¨1.2 20ä¸–çºªå¤§æ°”åŒ–å­¦çš„é‡è¦é‡Œç¨‹ç¢‘ æ—¶é—´ ç§‘å­¦å®¶ å‘ç° 1924 æˆˆç™»Â·å¤šå¸ƒæ£®(è‹±) å‘æ˜äº†åˆ†å…‰å…‰åº¦è®¡ï¼Œå¹¶è¿›è¡Œè‡­æ°§æŸ±æµ“åº¦çš„å¸¸è§„æµ‹é‡ 1930 è¥¿å¾·å°¼Â·æ°æ™®æ›¼(è‹±) å‘ç°äº†è‡­æ°§å±‚çš„ç”ŸæˆåŸç† 1960 å“ˆæ ¹Â·æ–½å¯†(ç¾) è§£é‡Šäº†å…‰åŒ–å­¦çƒŸé›¾çš„æˆå›  1973 è©¹å§†æ–¯Â·æ‹‰å¤«æ´›å…‹(è‹±) é¦–æ¬¡åœ¨å¤§æ°”ä¸­æ£€æµ‹åˆ°æ°Ÿæ°¯çƒƒ(CFCs) 1995 ä¿ç½—Â·å…‹é²å²‘(è·), é©¬é‡Œå¥¥Â·è«åˆ©çº³(å¢¨), å¼—å…°å…‹Â·èˆä¼å¾·Â·ç½—å…°(ç¾) å› â€œä»–ä»¬å¯¹å¤§æ°”åŒ–å­¦çš„ç ”ç©¶å·¥ä½œï¼Œç‰¹åˆ«æ˜¯è‡­æ°§çš„å½¢æˆä¸åˆ†è§£ç ”ç©¶â€ï¼Œè·å¾—1995å¹´è¯ºè´å°”åŒ–å­¦å¥– å¤§æ°”çš„ç»„æˆåœ°çƒå¤§æ°”åŒ…å«å¤šç§æ°”ä½“ä¸æ°”æº¶èƒ¶é¢—ç²’ã€‚æˆ‘ä»¬å¯ä»¥ä¾æ®ä¸åŒæ°”ä½“åœ¨å¤§æ°”ä¸­çš„æ¯”é‡ä¸åœç•™æ—¶é—´å°†å…¶åˆ†ç±»ã€‚åœç•™æ—¶é—´(äº¦è¢«ç§°ä¸ºæ¶ˆé™¤æ—¶é—´ï¼Œå¤§æ°”å¯¿å‘½)æ˜¯æŒ‡ä¸€ç§é¢—ç²’ç‰©æˆ–æ°”ä½“åœ¨ç‰¹å®šç¯å¢ƒä¸‹å­˜åœ¨æ—¶é—´çš„å¹³å‡å€¼ã€‚è¯¥æ•°å€¼å¯ä»¥ç”¨å¤§æ°”ä¸­è¿™ç±»ç‰©è´¨çš„æ€»é‡ä¸å»é™¤é€Ÿç‡çš„æ¯”å€¼æ¥è¡¨ç¤ºã€‚æ ¹æ®åœç•™æ—¶é—´æ•°å€¼çš„ä¸åŒï¼Œå¤§æ°”ä¸­çš„æ°”ä½“å¯ä»¥è¢«å½’ç±»ä¸ºç¨³å®šæ°”ä½“ä¸æ´»æ€§æ°”ä½“(åŒ…æ‹¬é«˜æ´»æ€§æ°”ä½“)ï¼Œå…·ä½“å¦‚è¡¨1.3 è¡¨1.3 å¤§æ°”ä¸­çš„æ°”ä½“åˆ†ç±» åœç•™æ—¶é—´ ä¸»è¦ç»„åˆ† å¾®é‡æ°”ä½“ ç¨³å®šæ°”ä½“ æ°®æ°”ã€æ°§æ°”ã€æ°©æ°”(å¤§æ°”çš„ä¸»è¦ç»„åˆ†) å…¶å®ƒæƒ°æ€§æ°”ä½“ æ´»æ€§æ°”ä½“ äºŒæ°§åŒ–ç¢³ å…¶å®ƒé•¿å¯¿å‘½æ°”ä½“ é«˜æ´»æ€§æ°”ä½“ æ°´è’¸æ°” å…¶å®ƒçŸ­å¯¿å‘½æ°”ä½“ è¯‘è€…è¡¥å›¾1.1 å¤§æ°”å„ç±»æ°”ä½“çš„æ—¶ç©ºå°ºåº¦(æ¥æº: Seinfeld and Pandis, 2016) æˆ‘ä»¬å¯ä»¥é‡‡ç”¨å¤šç§æ•°å€¼å•ä½å¯¹å„ç±»æ°”ä½“çš„é‡çº§è¿›è¡Œæè¿°ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè´¨é‡æµ“åº¦(kg m-3)ï¼Œä½“ç§¯æ¯”(æ¯m3ç©ºæ°”æ‰€å«æŸç§æ°”ä½“çš„ä½“ç§¯æ•°m3)ä»¥åŠæ‘©å°”åˆ†æ•°(mol mol-1)è¾ƒä¸ºå¸¸ç”¨ã€‚å¯¹äºç—•é‡æ°”ä½“(é™¤å»æ°®ã€æ°§ã€æ°©ã€äºŒæ°§åŒ–ç¢³)è€Œè¨€ï¼Œé€šå¸¸ç”¨ppmvæˆ–ppmæ¥è¡¨ç¤ºæ¯ã€‚1 ppmv = 10â€“6 mol molâ€“1, 1 ppbv = 10â€“9 mol molâ€“1 and 1 pptv = 10â€“12 mol molâ€“1 è¡¨1.4 åœ°çƒå¤§æ°”ç»„æˆ æ³¨:æ°´è’¸æ°”å¹¶æœªçº³å…¥è¯¥è¡¨æ ¼å†… é™„æ³¨:[1]å¤ªé˜³é£: æŒ‡ä»å¤ªé˜³ä¸Šå±‚å¤§æ°”å–·æ¶Œè€Œå‡ºçš„é«˜é€Ÿç²’å­æµ(åŒ…æ‹¬è´¨å­å’Œç”µå­) [2]å…‰åˆä½œç”¨: æŒ‡ç»¿è‰²æ¤ç‰©æˆ–å…¶ä»–ç”Ÿç‰©å°†æ¥è‡ªå¤ªé˜³çš„å…‰èƒ½è½¬åŒ–ä¸ºåŒ–å­¦èƒ½çš„è¿‡ç¨‹ï¼Œéœ€è¦äºŒæ°§åŒ–ç¢³å’Œæ°´çš„å‚ä¸ã€‚å¯¹æ¤ç‰©è€Œè¨€ï¼Œè¯¥è¿›ç¨‹å‘ç”Ÿäºå…¶å¶ç‰‡å†…éƒ¨ã€‚ è¯‘è€…æ¨èçš„è¡¥å……ææ–™:åœ°çƒå¤§æ°£çµ„æˆçš„æ¼”åŒ–The Origin of Life on Planet Wateræ°´å¾ªç¯","link":"/2015/09/08/2.%5BKnowledge%5D%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92-%E5%A4%A7%E6%B0%94%E5%8C%96%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%95%99%E6%9D%90-1/"},{"title":"å°å·¥å…·å¼€å‘ï¼šç½‘ç»œé¢„è§ˆæ–‡çŒ®èµ„æ–™çš„ä¸‹è½½åˆæˆ","text":"éƒ¨åˆ†äº’è”ç½‘èµ„æ–™ä»…æä¾›åœ¨çº¿é¢„è§ˆï¼Œæ— æ³•ç›´æ¥ä¸‹è½½ã€‚è‹¥é¢„è§ˆç³»ç»Ÿè®¾è®¡æ¬ ä½³ï¼Œæˆ–é™äºç½‘é€Ÿå†…å®¹åŠ è½½è¿‡æ…¢ï¼Œéƒ½ä¼šå¯¹é˜…è¯»ä½“éªŒé€ æˆä¸¥é‡å½±å“ã€‚åœ¨æ­¤åˆ†åˆ«ä»¥å›½å®¶è‡ªç„¶åŸºé‡‘ç ”ç©¶æŠ¥å‘Šå’Œå­¦ä½è®ºæ–‡ä¸ºä¾‹ï¼Œä»‹ç»ä¸¤ç§ä¸åŒçš„ä¸‹è½½åˆæˆæ–¹æ³•ï¼Œä¾›æœ‰éœ€è¦çš„è€å¸ˆåŒå­¦ä»¬å‚è€ƒã€‚æ¬¢è¿å¤§å®¶æå‡ºå®è´µæ„è§ã€‚ 1. å›½å®¶è‡ªç„¶åŸºé‡‘ç ”ç©¶æŠ¥å‘Šå›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘å…±äº«æœåŠ¡ç½‘ä¸­å…¬å¼€çš„é¡¹ç›®æŠ¥å‘Šåªèƒ½åœ¨çº¿é˜…è§ˆã€‚ä»¥ã€Šé¢—ç²’ç‰©è´¨åŸºæœ¬æ€§è´¨çš„ç ”ç©¶ã€‹ç ”ç©¶ä¸ºä¾‹ï¼Œåœ¨å…¶ç»“é¢˜æŠ¥å‘Šå…¨æ–‡é¡µï¼Œè·å–é¦–é¡µå›¾ç‰‡ç½‘å€-http://output.nsfc.gov.cn/report/10/10274071_1.pngç¡®å®šç›®æ ‡é¡µURLåï¼Œæ¥ä¸‹æ¥çš„å·¥ä½œå¯ç”¨Pythonä»£ç è‡ªåŠ¨å®Œæˆï¼Œä»£ç å…·ä½“å¦‚ä¸‹ï¼š 1234567891011121314151617import osimport natsortfrom fpdf import FPDFfor i in range(0,200,1): os.system(\"wget http://output.nsfc.gov.cn/report/10/10274071_%i.png\" %i)files = os.listdir('./')files = natsort.natsorted(files, reverse = False)files = files[0:]pdf = FPDF()for image in files[0:]: if image[-3:]=='png': pdf.add_page() pdf.image(image,0,0,210,297) pdf.output(\"../é¢—ç²’ç‰©è´¨åŸºæœ¬æ€§è´¨çš„ç ”ç©¶.pdf\", \"F\") è¯¥æ–¹æ³•ç¼ºé™·æœ‰äºŒï¼š æŠ¥å‘Šæ€»é¡µæ•°éœ€è¦äººå·¥æŸ¥è¯¢ï¼Œä¸‹è½½æ—¶åº”å°½é‡è®¾ç½®è¾ƒå¤§çš„å¾ªç¯æ¬¡æ•°(å¦‚200æ¬¡)ï¼› æŠ¥å‘Šéƒ¨åˆ†æ¨ªç½®é¡µé¢ï¼Œåœ¨åˆæˆå‰æœªåšæ—‹è½¬è°ƒæ•´ã€‚ 2. ç ”ç©¶ç”Ÿå­¦ä½è®ºæ–‡ä¸€äº›å¤§å­¦çš„å­¦ä½è®ºæ–‡ä¸èƒ½åœ¨ä¸­å›½çŸ¥ç½‘ç­‰æ•°æ®åº“ç›´æ¥ä¸‹è½½ï¼Œæœ¬æ ¡åœ¨çº¿é¢„è§ˆæ–¹å¼ä¹Ÿä¸å¤ªæ–¹ä¾¿ã€‚è€ƒè™‘åˆ°åŸå§‹ç½‘é¡µé‡‡ç”¨javascriptåŠ¨æ€åŠ è½½è®ºæ–‡å†…å®¹(å®ä¸ºjpgæ ¼å¼å›¾ç‰‡)ï¼Œæ— æ³•é€šè¿‡wgetæˆ–urllibç­‰ç®€å•å·¥å…·ç›´æ¥è·å–ã€‚å› æ­¤ï¼Œé€‰ç”¨seleniumå·¥å…·æ¨¡æ‹Ÿchromeæµè§ˆå™¨é¼ æ ‡æ“ä½œï¼Œé€é¡µä¸‹è½½ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# ä¸»è¦å‚è€ƒç½‘é¡µ https://www.devdungeon.com/content/grab-image-clipboard-python-pillowfrom selenium import webdriverfrom time import sleepimport pyperclip,pyautoguiimport numpy as np# ä¸‹è½½ç¯èŠ‚chrome_options = webdriver.ChromeOptions()prefs = {'download.default_directory' : '/user/defined/path'}chrome_options.add_experimental_option('prefs', prefs)driver = webdriver.Chrome(executable_path=\"./geckodriver/chromedriver\", chrome_options=chrome_options)## æ­¤urlå…·ä½“ä½ç½®å¦‚ä¸‹å›¾driver.get(\"https://thesis.lib.pku.edu.cn/onlinePDF?dbid=72&amp;objid=53_57_54_50_56_49&amp;flag=online\")for t in np.arange(1,3,1): img_url = driver.find_element_by_id('ViewContainer_BG_0').get_attribute('src')[:-10] driver.get(img_url+\"_%s.jpg\" %\"{:05d}\".format(t)) # Move to the specified location, right click pyautogui.rightClick(x=600, y=500) # V pyautogui.typewrite(['V']) pyautogui.hotkey('ctrlleft','V') sleep(0.8) pyautogui.press('enter') sleep(0.8) pyautogui.press('enter')driver.close()# å›¾ç‰‡è°ƒæ•´ç¯èŠ‚## æœ‰äº›å›¾ç‰‡æ˜¯æ¨ªç½®çš„ï¼Œé€†æ—¶é’ˆæ—‹è½¬90åº¦import osimport natsortfrom fpdf import FPDFimport cv2files = os.listdir('/DOWNLOAD_PATH/')files = natsort.natsorted(files, reverse = False)files = files[0:]for file in files: if file[-3:]=='jpg': img = cv2.imread('/DOWNLOAD_PATH/'+file) h,w,c = img.shape if h&lt;w: imgrot = cv2.rotate(img,cv2.ROTATE_90_COUNTERCLOCKWISE) cv2.imwrite('/DOWNLOAD_PATH/'+file,imgrot) # åˆæˆPDF pdf = FPDF()for file in files[0:]: if file[-3:]=='jpg': pdf.add_page() pdf.image('/DOWNLOAD_PATH/'+file,0,0,210,297) pdf.output(\"FILENAME.pdf\", \"F\") åœ¨åŒ—å¤§å›¾ä¹¦é¦†æ£€ç´¢åï¼Œç‚¹å‡»ç›®æ ‡æ–‡çŒ®ï¼Œåœ¨è¯¥é¡µç æ‰“å¼€chromeå¼€å‘è€…å·¥å…·ï¼Œå¯»æ‰¾åˆ°ä¸‹åˆ—æ ‡è®°å­—æ®µï¼Œå³é”®é€‰æ‹©â€œcopy link elementâ€å³ä¸ºé¢„è§ˆé¡µurlã€‚","link":"/2020/05/10/20.%5BCode%5D%E5%B0%8F%E5%B7%A5%E5%85%B7%E5%BC%80%E5%8F%91%EF%BC%9A%E7%BD%91%E7%BB%9C%E9%A2%84%E8%A7%88%E6%96%87%E7%8C%AE%E8%B5%84%E6%96%99%E7%9A%84%E4%B8%8B%E8%BD%BD%E5%90%88%E6%88%90/"},{"title":"Plotting WRF Nested Domains WRFåµŒå¥—ç½‘æ ¼çš„ç»˜åˆ¶","text":"æè¦ï¼šå®šä¹‰åµŒå¥—ç½‘æ ¼æ˜¯è¿è¡ŒWRFæ¨¡å‹çš„é‡è¦å·¥ä½œã€‚æœ¬æ–‡åŸºäºPython 3.7ï¼Œåœ¨Salemå·¥å…·åŒ…åŸºç¡€ä¸ŠåŠ ä»¥æ”¹è¿›ï¼Œç»“åˆCartopyå¯¹ä¸­å›½å›½ç•Œçº¿è¿›è¡Œæ›¿æ¢ï¼ŒåŒæ—¶å®ç°ä¿®æ”¹é»˜è®¤åº•å›¾ã€‚ 1. æ›¿æ¢åº•å›¾Salemæ˜¯ç”¨äºåœ°ç†æ•°æ®å¤„ç†å’Œå¯è§†åŒ–çš„Pythonå°å·¥å…·ï¼Œå…¶åŠŸèƒ½å¼ºå¤§ï¼Œä½¿ç”¨ç®€ä¾¿ã€‚ä¾‹å¦‚ï¼ŒSalemå¯ç›´æ¥è¯»å–WRFæ¨¡å‹çš„namelist.inputï¼Œç»˜åˆ¶ç½‘æ ¼åµŒå¥—å›¾ã€‚ 1234567import salemfrom salem.utils import get_demo_filefrom salem import geogrid_simulatorfpath = r'./namelist.wps'g, maps = geogrid_simulator(fpath)maps[0].set_rgb(natural_earth='lr') # add a background imagemaps[0].visualize(title='WRF Simlated Domains 1 to 3') Cartopyåº•å›¾å­˜æ”¾ä½ç½®/User/HYF/anaconda3/lib/â¨python3.7/â¨site-packagesâ©/cartopy/â¨dataâ©/rasterâ©é«˜æ¸…å›¾åƒä¸‹è½½åœ°å€ natural Earth I Natural Earth III Natural Earth III with no cloudsâ©ä¸‹è½½æ–°imageåï¼Œéœ€ä¿®æ”¹ç›¸åŒæ–‡ä»¶å¤¹ä¸‹çš„image.jsonï¼Œä¸¾ä¾‹å¦‚ä¸‹: 1234567891011121314{\"__comment__\": \"JSON file specifying the image to use for a given type/name and resolution. Read in by cartopy.mpl.geoaxes.read_user_background_images.\", \"ne_shaded\": { \"__comment__\": \"Natural Earth shaded relief\", \"__source__\": \"http://www.naturalearthdata.com/downloads/50m-raster-data/50m-natural-earth-1/\", \"__projection__\": \"PlateCarree\", \"low\": \"50-natural-earth-1-downsampled.png\", \"high\": \"natural-earth-1_large8192px.png\", \"medium\": \"natural-earth-1_large4096px.png\"}, \"ne_shaded\": { \"__comment__\": \"Natural Earth shaded relief III\", \"__source__\": \"http://www.shadedrelief.com/natural3/pages/textures.htmlâ©\", \"__projection__\": \"PlateCarree\", \"high\": \"natural-earth-3_large4096px\"}} 2. ä¸­å›½è¡Œæ”¿è¾¹ç•Œé‡æ–°ç»˜åˆ¶åŸå§‹Salemå·¥å…·åŒ…çš„ä¸­å›½è¡Œæ”¿åœ°å›¾å­˜åœ¨é”™è¯¯ã€‚å› æ­¤ï¼Œè‡ªè¡Œç»˜åˆ¶å›½å®¶è¾¹ç•Œï¼Œä»¥å‡†ç¡®ç»˜åˆ¶å›¾å½¢ã€‚å…·ä½“å®ç°ç›¸å¯¹ç®€å•ï¼Œé¦–å…ˆåº”ç”¨Salemå·¥å…·è¯»å–WRFæ¨¡å‹çš„namelistæ–‡ä»¶ï¼Œç„¶ååº”ç”¨Cartopyå·¥å…·ç»˜åˆ¶ï¼Œå¹¶æ¢ç”¨ä¸Šä¸€èŠ‚ä»‹ç»çš„æ›´ä¸ºæ¸…æ™°çš„åœ°å½¢åº•å›¾ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import salemfrom salem import geogrid_simulatorfrom salem import wgs84fpath = r'./namelist.wps'g, maps,rect= geogrid_simulator(fpath) x0,y0 = pyproj.transform(g[0].proj,wgs84,g[0].extent[0], g[0].extent[2])x1,y1 = pyproj.transform(g[0].proj,wgs84,g[0].extent[1], g[0].extent[3]) def background(ax): fname = './bou2_4p.shp' ## çœç•Œshapefile shape_feature = ShapelyFeature(Reader(fname).geometries(), ccrs.PlateCarree(),facecolor='none',edgecolor ='k',linewidth = 0.25) ax.add_feature(shape_feature) fname = './ä¸­å›½è½®å»“_å«å—æµ·.shp' ## å›½ç•Œshapefile shape_feature = ShapelyFeature(Reader(fname).geometries(),ccrs.PlateCarree(),facecolor='none',edgecolor ='k',linewidth = 0.75) ax.add_feature(shape_feature) fname = './country1.shp' ## å„å›½è¾¹ç•Œshapefile shape_feature = ShapelyFeature(Reader(fname).geometries(),ccrs.PlateCarree(),facecolor='none',edgecolor ='k',linewidth = 0.75) ax.add_feature(shape_feature) ax.set_extent([x0,125,y0+3,55], crs=ccrs.PlateCarree()) xticks = [80,90,100,110,120,130,140] yticks = [10,20,30,40,50,60] fig.canvas.draw() ax.gridlines(xlocs=xticks, ylocs=yticks, color='gray', alpha=0.5, linestyle='--',linewidth = 0.75) ax.xaxis.set_major_formatter(LONGITUDE_FORMATTER) ax.yaxis.set_major_formatter(LATITUDE_FORMATTER) lambert_xticks(ax, xticks) lambert_yticks(ax, yticks) ## LambertæŠ•å½±ç³»ç»Ÿä¸‹ç½‘æ ¼çº¿ç»˜åˆ¶ä»£ç å‚è€ƒhttps://gist.github.com/ajdawson/dd536f786741e987ae4edef find_side(ls, side): \"\"\" Given a shapely LineString which is assumed to be rectangular, return the line corresponding to a given side of the rectangle. \"\"\" minx, miny, maxx, maxy = ls.bounds points = {'left': [(minx, miny), (minx, maxy)], 'right': [(maxx, miny), (maxx, maxy)], 'bottom': [(minx, miny), (maxx, miny)], 'top': [(minx, maxy), (maxx, maxy)],} return sgeom.LineString(points[side])def lambert_xticks(ax, ticks): \"\"\"Draw ticks on the bottom x-axis of a Lambert Conformal projection.\"\"\" te = lambda xy: xy[0] lc = lambda t, n, b: np.vstack((np.zeros(n) + t, np.linspace(b[2], b[3], n))).T xticks, xticklabels = _lambert_ticks(ax, ticks, 'bottom', lc, te) ax.xaxis.tick_bottom() ax.set_xticks(xticks) ax.set_xticklabels([ax.xaxis.get_major_formatter()(xtick) for xtick in xticklabels]) def lambert_yticks(ax, ticks): \"\"\"Draw ricks on the left y-axis of a Lamber Conformal projection.\"\"\" te = lambda xy: xy[1] lc = lambda t, n, b: np.vstack((np.linspace(b[0], b[1], n), np.zeros(n) + t)).T yticks, yticklabels = _lambert_ticks(ax, ticks, 'left', lc, te) ax.yaxis.tick_left() ax.set_yticks(yticks) ax.set_yticklabels([ax.yaxis.get_major_formatter()(ytick) for ytick in yticklabels])def _lambert_ticks(ax, ticks, tick_location, line_constructor, tick_extractor): \"\"\"Get the tick locations and labels for an axis of a Lambert Conformal projection.\"\"\" outline_patch = sgeom.LineString(ax.outline_patch.get_path().vertices.tolist()) axis = find_side(outline_patch, tick_location) n_steps = 30 extent = ax.get_extent(ccrs.PlateCarree()) _ticks = [] for t in ticks: xy = line_constructor(t, n_steps, extent) proj_xyz = ax.projection.transform_points(ccrs.Geodetic(), xy[:, 0], xy[:, 1]) xyt = proj_xyz[..., :2] ls = sgeom.LineString(xyt.tolist()) locs = axis.intersection(ls) if not locs: tick = [None] else: tick = tick_extractor(locs.xy) _ticks.append(tick[0]) # Remove ticks that aren't visible: ticklabels = copy(ticks) while True: try: index = _ticks.index(None) except ValueError: break _ticks.pop(index) ticklabels.pop(index) return _ticks, ticklabels proj = salem.proj_to_cartopy(g[0].proj)fig = plt.figure(figsize=(6.,5), frameon=True)ax1 = plt.subplot(111, projection=proj)cs = cn_back_plot(ax1)ax1.background_img(name='ne_shaded3',resolution='low')ax1.add_geometries([rect[0][0]], crs = proj,facecolor='none', edgecolor='k', lw=2.5)plt.savefig(\"./æ¨¡æ‹ŸåµŒå¥—ç½‘æ ¼.png\",dpi=400)plt.show() å‚è€ƒ ä»NetCDFä¸­æå–å±€éƒ¨æ•°æ® WRF projection Pythonç»˜åˆ¶å®ç”¨åœ°å›¾ WRFå’ŒWPSéƒ¨åˆ†é…ç½®é€‰é¡¹ç®€ä»‹ Springer Cartographics LLC","link":"/2020/08/04/23.%5BModel%5DWRF%E5%B5%8C%E5%A5%97%E7%BD%91%E6%A0%BC%E7%9A%84%E7%BB%98%E5%88%B6/"},{"title":"FLEXPART installation notes","text":"FLEXPART is a Lagrangian particle dispersion model (LPDM) developed by Norwegian Institute of Air Research, Norway. It allows researchers to simulate the long-range transportation, diffusion, dry/wet deposition processes of atmospheric spcecies from their sources. It also can be utilized for backward calculation based on the observation of receptor to anaysis source-receptor relationships. This model is coded following the Fortran 95 standard, and can be freely download from the page here. Flexpart 8.x/9.x is easy for compilation following the offical reference. I noticed that netCDF-format output (which would make the post-processing easier compared to the original binary output files) has been merged in the newer veision. Therefore, I tried to compile FLEXPART 10.0 beta version in the Linux system, while lots of issues appeared. My installing steps are listed as follows Fig.1 Application example: CO age composition of the plume arrived at the receptor source 1. Installing required packagesCompile flag â€œ-fuse-linker-pluginâ€ option is enabled in the makefile of flexaprt. The error gfortran: error: -fuse-linker-plugin is not supported in this configuration indicated that this option may not work in the current system. The website Explain Linux Commands shows that: Enables the use of a linker plugin during link-time optimization. This option relies on the linker plugin support in linker that is available in gold or in GNU ld 2.21 or newer. Thus, I need to re-compile binutils to upgrade the GNU ld (my current version is 2.17 which is attched in Cent OS 5.9). Later on, I have to re-compile all the required packages in theo order: binutils $\\rightarrow$ gcc $\\rightarrow$ netcdf-c/fortran $\\rightarrow$ jasper $\\rightarrow$ grib_api. (PS: since there are many softwares have been compiled in the current environment, I had to edit a new ~/.bashrc_forFlex as an switch to select the suitable system configurations ) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#Install binutils mkdir /disk2/hyf/lib/buildcd /disk2/hyf/lib/buildwget https://ftp.gnu.org/gnu/binutils/binutils-2.27.tar.gztar -xvf binutils-2.27.tar.gzcd binutils-2.27./configure prefix=/home/hli2/libs/binutilsmakemake installcd ../vi ~/.bashrc_forFLexexport DIR=/disk2/hyf/libexport PATH=$DIR/binutils-2.25/bin:$PATHsource ~/.bashrc_forFLex#Install gcc#source code in https://ftp.gnu.org/gnu/gcc/#Referenc in https://gcc.gnu.org/wiki/InstallingGCCcd gcc-7.3.0./contrib/download_prerequisitescd ..mkdir objdircd objdir../gcc-6.1.0/configure --prefix=$DIR/gcc-6.1.0 --enable-languages=c,c++,fortran,go makemake installvi ~/.bashrc_forFlexexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$DIR/gcc-6.1.0/lib/../lib64export PATH=/disk2/hyf/lib/gcc-6.1.0/bin:$PATHexport CC=gcc#export CFLAGS='-gdwarf-2 -gstrict-dwarf -02 -fPIC'export CFLAGS='-gdwarf-2 -gstrict-dwarf'export CXX=g++export FC=gfortranexport FCFLAGS=-m64export F77=gfortranexport FFLAGS=-m64source ~/.bashrc_forFlex#Install netCDFwget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-4.6.1.tar.gztar -zxvf netcdf-4.6.1.tar.gzcd netcdf-4.6.1/wget 'http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.guess;hb=HEAD' -O config.guesswget 'http://git.savannah.gnu.org/gitweb/?p=config.git;a=blob_plain;f=config.sub;hb=HEAD' -O config.sub./configure --prefix=/disk2/hyf/netcdf-4.6.1 CPPFLAGS=\"-I/disk2/hyf/lib/hdf5/include -I/diks2/hyf/lib/grib2/include -O3\" LDFLAGS=\"-L/disk2/hyf/lib/hdf5/lib -L/diks2/hyf/lib/grib2/lib\" --enable-shared --enable-netcdf-4 --disable-dap --disable-doxygenmake -jmake checkmake installvi~/.bashrc_forFlexexport PATH=\"$DIR/netcdf-4.6.1/bin:$PATH\"export NETCDF=$DIR/netcdf-4.6.1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NETCDF/libexport CFLAGS=\"-L$HDF5/lib -I$HDF5/include -L$NETCDF/lib -I$NETCDF/include\"export CXXFLAGS=\"-L$HDF5/lib -I$HDF5/include -L$NETCDF/lib -I$NETCDF/include\"xport FCFLAGS=\"-L$HDF5/lib -I$HDF5/include -L$NETCDF/lib -I$NETCDF/include\"export CPPFLAGS=\"-I${HDF5}/include -I${NETCDF}/include\"export LDFLAGS=\"-L${HDF5}/lib -L${NETCDF}/lib\"source ~/.bashrc_forFlex#Install netCDF-Fortranwget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-fortran-4.4.4.tar.gztar -zxvf netcdf-fortran-4.4.4.tar.gz./configure --prefix=${NETCDF} --enable-shared --disable-doxygenmake -jmake checkmake install#Install jasper./configure --prefix=$DIR/jaspermakemake installvi~/.bashrc_forFlexexport JASPER=$DIR/jasperexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JASPER/libsource ~/.bashrc_forFlex#Install grib_api#download page https://confluence.ecmwf.int/display/GRIB/Releases./configure --prefix=$DIR/grib_api --with-jasper=$DIR/jasper --disable-shared # --enable-sharedvmakemake install#Noted that if disable-shared is not added into the compile flag, the error \"/disk2/hyf/lib/jasper/lib/libjasper.a: error adding symbols: Bad value\" would appear. vi ~/.bashrc_forFlexexport PATH=$GRIB_API_BIN:$PATHexport GRIB_API=$DIR/grib_api/export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$GRIB_API/libexport GRIB_API_BIN=$GRIB_API/binexport GRIB_API_LIB=$GRIB_API/libexport GRIB_API_INCLUDE=$GRIB_API/includesource ~/.bashrc_forFlex 2. Installing FLEXPART123456789101112131415161718192021222324252627282930313233#edit the makefile suitable for the own computer#dowanload page: https://www.flexpart.eu/downloadstar -xvzf flexpart10.01beta.tar.gzcd ./flexpart10.01beta/flexpart# Both ECMWF and GFS data can be used as the inputs for FLEXPART, while the specific model for them should be clarified befoe installation. cp -r src/ test1/cd test1vi Makefile# the lines below should be revised depending on the own computer system. F90 = $GCC/gcc-6.1.0/bin/gfortranINCPATH1 = /disk2/hyf/lib/grib_api/includeINCPATH2 = /disk2/hyf/lib/jasper/includeINCPATH3 = /disk2/hyf/lib/netcdf-4.6.1/includeLIBPATH1 = /disk2/hyf/lib/grib_api/libLIBPATH2 = /disk2/hyf/lib/jasper/libLIBPATH3 = /disk2/hyf/lib/netcdf-4.6.1/libvi par_mod.f90# the default parameters in the par_mod.f90 was set for ECMWF windfield datasets. Therefore, some lines need to be revised. # Adjust the following parameters to switch between FNL/ECMWF datasets.!integer,parameter :: nxmax=361,nymax=181,nuvzmax=92,nwzmax=92,nzmax=92 !FNL XF!integer,parameter :: nxmax=361,nymax=181,nuvzmax=152,nwzmax=152,nzmax=152 !ECMWF new !integer,parameter :: nxmax=361,nymax=181,nuvzmax=92,nwzmax=92,nzmax=92 !ECMWF!integer,parameter :: nxmax=361,nymax=181,nuvzmax=26,nwzmax=26,nzmax=26integer,parameter :: nxmax=721,nymax=361,nuvzmax=64,nwzmax=64,nzmax=64!integer,parameter :: nxmax=1201,nymax=235,nuvzmax=58,nwzmax=58,nzmax=58!integer,parameter :: nxshift=359 ! for ECMWFinteger,parameter :: nxshift=0 ! for GFS or FNLmake -j 8 gfs 3. Installing FLEXPART-WRFIt has been known that the results of Lagrangian particle dispersion models (LPDM) would affected by the errors in spatio-temporal interpolation of the simulated meteorological field. Furthermore, there are some uncertainties in the aspect of simulating mixing status for particles due to the erros in the vertical velocity. Therefore, the output from mesoscale models which were finer in vertical/horizontial resolution and time interval have been introduced for partially address these issue. In 2006, Flexpart-WRF model has been developed combining the Flexpart utilities and WRF output. Compared to the standard model using global GFS/ECMWF datasets, Flexpart-WRF can better depict the atmospheric transportation especially for small-scale applications. 123456789tar -xvzf ./flexpart_wrf_3.3.2.tar.gzcd flexpart_wrfcp -r src testcd testvi makefile.mom # set the ${NETCDF} environment variablemake -f makefile.mom omp#Done! 4. Some tips123456789# Error1 NetCDF: Invalid dimension ID or name when using forward mode of FLEXPART-WRF# REFERENCE: https://www.flexpart.eu/ticket/233# There is a typo error in the original netcdf_out_mod.F90!Change Line 624- ncret = NF90_DEF_VAR(ncid,'DRYDEP',NF90_FLOAT,ncdimsid2,ncddvid,&amp;+ ncret = NF90_DEF_VAR(ncid,'DRYDEP',NF90_FLOAT,ncdimsid22,ncddvid,&amp; å‚è€ƒèµ„æ–™1. FlXEXPART Offical website 2. Problem compiling GRIB AP 3. Compile with flag -fPIC 4. FLEXPART various compilation problems","link":"/2018/08/10/10.%5BModel%5DFLEXPART_INSTALLATION_NOTE/"},{"title":"å®‰è£…ç¬”è®°: åœ¨Jupyterä¸­æ·»åŠ R Kernel","text":"Jupyter(å³åŸæ¥çš„Ipython Notebook Project)åŸºäºWebç«¯çš„å®æ—¶äº¤äº’ï¼Œæ‹¥æœ‰ç€æä½³çš„ç”¨æˆ·ä½“éªŒã€‚åŒæ—¶ï¼ŒJupyterä½“ç³»å¯ç›´æ¥ä½¿ç”¨å…¶ä»–è¯­è¨€ï¼Œç”¨æˆ·å‘¢èƒ½å¤Ÿåœ¨åŒä¸€ç•Œé¢ä¸‹è°ƒç”¨Rå†…æ ¸æˆ–Pythonå†…æ ¸(å…±æ”¯æŒ49ç§)ï¼ŒåŠŸèƒ½ååˆ†å¼ºå¤§ã€‚åœ¨Mac OSä¸‹å°è¯•åœ¨Jupyterä¸­æ·»åŠ Rè¯­è¨€å†…æ ¸ï¼Œé‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œåœ¨æ­¤è®°å½•ä¸ªäººå®‰è£…è¿‡ç¨‹å’Œè¯¸é—®é¢˜çš„è§£å†³æ–¹æ³•ã€‚ STEP1 åŸºæœ¬è½¯ä»¶å‡†å¤‡ Pythonå’ŒIPython.ä½œä¸ºåˆå­¦è€…ï¼Œæˆ‘ç›´æ¥å®‰è£…äº†Pythonçš„ç§‘å­¦è®¡ç®—å‘è¡Œç‰ˆAnacondaï¼Œå…¶åŒ…å«äº†Python2.7å¹¶å†…ç½®å¾ˆå¤šæœ‰ç”¨çš„Package(numpy, SciPy, Matplotlib)ï¼ŒJupyter(Ipython Notebook)ä»¥åŠSpyder(ä¸ªäººæ„Ÿè§‰ç±»ä¼¼äºR studioçš„é›†æˆå¼€å‘ç¯å¢ƒ) R 3.2.1(World-Famous Astronaut)ï¼Œå®˜ç½‘ä¸‹è½½å®‰è£…å³å¯ã€‚ STEP2 æœ‰å…³åº“çš„å®‰è£…ä¸»è¦å·¥ä½œæ˜¯ä¸ºRè¯­è¨€æ·»åŠ IRkernalè¿™ä¸€æ–°çš„Package, ä»¥å®ç°Jupyterå¯¹Rè¯­è¨€çš„æ”¯æŒã€‚ä¸»è¦çš„æ­¥éª¤å¦‚ä¸‹(ä¾ç…§IRkernalçš„READMEæ–‡æ¡£)ï¼š R console123456789## å®‰è£…å¼€å‘å·¥å…·åº“install.packages(\"devtools\") ## è°ƒç”¨RCurlï¼Œå¯é€šè¿‡githubä¸‹è½½æœ‰å…³æ–‡ä»¶install.packages('RCurl')library(devtools)install_github('armstrtw/rzmq')install_github(\"takluyver/IRdisplay\")install_github(\"takluyver/IRkernel\")IRkernel::installspec() åœ¨å®‰è£…rzmqæ—¶å‡ºç°é—®é¢˜ï¼Œå¯¼è‡´åç»­æ­¥éª¤æ— æ³•è¿›è¡Œã€‚å‚è€ƒç½‘ç»œèµ„æºï¼Œæ”¹ç”¨brewå·¥å…·è¿›è¡Œå®‰è£… Terminal1$ brew install zmq ä¸‹è½½æˆåŠŸä½†æœªèƒ½å®‰è£…ï¼Œåˆ©ç”¨brew doctorè¿›è¡Œè¯Šæ–­ ç»“æœæ˜¾ç¤ºï¼Œæ˜¾ç¤º/usr/local/lib/pkgconfig is not writable. é¦–å…ˆè€ƒè™‘çš„æ˜¯MacPortå¯èƒ½å­˜åœ¨å¹²æ‰°ï¼Œå°†å…¶æš‚æ—¶ç§»é™¤ 1$ sudo mv /opt/local /macports pkgconfigè¿›è¡Œé‡æ–°å®‰è£…å…³è” 12345$ sudo chown -R `whoami` /usr/local/lib/pkgconfig$ brew uninstall pkgconfig $ brew install pkgconfig $ brew unlink pkg-config &amp;&amp; brew link pkg-config $ brew link zmq å›åˆ°R Console123install_github(\"takluyver/IRdisplay\")install_github(\"takluyver/IRkernel\")IRkernel::installspec() æœ€åä¸€ä¸ªæ­¥éª¤å†æ¬¡å‡ºç°é—®é¢˜ï¼š 123Error in IRkernel::installspec(): Jupyter or IPython 3.0 has to be installed but could neither run â€œjupyterâ€ nor â€œipythonâ€, â€œipython2â€ or â€œipython3â€.(Note that â€œipython2â€ is just IPython for Python 2, but still may be IPython 3.0) å®‰è£…åŒ…çš„ä½ç½®å‡ºç°é—®é¢˜ï¼Œè§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š 1print(system.file(\"kernelspec\", package = \"IRkernel\")) å¤åˆ¶è¯¥å‘½ä»¤çš„è¾“å‡ºç»“æœï¼Œæˆ‘çš„ç»“æœæ˜¯ï¼š/Library/Frameworks/R.framework/Versions/3.2/Resources/library/IRkernel/kernelspec) å†å›åˆ°Terminal(å¤ŸæŠ˜è…¾çš„)1ipython kernelspec install --replace --name ir --user /Library/Frameworks/R.framework/Versions/3.2/Resources/library/IRkernel/kernelspec æŒ‡ä»¤ä¸­çš„è·¯å¾„æ”¹ä¸ºä¸ªäººçš„è¾“å‡ºç»“æœå³å¯ã€‚ STEP3 æ£€éªŒTerminalä¸­é”®å…¥ipython notebookæˆ–jupyter notebookæˆ–åœ¨Anacondaå›¾å½¢ç•Œé¢ä¸‹å°±å¯ä»¥æ‰“å¼€Jupyterçš„ç½‘é¡µï¼Œå¦‚æœå‡ºç°ä¸‹å›¾çš„é€‰é¡¹ï¼Œå°±è¯´æ˜é¡ºåˆ©å®Œæˆäº†! ä½¿ç”¨ä¸­å‡ºç°çš„å…¶ä»–é—®é¢˜ä½¿ç”¨æ—¶çªç„¶å‡ºç°ï¼ŒR kernel æ— æ³•è¿æ¥çš„é—®é¢˜ jupyter-client has to be installed but â€œjupyter kernelspec â€“versionâ€ exited with code 127. æ­¤ç§æƒ…å†µï¼Œæ‰§è¡Œä¸‹è¿°å‘½ä»¤ï¼Œå¯ä¿®å¤ 12## æ¢æˆè‡ªå·±çš„ç”¨æˆ·åè·¯å¾„ sudo ln -s /Users/HYF/anaconda/bin/jupyter /usr/bin/jupyter å‚è€ƒ Native R kernel for Jupyter Multi-Language IPython (Jupyter) setup","link":"/2015/08/16/1.%5BCode%5D%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0-%E5%9C%A8Jupyter%E4%B8%AD%E6%B7%BB%E5%8A%A0R-Kernel/"},{"title":"å¯è§†åŒ–å·¥å…·Altairå­¦ä¹ ","text":"æè¦ï¼šAltairæ˜¯å¼ºå¤§çš„å¯è§†åŒ–åº“ï¼Œå…¶åŸºäºVega-lite,å¯å¿«é€Ÿç”Ÿæˆç®€æ´ã€ç¾è§‚ã€å¯äº’åŠ¨çš„ç»Ÿè®¡å›¾å½¢ã€‚æœ¬æ–‡ä»‹ç»ä¸ªäººç›¸å…³å­¦ä¹ ç»å†ï¼Œå…·ä½“åŒ…æ‹¬ï¼š(1)æ•°æ®è½½å…¥ä¸åŸºæœ¬å›¾å½¢ç»˜åˆ¶; (2)åŸºäºwebç«¯å‘ˆç° èƒŒæ™¯çŸ¥è¯†ä»‹ç»å¸¸ç”¨çš„æ•°æ®å¯è§†åŒ–å·¥å…·åŒ…ï¼Œä¾‹å¦‚Rè¯­è¨€ä¸‹çš„ggplot2å’ŒPythonè¯­è¨€ä¸‹çš„matplotlibï¼Œå‡éš¾ä»¥åˆ¶ä½œäº¤äº’å¼å›¾è¡¨ã€‚ä¸åŒäºä¼ ç»Ÿçš„é™æ€å›¾è¡¨ï¼Œäº¤äº’å¼å›¾è¡¨å…è®¸ç ”ç©¶è€…åŠè¯»è€…å¯ä»¥æ¢ç´¢ç”šè‡³æ“çºµåŸå§‹æ•°æ®ï¼Œä¾¿äºæˆ‘ä»¬ç†è§£æ•°æ®å…³ç³»ï¼Œå‘ç°å…¶éšè—è§„å¾‹ã€‚Pythonè¯­è¨€ä¸‹å¯äº¤äº’çš„å¯è§†åŒ–å·¥å…·åŒ…æ‹¬Altair, BokehåŠplotlyç­‰å¤šç§ã€‚ MatPlotlibçš„ä¼˜åŠ¿ï¼š Designed like MatLab: switching was easy Many rendering backends well-tested, standard tool for 15 years can reproduce just about any plot with a bit of effort MatPlotlibçš„åŠ£åŠ¿ API is imperative &amp; often overly verbose Poor/no support for interactive/web graphså­˜åœ¨é—®é¢˜: we are mixing the what with the how åœ¨åº”ç”¨MatPlotLibçš„è¿‡ç¨‹ä¸­ï¼Œä¸ä»…éœ€è¦è€ƒè™‘ç”»ä»€ä¹ˆï¼Œè¿˜éœ€è¦è€ƒè™‘å¦‚ä½•å®ç°ã€‚åæ ‡è½´ç­‰äº¦éœ€è¦è‡ªè¡Œè®¾ç½®ï¼Œæ¯æ¬¡éƒ½åœ¨ç­‰å¾®è°ƒä¸­è€—è´¹å¤§é‡æ—¶é—´ã€‚å®é™…ä¸Šï¼Œåº”å½“å°†æ›´å¤šçš„æ—¶é—´ç”¨äºæ•°æ®çš„åˆ†æåŠå†™ä½œä¸­ï¼Œå‡ºå›¾æ•ˆç‡åº”æé«˜ã€‚å› æ­¤ï¼Œæˆ‘å†³å®šå…¨é¢è½¬å‘altairä½œä¸ºæœªæ¥ä¸ªäººçš„ä¸»è¦å¯è§†åŒ–å·¥å…·ï¼Œåœ¨æ—¥å¸¸å·¥ä½œä¸­é¢‘ç¹ä½¿ç”¨ï¼Œä¸æ–­ç§¯ç´¯ã€‚ 123## å®‰è£…## https://raw.githubusercontent.com/vega/vega/master/docs/examples/bar-chart.vg.jsonpip install altair vega_datasets Webç«¯æ•°æ®å±•ç¤ºå…ˆå­˜æˆjsonæ–‡ä»¶ï¼Œå†åˆ©ç”¨vega embed å®ç°ã€‚å…¶åŸºæœ¬æ ¼å¼æ€»ç»“å¦‚ä¸‹ï¼š 123456789101112131415161718{% raw %}&lt;html&gt;&lt;head&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vis\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; # specä¸­è½½å…¥altarå¯¹è±¡ç”Ÿæˆçš„jsonæ–‡ä»¶ var spec = \"/uploads/vconcat.json\"; var opt = {\"renderer\": \"canvas\", \"actions\": false}; vegaEmbed(\"#vis\", spec, opt); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;{% endraw %} æ­¤å¤„å±•ç¤ºåŸºæœ¬ä¾‹å­ 123456789101112131415161718{% raw %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vis1\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var spec1 = \"/uploads/vconcat.json\"; var opt1 = {\"renderer\": \"canvas\", \"actions\": false}; vegaEmbed(\"#vis1\", spec1, opt1); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;{% endraw %} åœ¨åŒä¸€ä¸ªé™æ€ç½‘ç«™ä¸­éœ€è¦å±•ç¤ºå¤šä¸ªinteractive plots, åŸºæœ¬æ–¹æ³•å¦‚ä¸‹ï¼šä¿®æ”¹ä¸Šè¿°æ–‡ä»¶ä¸­3ä¸ªå˜é‡, vis, spec, opt 123456789101112131415161718{% raw %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vis2\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var spec2 = \"/uploads/cc.json\"; var opt2 = {\"renderer\": \"canvas\", \"actions\": false}; vegaEmbed(\"#vis2\", spec2, opt2); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;{% endraw %} 123456789101112131415161718{% raw %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"&gt;&lt;/script&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vis3\"&gt;&lt;/div&gt; &lt;script type=\"text/javascript\"&gt; var spec3 = \"/uploads/car-clickable-legend.json\"; var opt3 = {\"renderer\": \"canvas\", \"actions\": false}; vegaEmbed(\"#vis3\", spec3, opt3); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;{% endraw %} å¸¸è§é—®é¢˜ æ•°æ®é›†é•¿åº¦è¿‡å¤§ (The number of rows in your dataset is greater than the maximum allowed (5000).)å®˜æ–¹å›ç­”æ‘˜å½•å¦‚ä¸‹ This is not because Altair cannot handle larger datasets, but it is because it is important for the user to think carefully about how large datasets are handled. As noted above in Why does Altair lead to such extremely large notebooks?, it is quite easy to end up with very large notebooks if you make many visualizations of a large dataset, and this error is a way of preventing that.2.ä¸Šæ ‡ã€ä¸‹æ ‡åº”å½“å¦‚ä½•å±•ç¤ºaltairä¸æ”¯æŒlatexç¬¦å·çš„æ˜¾ç¤ºï¼Œå› æ­¤éœ€è¦å¯»æ‰¾å„æ•°å­¦ç¬¦å·çš„unicodeå­—ç¬¦ã€‚å¸¸ç”¨å­—ç¬¦æ•´ç†å¦‚ä¸‹ï¼š å¾®å…‹/ç±³ä¸‰æ¬¡æ–¹ï¼šâ€™\\u03BCâ€™+â€™g/mâ€™+â€™\\u00B3â€™ å‚è€ƒæ–‡çŒ® PYCON 2018 D3 in hexo åµŒå…¥bokehç»˜å›¾è‡³hexoåšå®¢ä¸­ The reason I am using Altair for most of my visualization in Python å¦‚ä½•æ’å…¥å¤šä¸ªå›¾ç‰‡ vega-embed vega-lite How to find the unicode of the subscript alphabet?","link":"/2020/05/10/21.%5BCode%5DAltair%E5%AD%A6%E4%B9%A0/"},{"title":"Making Your Own Python Package Publicly Available","text":"In this post, I document the full procedure for turning a personal Python project into a public package, available on both GitHub and the Python Package Index (PyPI). As a demonstration, I create a small package named PalAniSh (Palette of Animation of Shanghai), which extracts and displays color palettes from classical Chinese animations produced by the Shanghai Animation Film Studio. 1. Color Palette generationHerein, I will use some of the screenshots from the Chinese traditional animations, e.g., the 1956 animation _The Proud General (éª„å‚²çš„å°†å†›). All raw images as source of color palette were derived from Bilibili. The extraction and combination of colors were similar to the web-based tool Adobe color or Colormind (http://colormind.io/image). Below is the core workflow: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384from PIL import Imageimport matplotlib as mplfrom matplotlib.offsetbox import OffsetImage, AnnotationBboxfrom matplotlib.colors import ListedColormapimport cv2import extcolorsfrom colormap import rgb2hex,hex2rgbimport colorsysdef get_hsv(hexrgb): hexrgb = hexrgb.lstrip(\"#\") # in case you have Web color specs r, g, b = (int(hexrgb[i:i+2], 16) / 255.0 for i in np.arange(0,5,2)) return colorsys.rgb_to_hsv(r, g, b)def get_hls(hexrgb): hexrgb = hexrgb.lstrip(\"#\") # in case you have Web color specs r, g, b = (int(hexrgb[i:i+2], 16) / 255.0 for i in np.arange(0,5,2)) return colorsys.rgb_to_hls(r, g, b)### HSV sorting def get_sorted_df_color(input_color,sort_type): input_ = input_color colors_pre_list = str(input_).replace('([(','').split(', (')[0:-1] df_rgb = [i.split('), ')[0] + ')' for i in colors_pre_list] df_percent = [i.split('), ')[1].replace(')','') for i in colors_pre_list] df_color_up = [rgb2hex(int(i.split(\", \")[0].replace(\"(\",\"\")), int(i.split(\", \")[1]), int(i.split(\", \")[2].replace(\")\",\"\"))) for i in df_rgb] df_color_up_new = df_color_up.copy() if sort_type == 'hsv': df_color_up_new.sort(key = get_hsv) if sort_type == 'hls': df_color_up_new.sort(key = get_hls) df_color_up_new = df_color_up_new[::-1] if \"#FFFFFF\" in df_color_up_new: df_color_up_new.insert(0, \"#FFFFFF\") df_color_up_new = df_color_up_new[0:-1] re_index = [df_color_up.index(c) for c in df_color_up_new] df_percent_new = [df_percent[i] for i in re_index] df = pd.DataFrame(zip(df_color_up_new, df_percent_new), columns = ['hsv_code','occurence']) if sort_type == 'none': df = pd.DataFrame(zip(df_color_up, df_percent), columns = ['hsv_code','occurence']) df['rgb_color'] = [[hex2rgb(c)[0]/255.0,hex2rgb(c)[1]/255.0,hex2rgb(c)[2]/255.0,] for c in df['hsv_code']] return dfdef get_color_x(input_name,tol,num): output_width = 200 #set the output size img = Image.open(input_name) wpercent = (output_width/float(img.size[0])) hsize = int((float(img.size[1])*float(wpercent))) img = img.resize((output_width,hsize), Image.ANTIALIAS) # 1. Show the resized image resize_name = './resize_' + 'test_.png'#the resized image name img.save(resize_name) img_url = resize_name colors_x = extcolors.extract_from_path(img_url, tolerance=tol, limit = num) return colors_x# plot the result color palattedef plot_image_with_color_palette(df_color,input_name,palatee_name,axs): img_url = input_name img = plt.imread(img_url) axs[0].imshow(img) axs[0].axis('off') name = palatee_name color_list = df_color['rgb_color'].values color_name = df_color['hsv_code'].values cmap_diy = ListedColormap(color_list, name) col_map = cmap_diy new_val = [] ticks = np.linspace(0.0,1, len(color_list)+1) for i in range(0,len(ticks)-1,1): new_val.append((ticks[i]+ticks[i+1])/2.0) cbar = mpl.colorbar.ColorbarBase(axs[1], cmap=col_map, orientation = 'horizontal', ticks =new_val, alpha = 0.75) cbar.ax.set_xticklabels(color_name, fontsize =10, rotation = 30) ttl = plt.title(name,fontweight=\"bold\",fontsize =16,) ttl.set_position([.5, 1.15]) Example output 12345input_name = './SA_palette/ä¹è‰²é¹¿/ä¹è‰²é¹¿9.jpg'colors_x = get_color_x(input_name,10,10)df_color_jsl = get_sorted_df_color(colors_x,'hsl')fig, axs = plt.subplots(2,1,figsize=(6,5), gridspec_kw={'height_ratios': [5, 1]})plot_image_with_color_palette(df_color_jsl,input_name,'A Deer of Nine Colors', axs) Other examples are shown as follows 2. Packaging the Code and Uploading to GitHubUsually, a package will have such structure: 12345678910palanish/â”œâ”€â”€ palanish/ # main folderâ”‚ â”œâ”€â”€ __init__.py # essential even it is empty. With this file included, Python treats the folder as a package.â”‚ â””â”€â”€ color_extractor.py # main functionsâ”œâ”€â”€ tests/ # optionalâ”‚ â””â”€â”€ test_basic.pyâ”œâ”€â”€ README.mdâ”œâ”€â”€ LICENSEâ”œâ”€â”€ setup.pyâ””â”€â”€ requirements.txt First step, add the folder of palanish as previous listed and add the main function into the color_extractor.py. Later, within the __init__.py, add following line: 1from .color_extractor import get_color_palette, hex_sort Then, write the script of the setup.py as follows: 123456789101112131415161718192021222324from setuptools import setup, find_packagessetup( name='palanish', version='0.1.0', description='Palette extraction from Shanghai Animation images', author='Yufang Hao', author_email='your_email@example.com', url='https://github.com/envhyf/palanish', packages=find_packages(), install_requires=[ 'pillow', 'extcolors', 'matplotlib', 'colormap', 'pandas' ], classifiers=[ 'Programming Language :: Python :: 3', 'License :: OSI Approved :: MIT License', ], python_requires='&gt;=3.6',) As well as writing a small README.md, 12345678910111213141516171819202122# PalAniShExtract and visualize color palettes from classic Shanghai animations or any screenshots.## Installationpip install git+https://github.com/envhyf/palanish.git## Usage```pythonfrom palanish import get_color_palettecolors = get_color_palette(\"screenshot.jpg\")### ğŸ”‘ 4. `LICENSE`You can use [MIT License](https://choosealicense.com/licenses/mit/) for open source:```txtMIT LicenseCopyright (c) 2024 Yufang HaoPermission is hereby granted, free of charge, ... The final step is uploading to Github 1234567cd palanishgit initgit add .git commit -m \"Initial commit\"git branch -M maingit remote add origin git@github.com:envhyf/palanish.gitgit push -u origin main After all steps done, anyone can install via: 1pip install git+https://github.com/envhyf/palanish.git 3. Web appWith","link":"/2023/06/01/29.%5BCode%5DMaking_my_own_Python_Package/"},{"title":"Dataset and Tools Collection for Earth Science","text":"æœ¬æ–‡æŒç»­æ›´æ–°ä¸ªäººç½‘ç»œæœé›†çš„å„ç±»æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸­å›½/ä¸–ç•Œè¡Œæ”¿åŒºåˆ’ã€è‡ªç„¶åœ°ç†ã€æ°”è±¡æ°´æ–‡ã€ç©ºæ°”è´¨é‡åŠç¤¾ä¼šç»æµæ•°æ®ç­‰ã€‚ 1. è¡Œæ”¿åŒºåˆ’ / ç¤¾ä¼šç»æµæ•°æ® Countries States Cities Database (å…¨çƒè¡Œæ”¿åŒº)æä¾› JSON, SQL, XML, YAML, CSV ç­‰å¤šç§æ ¼å¼ World Countries in 31 Languagesæä¾›ä¸–ç•Œå›½å®¶ä¿¡æ¯çš„å¤šè¯­ç§ç‰ˆæœ¬å’Œæ ¼å¼ï¼ˆCSV, JSON, PHP, SQL, XMLï¼‰ ä¸­å›½å¿çº§ä»¥ä¸Šè¡Œæ”¿åŒºåˆ’å˜æ›´æŸ¥è¯¢æ°‘æ”¿éƒ¨å®˜æ–¹å¹³å°ï¼Œæ›´æ–°è‡³2021å¹´ å°åº¦è¡Œæ”¿åŒºåˆ’ GeoJSON æ–‡ä»¶ï¼‰ 2. æ±¡æŸ“æ’æ”¾ä¸èƒ½æºæ•°æ® Open-Source Mapping of Chinaâ€™s Energy Infrastructureç”±è±æ–¯å¤§å­¦ä¸­å›½ç ”ç©¶ä¸­å¿ƒå‘å¸ƒï¼ŒåŒ…å«å…¨å›½èƒ½æºè®¾æ–½ç©ºé—´åˆ†å¸ƒ Steel, Chemicals and Heavy Industries in Chinaç®€è¦ä»‹ç»ä¸­å›½é‡å·¥ä¸šå‘å±•çŠ¶å†µ ä¸–ç•Œèƒ½æºæ¶ˆè€—ä¸åŒ–çŸ³ç‡ƒæ–™æ ¼å±€æ³•å›½ Sciences Po åœ°ç†èµ„æºå¹³å° å…¨çƒç«ç”µå‚åˆ†å¸ƒå›¾2020å¹´å…¨çƒç«ç”µå‚åœ°å›¾ Global Power Plant DatabaseåŒ…å«å„å›½ç«ç”µã€æ°´ç”µã€æ ¸ç”µç­‰å‘ç”µå‚ä¿¡æ¯ 3. å¤§æ°”åŒ–å­¦ç»„åˆ†, ç©ºæ°”è´¨é‡æ•°æ®åŠå¯è§†åŒ–å¹³å° Sentinel-5Pé€æ—¥æ•°æ®åŒ…å«æ°”æº¶èƒ¶å…‰å­¦åšåº¦(AOD)ä»¥åŠCH4, C2H4O2, CO, HCHO, O3, NO2, SO2ç­‰æ°”æ€ç»„åˆ†çš„å…¨çƒé€æ—¥åˆ†å¸ƒæ•°æ®ã€‚User caseé‡Œæœ‰éå¸¸å¥½çš„åˆ¶å›¾å‚è€ƒã€‚ å…¨çƒé€æ—¥PM2.5æµ“åº¦åˆ†å¸ƒ 2017-2022 1Kmåˆ†è¾¨ç‡, ç±»ä¼¼çš„å…¶ä»–æ±¡æŸ“ç‰©æ•°æ®ï¼ŒåŒæ ·æ˜¯éŸ¦æ™¶åšå£«æä¾›çš„ï¼Œè¯¦è§[https://weijing-rs.github.io/product.html] å…¨çƒ1998-2023 å«æ˜Ÿåæ¼”PM2.5åˆ†å¸ƒ AQICN å…¨çƒç©ºæ°”è´¨é‡ä¿¡æ¯å…¨çƒåˆ†å¸ƒ | äºšæ´²8æ—¥é¢„æµ‹ ä¸­å›½ä¸»è¦åŸå¸‚ç©ºæ°”è´¨é‡å¯¹æ¯”å›¾è¡¨ï¼ˆç½‘å‹ä½œå“ï¼‰ PiMi ç©ºæ°”è´¨é‡å¯è§†åŒ–å¹³å° ç»¿è‰²åœ°çƒ - å¯è§†åŒ–ç©ºæ°”è´¨é‡åœ°å›¾ Green Earth from NOAA éŸ©å›½WRF-CHEMæ¨¡æ‹ŸåŠ¨ç”» NASA Earth Observatory NASA Worldview å®æ—¶çœŸå½©è‰²åœ°çƒ NASA Visualization Studio Maphub åœ°å›¾äº‘é›†å¯æŸ¥çœ‹å¹¶è‡ªåˆ¶å„ç§ç±»å‹çš„å†å²ä¸ç°ä»£åœ°å›¾ AirVisual Earth â€“ ç©ºæ°”è´¨é‡ä¸é£åœºå¯è§†åŒ– 4. äººä½“å¥åº·ç›¸å…³æ•°æ®é›†Global Burden of Disease (GBD) by IHME åç››é¡¿å¤§å­¦å¥åº·æŒ‡æ ‡ä¸è¯„ä¼°ç ”ç©¶æ‰€ï¼ˆIHMEï¼‰æä¾›çš„å…¨çƒç–¾ç—…è´Ÿæ‹…æ•°æ®åº“ï¼ŒåŒ…æ‹¬æ­»äº¡ã€è‡´æ®‹ã€é£é™©å› å­ã€æš´éœ²ã€æœŸæœ›å¯¿å‘½ç­‰é«˜åˆ†è¾¨ç‡æ•°æ®ï¼Œå¯æŒ‰å›½å®¶ã€çœã€æ€§åˆ«ã€å¹´é¾„ç­‰ç»´åº¦ä¸‹è½½ã€‚","link":"/2023/01/03/27.%5BDataset%5DEarth-Science-Tools-and-Data/"},{"title":"Windowsç”µè„‘å·¥ä½œç¯å¢ƒé…ç½®ç¬”è®°","text":"åœ¨å›½å¤–æ–°è´­ç½®äº†ä¸€å°PCï¼Œæ€§èƒ½æ¯”ä»å›½å†…å¸¦æ¥çš„Macç¬”è®°æœ¬è¦å¥½ï¼Œä¹Ÿå¯ä»¥ç»§ç»­ç©Steamä¸Šçš„æ¸¸æˆã€‚å†³å®šå°†ä¸ªäººå­¦ä¹ å·¥ä½œç¯å¢ƒè¿ç§»è‡³Windowsç³»ç»Ÿã€‚åœ¨æ­¤è®°å½•ç›¸å…³é…ç½®æ­¥éª¤ï¼Œä»¥ä¾¿æœªæ¥ä¹‹éœ€ã€‚ 1. å¸¸ç”¨è½¯ä»¶å®‰è£…é¦–å…ˆå®‰è£…å¸¸ç”¨è½¯ä»¶ï¼Œå°½é‡é€‰æ‹©å…è´¹å¼€æºè½¯ä»¶æ›¿ä»£ç›¸ä¼¼åŠŸèƒ½çš„å•†ä¸šè½¯ä»¶ã€‚ Markdownæ–‡æœ¬ç¼–è¾‘ï¼šMark text è§†é¢‘æ’­æ”¾å·¥å…·ï¼šmpv å›¾ç‰‡æµè§ˆå·¥å…·ï¼šImageGlass PDFæ–‡æœ¬é˜…è¯»ï¼šSumatra PDF (Ctrl+a é«˜äº®æ–‡æœ¬) å°å·¥å…·åˆé›†ï¼šPowerToys ï¼ˆWin+Shift+C å–è‰²å™¨) æˆªå›¾å·¥å…·ï¼šSnipaste (F1æˆªå›¾å¿«æ·é”®ï¼‰ æ‹¼éŸ³è¾“å…¥æ³•ï¼šRIME (å®‰è£…ååˆå§‹ä¸ºç¹ä½“å­—ï¼ŒF4è¿›å…¥è®¾ç½®èœå•è°ƒæ•´ä¸ºç®€ä½“) ç•ªèŒ„æ—¶é’Ÿ: Pomotroid FTPæ•°æ®ä¼ è¾“ç®¡ç†: WinSCP 2. Python+VSCODE+WSLPythonåŠJupyter notebook/labæ˜¯æˆ‘ä¸»è¦çš„æ•°æ®åˆ†æå·¥å…·ã€‚VS CODEç¼–è¾‘å™¨æ˜¯åŠŸèƒ½å¼ºå¤§ï¼Œæ’ä»¶ä¸°å¯Œçš„å¼€å‘å·¥å…·ï¼Œæœªæ¥ä¼šå¤šå°è¯•åœ¨è¯¥ç¯å¢ƒä¸‹å·¥ä½œã€‚WSLå³Windows subsystem for Linuxï¼Œæ˜¯Windowsç¯å¢ƒä¸‹Linuxå­ç³»ç»Ÿï¼Œæ— éœ€å®‰è£…åŒç³»ç»Ÿï¼Œä¸”å¯ä»¥åœ¨Linuxç¯å¢ƒå¯ç›´æ¥è®¿é—®Windowsæ–‡ä»¶ç³»ç»Ÿï¼Œç›¸æ¯”è™šæ‹Ÿæœºæœ‰æ›´å¥½çš„è¿è¡Œæ€§èƒ½ã€‚ å…·ä½“ä¸‹è½½å®‰è£…ç¯èŠ‚åŒ…æ‹¬ Anaconda VS Code Insiders (æ­¤å¤„å®‰è£…Insiderï¼Œè€Œéæ ‡å‡†ç‰ˆVS Codeã€‚åŸå› æ˜¯å› ä¸ºåœ¨åŠ è½½remote developmentç­‰æ’ä»¶æ—¶æ€»æ˜¯æŠ¥é”™ï¼Œè‡ªå·±æ²¡æ‰¾åˆ°è§£å†³æ–¹æ³•) Ubuntu 20.04 LTS Windows Terminal å®‰è£…ä¸Šè¿°è½¯ä»¶å·¥å…·åï¼Œåœ¨VS Codeä¸­å®‰è£…remote-WSLã€jupyterã€VS Code Jupyter Notebook Previewerã€Visual Studio IntelliCodeã€Chinese Language packç­‰å¿…è¦æ’ä»¶ã€‚ 1234567891011121314151617## terminalç¯å¢ƒä¸‹wslåˆ‡æ¢è‡³linuxcurl -O https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.shç¯å¢ƒï¼Œå®ŒæˆåŸºæœ¬é…ç½®## å®‰è£…å¿…è¦ç¼–è¯‘ç¯å¢ƒsudo apt updatesudo apt install build-essentia## å®‰è£…zsh, oh-my-zsh, powerlinesudo apt-get install zshgit clone https://github.com/ohmyzsh/ohmyzsh.git ~/.oh-my-zshcp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrcchsh -s /bin/zshsudo apt-get install powerline fonts-powerline##WSLç¯å¢ƒå®‰è£…anaconda3cd /tmpcurl -O https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.shbash ./Anaconda3-2021.05-Linux-x86_64.sh æ¥ä¸‹æ¥å®ç°åœ¨wslç¯å¢ƒå¼€å¯Jupter labï¼Œåœ¨Windowsç¯å¢ƒchromeæµè§ˆå™¨æ‰“å¼€ï¼Œç›´æ¥å¼€å¯jupyter notebook/labï¼Œç³»ç»ŸæŠ¥é”™å‡ä¸ºâ€Start : This command cannot be run due to the error: The system cannot find the file specified.â€, ä½†å¹¶å½±å“ä½¿ç”¨ã€‚ æ ¹æ®æ­¤é—®é¢˜çš„å›ç­”ï¼ŒUbuntu 20.04ç³»ç»Ÿä¼šå‡ºç°è¯¥é—®é¢˜ã€‚ æœ‰å…³terminalç¯å¢ƒä¸‹æ–‡ä»¶å¤¹å‡ºç°ç»¿è‰²èƒŒæ™¯ï¼Œå½±å“ç¾è§‚ï¼Œå‚è€ƒè¯¥é—®ç­”è¿›è¡Œä¿®æ”¹ã€‚ bashç¯å¢ƒä¸‹vi ~/.dir_colorsä¿®æ”¹OTHER_WRITABLEä»â€34;42â€è‡³â€œ34;49â€ï¼Œå¦‚ä¸‹ zshç¯å¢ƒä¸‹åˆ™ä¸ä¸ºæ‰€åŠ¨ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹è‡³~/.zshrcï¼Œå¹¶sourceä¹‹ 12345if [[ -f ~/.dircolors ]] ; then eval $(dircolors -b ~/.dircolors) elif [[ -f /etc/DIR_COLORS ]] ; then eval $(dircolors -b /etc/DIR_COLORS)fi 3. Latexé…ç½®æœªæ¥å¯èƒ½æœ‰Latexç¼–è¾‘æ–‡æ¡£éœ€æ±‚ï¼Œåœ¨æ­¤è®°å½•VS Codeç¯å¢ƒå®ç°ç›´æ¥ç¼–è¯‘æºç å¹¶å±•ç¤ºPDFæ–‡æ¡£çš„ä¸»è¦æ­¥éª¤ï¼š å®‰è£…Texlive å®‰è£…VS Codeæ’ä»¶Latex Workshop ä¿®æ”¹settings.json è‹¥é€‰æ‹©å¤–éƒ¨PDFè½¯ä»¶å‘ˆç°å·¥å…·ï¼Œä¿®æ”¹å‚è€ƒæ­¤ç½‘é¡µã€‚æ­¤å¤„é€‰æ‹©VS Codeè‡ªå¸¦é¢„è§ˆè¯»å–PDFï¼ŒCtrl+Shift+Pè¿›å…¥Command Paletteï¼Œé”®å…¥settins.jsonï¼Œé€‰æ‹©ç¬¬äºŒé¡¹ï¼Œæ’å…¥å¦‚ä¸‹ä»£ç ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768\"latex-workshop.latex.tools\": [ { \"name\": \"pdflatex\", \"command\": \"pdflatex\", \"args\": [ \"-synctex=1\", \"-interaction=nonstopmode\", \"-file-line-error\", \"%DOC%\" ] }, { \"name\": \"xelatex\", \"command\": \"xelatex\", \"args\": [ \"-synctex=1\", \"-interaction=nonstopmode\", \"-file-line-error\", \"%DOC%\" ] }, { \"name\": \"bibtex\", \"command\": \"bibtex\", \"args\": [ \"%DOCFILE%\" ] } ], \"latex-workshop.latex.recipes\": [ { \"name\": \"pdflatex\", \"tools\": [ \"pdflatex\" ] }, { \"name\": \"xelatex\", \"tools\": [ \"xelatex\" ] }, { \"name\": \"xe-&gt;bib-&gt;xe-&gt;xe\", \"tools\": [ \"xelatex\", \"bibtex\", \"xelatex\", \"xelatex\" ] }, { \"name\": \"pdflatex -&gt; bibtex -&gt; pdflatex*2\", \"tools\": [ \"pdflatex\", \"bibtex\", \"pdflatex\", \"pdflatex\" ] } ], \"latex-workshop.view.pdf.viewer\": \"tab\", \"latex-workshop.latex.autoClean.run\": \"onBuilt\", \"security.workspace.trust.untrustedFiles\": \"open\" å½“ç„¶ï¼Œåœ¨çº¿ä½¿ç”¨Overleafæ›´ä¸ºä¾¿æ·ã€‚å¥½ä¸å®¹æ˜“é…ç½®å¥½ï¼Œæ‰æƒ³åˆ°â”—( Tï¹T )â”›ã€‚ 4. Hexoåšå®¢å¤šç»ˆç«¯ç¼–è¾‘Hexoæ˜¯ç®€å•ã€ä¾¿æ·çš„åŸºäºGithub Pagesçš„åšå®¢æ¡†æ¶ã€‚æœ¬åšå®¢å³åŸºäºæ­¤æ„å»ºå»ºç«‹ã€‚æ­¤å¤„æˆ‘è®¾æƒ³åœ¨å¤šå°ç»ˆç«¯å‡å¯è¿›è¡Œåšå®¢å†™ä½œå’Œå‘å¸ƒã€‚ç”±äºgithubç«¯æ–‡ä»¶åªåŒ…æ‹¬ç¼–è¯‘åç»“æœï¼Œæ— åŸå§‹æ–‡ç« ä¿¡æ¯ï¼Œå› æ­¤æ— æ³•ç›´æ¥åœ¨æ–°PCä¸Šcç›´æ¥cloneå¹¶ä½¿ç”¨ã€‚ é¦–å…ˆï¼Œåœ¨WSLç¯å¢ƒä¸‹å®‰è£…Hexoæ¡†æ¶åŠNode.jsï¼Œå…·ä½“æŒ‡ä»¤å¦‚ä¸‹ï¼š 12345678910111213141516## 1. å®‰è£…Node.jscurl -sL https://deb.nodesource.com/setup14.x | sudo -E bash -sudo apt-get install -y nodejs## æŠ¥é”™ \"not foundram Files/nodejs/npm: 3: /mnt/c/Program Files/nodejs/npm:\"## å‚è€ƒ https://stackoverflow.com/questions/45853530/npm-install-error-not-foundram-files-nodejs-npm-3-mnt-c-program-files-nodejs## åœ¨~/.zshrcä¸­æ·»åŠ ä»¥ä¸‹è¡Œï¼Œå¹¶æ·»åŠ sourcePATH=\"$HOME/bin:$HOME/.local/bin:/usr/bin:$PATH\"## 2. å®‰è£…Node.jsæ¡†æ¶sudo npm install hexo-cli -g### å»ºç«‹ä¸€ä¸ªæ–°æ–‡ä»¶å¤¹blogï¼Œå­˜æ”¾åšå®¢æ–‡ä»¶sudo npm installnpm install -S highlight.js --savenpm install -S cheerio --savenpm install -g hexo-renderer-sass --savenpm install hexo-deployer-git --save è¿›ä¸€æ­¥è®¾ç½®ï¼Œæœ‰ä¸¤ç±»æ€è·¯ï¼š ç›´æ¥è¿ç§»åŸæœ‰è®¡ç®—æœºä¸­hexoæ–‡ç« å†…å®¹ï¼Œä¸»é¢˜è®¾ç½®å’Œé…ç½®æ–‡ä»¶ï¼Œå…·ä½“åŒ…æ‹¬_config.yml,themeã€sourceï¼› åœ¨æ–°PCä¸­ç›´æ¥åˆå§‹åŒ–hexoæ¡†æ¶ï¼Œå‚è€ƒåŸæœ‰*.ymlæ–‡ä»¶é‡æ–°è®¾ç½®ã€‚ ç»æœ¬äººæµ‹è¯•ï¼Œæ–¹æ³•1ï¼‰åœ¨hexo cleané˜¶æ®µåå¤æŠ¥é”™ï¼Œéš¾ä»¥è§£å†³ã€‚ä¸ªäººè®¤ä¸ºå¯èƒ½åŸæœ‰ç¼–è¯‘æ–‡ä»¶å’Œç°åœ¨çš„hexoç‰ˆæœ¬ä¸ç¬¦ã€‚å› æ­¤ï¼Œæˆ‘åˆå§‹åŒ–äº†logæ–‡ä»¶å¤¹ï¼Œç§»å…¥åšå®¢å†å²æ–‡ç« ï¼Œä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œå…·ä½“å¦‚ä¸‹ï¼š 123456789hexo init## è½¬ç§»æ–‡ç« cp ./../hexo_ori/source/_post/* ./source/_post/.## å®‰è£…ä¸»é¢˜git clone https://github.com/ppoffice/hexo-theme-minos.git themes/minos## æŒ‰ç…§åŸè®¾ç½®ï¼Œä¿®æ”¹ä¸»æ–‡ä»¶å¤¹ä¸‹_config.ymlæ–‡ä»¶å’Œthemeæ–‡ä»¶å¤¹ä¸‹_config.ymlæ–‡ä»¶hexo ghexo server æœ¬åœ°ç¼–è¯‘æµ‹è¯•æ— è¯¯åï¼Œè®¾ç½®SShå¯†é’¥ï¼Œå…³è”githubä¸ªäººè´¦æˆ·ï¼Œå…·ä½“å¦‚ä¸‹ï¼š 123456789101112131415161718## 1. ç”Ÿæˆsshssh-keygen -t rsa -b 4096 -C \"your_github_email@example.com\"## 2. å¤åˆ¶/home/your_name/.ssh/id_rsa.pubå…¨éƒ¨å†…å®¹ï¼Œæ·»åŠ è‡³githubè´¦æˆ·ä¸­çš„SSH keysä¸­## 3. æ£€æµ‹æˆåŠŸè®¾ç½®ssh -T git@github.com## 4. gitåˆå§‹åŒ–git config --global user.email \"you@example.com\"git config --global user.name \"Your Name\"## 5. é…ç½®ä¸ªäººåŸŸåï¼Œæ–°å»ºCNAMEæ–‡ä»¶ï¼Œæ·»åŠ ä¸ªäººåŸŸåvi ./source/CNAME ## 6. ç§»å…¥æœ¬ç¯‡æ–‡ç« ï¼Œå¼€å§‹éƒ¨ç½²hexo g hexo d ä¸ºå®ç°Hexoåšå®¢å¯åœ¨å¤šè®¾å¤‡åŒæ—¶æ’°å†™å‘å¸ƒï¼ŒæŸ¥è¯¢ç½‘ç»œèµ„æ–™å¯»æ‰¾å¯è¡Œæ–¹æ³•ã€‚å¤šæ•°æ•™ç¨‹å»ºè®®é‡‡ç”¨æ•™ç¨‹æä¾›äº†hexoæºæ–‡ä»¶æ–°å»ºgithubåˆ†æ”¯çš„åšæ³•ã€‚ä½†è¯¥æ–¹æ³•å…¬å¼€äº†sshå¯†é’¥ä¿¡æ¯ã€‚æ­¤å¤„ï¼Œæˆ‘ä¸“è®¾æ–‡ä»¶å¤¹æ’°å†™.mdæ ¼å¼æ–‡ä»¶ï¼Œç½®äºDropboxæ–‡ä»¶å¤¹å†…ï¼Œä¸åŒç»ˆç«¯é—´å¯å…±äº«ã€‚æ›´æ–°åšå®¢æ—¶ï¼Œåªéœ€å°†å…¶æ–°å†™å…¥çš„æ–‡ç« æ‹–å…¥hexoçš„ç¼–è¯‘æ–‡ä»¶å¤¹å†…æ›´æ–°éƒ¨ç½²ã€‚ æœªæ¥ä¼šç»§ç»­æ•´ç†åœ¨WSLç¯å¢ƒä¸‹å®‰è£…ï¼Œè°ƒç”¨å­¦ç§‘ä¸“ä¸šè½¯ä»¶å†…å®¹ã€‚ PS. 2025.08.03ä¸€ä¸å°å¿ƒå°† Hexo å‡çº§åˆ°äº† 7.0+ ç‰ˆæœ¬ï¼Œç»“æœå‘ç°è‡ªå·±é’Ÿçˆ±çš„ Minos ä¸»é¢˜å‡ºç°äº†è¿è¡ŒæŠ¥é”™ï¼ˆæ¯•ç«Ÿè¯¥ä¸»é¢˜å¤šå¹´æœªæ›´æ–°ï¼‰ã€‚ ä¸»è¦é”™è¯¯ä¸ºï¼š 12ERROR Script load failed: themes/minos/scripts/01_check.jsTypeError: require(...) is not a function è¿™äº›é”™è¯¯ä¸»è¦å‡ºç°åœ¨ Minos çš„è‡ªå®šä¹‰ JS æ–‡ä»¶ä¸­ï¼Œç”±äºæœ¬äººå¹¶ä¸ç†Ÿæ‚‰ JavaScriptï¼Œå› æ­¤æš‚æ—¶æ²¡æœ‰æ·±å…¥ä¿®æ”¹ã€‚åŸæœ¬æ‰“ç®—æ›´æ¢å…¶ä»–ä¸»é¢˜ï¼Œä½†è¿˜æ˜¯â€œä¸æ€è¿›å–â€ï¼Œè§‰å¾— Minos çš„æ’ç‰ˆé£æ ¼å°±æ˜¯é¡ºçœ¼ ğŸ˜…ã€‚ æœç´¢ç½‘é¡µè®ºå›ä¿¡æ¯ï¼Œé”™è¯¯å¯èƒ½æ˜¯ç”± Hexo 7.x ä¸æ—§ä¸»é¢˜/æ’ä»¶ä¸å…¼å®¹ å¯¼è‡´çš„ï¼Œæ¯”å¦‚ require(â€¦) is not a function è¿™ç±» JS æ¨¡å—è°ƒç”¨æ–¹å¼çš„æ”¹å˜ã€‚å› è€Œå­¦ä¹ å¦‚ä½•å›é€€ã€‚ 12345678910111213141516# åˆå§‹åŒ– Hexo é¡¹ç›®hexo init blogcd blog# é™çº§ Hexo åˆ° 6.1.0npm install hexo@6.1.0# å®‰è£…å…¼å®¹çš„ Sass æ¸²æŸ“å™¨npm uninstall hexo-renderer-sass sassnpm install --save hexo-renderer-sass sass# æ¸…ç† + ç¼–è¯‘ + æœ¬åœ°æœåŠ¡hexo cleanhexo ghexo server -p 5001 è¿è¡Œæ—¶å†æ¬¡å‡ºç°å¦ä¸€ä¸ªé—®é¢˜ 1Error: Unknown output style \"nested\" è¿™æ˜¯å› ä¸ºblog/_config.ymlä¸­è®¾ç½®äº†ï¼š 12node_sass: outputStyle: nested å°†nestedæ”¹ä¸ºexpandedä»¥åæŠ¥é”™å–æ¶ˆï¼Œä½†warningé˜´é­‚ä¸æ•£ï¼š 1Deprecation Warning [import]: Sass @import rules are deprecated... è¿™æ˜¯å› ä¸ºè®¸å¤šè€ä¸»é¢˜ä»ä½¿ç”¨äº† @importï¼Œè€Œéæ–°æ ‡å‡†ä¸­çš„ @useã€‚ç›®å‰ä¸å½±å“ä½¿ç”¨ï¼Œåç»­å¦‚æœ‰ç²¾åŠ›æˆ‘ä¹Ÿä¸æƒ³èŠ±åœ¨è¿™ä¸Šé¢ğŸ˜…ã€‚ åˆå‡ºç°ä¸€ä¸ªé—®é¢˜: 123FATAL Something's wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.htmlError: EACCES: permission denied, rmdir '/mnt/d/Dropbox/code/0.Work/blog/public/images' at async Object.rmdir (node:internal/fs/promises:824:10) é€šè¿‡é‡æ–°å®‰è£…sudo npm install -g react-native-cliè§£å†³ã€‚ éƒ¨åˆ†å‚è€ƒèµ„æ–™ Running Jupyter Notebook on WSL while Using Browser on Windows WSL2ä¸‹ç”¨Hexoåœ¨githubä¸Šæ­å»ºé™æ€åšå®¢ Â· Joe Suen's Blog","link":"/2021/07/14/24.Windows%E7%94%B5%E8%84%91%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/"},{"title":"WRFå®‰è£…æœ­è®°","text":"å®éªŒå®¤çš„æœåŠ¡å™¨çš„å››å—ç¡¬ç›˜å…¨åï¼Œè€Œæœªè¿›è¡Œå¤‡ä»½ï¼Œä¸å¾—ä¸å°†ä»¥å‰çš„å†…å®¹å…¨éƒ¨é‡è£…ä¸€æ¬¡ã€‚Pythonéƒ¨åˆ†æœ‰Anacondaçš„åˆ†å‘åŒ…ï¼Œæ¯”è¾ƒå®¹æ˜“ã€‚ä½†å¸¸ç”¨çš„WRFæ¨¡å‹å´ç ”ç©¶äº†å¥½ä¹…ã€‚ ä¹‹å‰ï¼ŒæœåŠ¡å™¨é‡‡ç”¨çš„æ˜¯ifortç¼–è¯‘å™¨ã€‚é‡è£…åï¼Œç³»ç»Ÿä¸­åªæœ‰gcc, ç‰ˆæœ¬ä¹Ÿæ¯”è¾ƒæ—§(4.1.2)ã€‚ WRFç¼–è¯‘å¤±è´¥ï¼Œæˆ‘è®¤ä¸ºåŸå› å¯èƒ½æ˜¯gccå¤ªæ—§äº†ã€‚ å› è€Œï¼Œæˆ‘è€ƒè™‘å‡çº§gccçš„ç‰ˆæœ¬ï¼ŒåŒæ—¶ç”±äºæœ¬äººä¸æ˜¯rootç”¨æˆ·ï¼Œä¹Ÿä¸èƒ½å€ŸåŠ©yum install gccçš„ç®€å•åŠæ³•ï¼Œåªå¾—step by stepåœ°åšã€‚ æœ¬æ–‡è®°å½•äº†è‡ªgccå‡çº§è‡³WRF-ChemæˆåŠŸå®‰è£…çš„å…¨éƒ¨æµç¨‹ï¼Œä½œä¸ºä»Šåç±»ä¼¼å·¥ä½œçš„å‚è€ƒã€‚ GCCçš„å®‰è£…éµå¾ªGCCå®˜ç½‘ä¸­çš„ä»‹ç»ï¼Œç”±äºGCCçš„å®‰è£…æœ‰èµ–äºGMP, MPFR, MPCç­‰åº“ï¼Œè€Œè‡ªå·±ä»¥å‰æ˜¯é€ä¸ªç¼–è¯‘ã€‚å®˜ç½‘å¹¶ä¸æ¨èè¿™ç§æ–¹æ³•,è€Œæ¨èé‡‡ç”¨å¦‚ä¸‹æ‰€ç¤ºçš„æ–¹æ¡ˆã€‚ GCCæ–‡ä»¶å¤¹å†…å«æœ‰download_prerequisitesçš„ä¿¡æ¯ï¼Œå¯è‡ªè¡Œç›´æ¥ä¸‹è½½æœ‰å…³ä¾èµ–èµ„æ–™åº“ï¼Œé¿å…è‡ªå·±é€‰æ‹©åº“æ–‡ä»¶æ—¶çš„ç‰ˆæœ¬ä¸ä¸€è‡´ã€‚GCCå®˜æ–¹ç½‘å€ 123456789101112#ä¸‹è½½æŸä¸ªç‰ˆæœ¬çš„gcc(å°½é‡é€‰æ‹©è¾ƒæ–°çš„ç‰ˆæœ¬)åtar xzf gcc-4.6.2.tar.gzcd gcc-4.6.2./contrib/download_prerequisites ## æŒ‰ç…§ä¾èµ–èµ„æ–™åº“cd ..mkdir objdir ## ç¼–è¯‘ä¿¡æ¯å‚¨å­˜çš„æ–‡ä»¶å¤¹è®¾ä¸ºobjdircd objdir$./../gcc-4.6.2/configure --prefix=$HOME/lib/gcc-4.6.2 \\ --enable-languages=c,c++,fortran,go ## $HOME/gcc-4.6.2å³ä¸ºå®‰è£…ä½ç½®makemake install cd .. gccçš„ç¼–è¯‘è¿‡ç¨‹æ¯”è¾ƒæ…¢ï¼Œéœ€è¦ç­‰å¾…ä¸€ä¼šå„¿ å®‰è£…å®Œæ¯•åï¼Œæ‰“å¼€~/.bashrc, åœ¨å…¶ä¸­æ·»åŠ ä¸€è¡Œ: export LD_LIBRARY_PATH=$HOME/gcc-4.9.4/lib64 åˆ©ç”¨gcc -væ ¸æŸ¥ç‰ˆæœ¬ï¼ŒæŸ¥è¯¢æ˜¯å¦å®‰è£…æˆåŠŸ WRFä¸WPSæœ‰å…³ä¾èµ–åº“çš„å®‰è£…ä¸»è¦ä¾èµ–çš„èµ„æ–™åº“åŒ…æ‹¬mpich, netcdf, Jasper,libpngä»¥åŠzlibã€‚zlibå’Œlibpngé—´æœ‰å…ˆåå…³ç³»ï¼Œéœ€å…ˆç¼–è¯‘zlibã€‚ NETCDFå®‰è£…æ­¤å¤„å®‰è£…çš„æ˜¯ç”¨ä»¥æ”¯æŒNetCDFæ ¼å¼æ–‡ä»¶åˆ›å»ºã€è®¿é—®å’Œå…±äº«çš„å‡½å¼åº“ã€‚å®‰è£…æ­¥éª¤å¦‚ä¸‹: 12345678910# åœ¨~/.bashrcä¸­æ·»åŠ ä»¥ä¸‹é€è¡Œexport DIR=$HOME/libexport PATH=\"$DIR/netcdf/bin:$PATH\"export NETCDF=\"$DIR/netcdf\"export CC=gccexport CXX=g++export FC=gfortranexport FCFLAGS=-m64export F77=gfortranexport FFLAGS=-m64 ä¸‹è½½NETCDFåº“æ–‡ä»¶ï¼Œè¿›è¡Œè§£å‹ä¸ç¼–è¯‘: 123456789tar xzvf netcdf-4.1.3.tar.gz #or just .tar if no .gz presentcd netcdf-4.1.3./configure --prefix=$DIR/netcdf --disable-dap \\ --disable-netcdf-4 --disable-sharedmakemake installsetenv PATH $DIR/netcdf/bin:$PATHsetenv NETCDF $DIR/netcdfcd .. è‹¥å®‰è£…æˆåŠŸï¼Œterminalä¼šå‡ºç°â€œCongratulations, xxxâ€œä¹‹ç±»çš„è¾“å‡ºã€‚ MPICHå®‰è£…MPICHç”¨ä»¥æ”¯æŒWRFç­‰æ•°å€¼æ¨¡å¼çš„å¹¶è¡Œè®¡ç®—ï¼Œå®‰è£…æ­¥éª¤å¦‚ä¸‹: 12345678tar xzvf mpich-3.0.4.tar.gz cd mpich-3.0.4./configure --prefix=$DIR/mpichmakemake installvim ~/.bashrcexport PATH=\"$DIR/mpich/bin:$PATH\"cd .. zlib, libpng, Jasperå®‰è£…zlibæ˜¯WPSç¨‹åºç”¨ä»¥æå–gribæ ¼å¼æ•°æ®çš„å¿…éœ€å‡½å¼åº“ï¼Œå®‰è£…æ­¥éª¤å¦‚ä¸‹: 123456789vim ~/.bashrcexport LDFLAGS=-L/diks2/hyf/lib/grib2/libexport CPPFLAGS=-I/disk2/hyf/lib/grib2/includetar xzvf zlib-1.2.7.tar.gz cd zlib-1.2.7./configure --prefix=$DIR/grib2makemake installcd .. libpngåŒæ ·æ˜¯WPSç¨‹åºç”¨ä»¥æå–gribæ ¼å¼æ•°æ®çš„å¿…éœ€å‡½å¼åº“ï¼Œå®‰è£…æ­¥éª¤å¦‚ä¸‹: 123456tar xzvf libpng-1.2.50.tar.gz cd libpng-1.2.50./configure --prefix=$DIR/grib2makemake installcd .. JasperåŒæ ·æ˜¯WPSç¨‹åºç”¨ä»¥æå–gribæ ¼å¼æ•°æ®çš„å¿…éœ€å‡½å¼åº“ï¼Œå®‰è£…æ­¥éª¤å¦‚ä¸‹: 123456tar xzvf jasper-1.900.1.tar.gzcd jasper-1.900./configure --prefix=$DIR/grib2makemake installcd .. ä¸‰è€…å‡å®‰è£…åœ¨grib2è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ã€‚ WRFå’ŒWPSçš„å®‰è£…è¯·æ³¨æ„, WRFåŠå…¶å‰å¤„ç†WPSå·¥å…·åº”å®‰è£…äºåŒä¸€æ–‡ä»¶å¤¹ä¸‹. WRFçš„å®‰è£…12345## è§£å‹WRFV3.8gunzip WRFV3.8.TAR.gztar -xf WRFV3.8.TARcd WRFV3mv ../chem . ## å°†é¢„å…ˆä¸‹è½½å¥½çš„wrf-chemè§£å‹æ–‡ä»¶æ”¾åˆ°è¯¥æ–‡ä»¶å¤¹ä¸‹ å¼€å§‹ç¼–è¯‘,./configure.å¯¹ç¼–è¯‘å™¨å’Œå¹¶è¡Œæ–¹å¼è¿›è¡Œé€‰æ‹©ï¼Œæ­¤å¤„æˆ‘é‡‡ç”¨çš„æ˜¯gfortranç¼–è¯‘å™¨ä»¥åŠdmprå¹¶è¡Œæ–¹å¼ã€‚ dmpræŒ‡çš„æ˜¯åˆ†å¸ƒå¼å¹¶è¡Œæ–¹å¼ï¼Œä¸smprå…±äº«å†…å­˜å‹å¹¶æ€§æ–¹å¼æœ‰æ‰€åŒºåˆ«ã€‚ä¸€èˆ¬å°†è¿™ä¸¤ç±»å¹¶è¡Œæ–¹å¼ç§°ä¸ºOpenMP(OMP)å’ŒMessage Passing Interface(MPI)ã€‚_ é”®å…¥:./compile em_real &gt;&amp; log.compileè¿›è¡Œç¼–è¯‘ï¼Œç­‰å¾…â€¦â€¦ é”®å…¥:./compile convert_emiss&gt;&amp; log.compileè¿›è¡Œç¼–è¯‘ï¼Œç­‰å¾…â€¦â€¦ å®Œæˆå, åˆ©ç”¨ls ./main/*.exeè¿›è¡Œæ£€æŸ¥ï¼Œè‹¥å‡ºç°äº”ä¸ª .exe æ ¼å¼çš„å¯æ‰§è¡Œæ–‡ä»¶ï¼Œæ‰“å¼€chemæ–‡ä»¶å¤¹ï¼Œå‡ºç°convert_emiss.exe, è¯´æ˜å®‰è£…æˆåŠŸã€‚ WPSçš„å®‰è£…WPSæ˜¯ç”¨ä»¥æ•´åˆé™æ€åœ°ç†æ•°æ®å’Œæ°”è±¡åœºåˆå§‹æ¡ä»¶ã€è¾¹ç•Œæ¡ä»¶èµ„æ–™çš„å·¥å…·ï¼Œå…¶å®‰è£…è¾ƒå¿«ã€‚ 123456789# è§£å‹æ–‡ä»¶gunzip WPSV3.8.TAR.gztar -xf WPSV3.8.TARcd WPSvim ~/.bashrcexport JASPERLIB=$DIR/grib2/libexport JASPERINC=$DIR/grib2/include./configure./compile &gt;&amp; log.compile è‹¥å®‰è£…æˆåŠŸåï¼Œä¼šå‡ºç°geogrid.exe, ungrib.exe, metgrid.exeä¸‰ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ã€‚ å‚è€ƒèµ„æ–™1. GCC install 2. Compiling WRF 3. Install WRF on Ubuntu Server","link":"/2016/08/10/3.%5BModel%5DWRF%E5%AE%89%E8%A3%85%E6%9C%AD%E8%AE%B0/"},{"title":"2 Easy Steps to Download Historical Weather Data","text":"The National Oceanic and Atmospheric Administration (NOAA) Integrated Surface Database (ISD) provides one of the richest sources of historical weather data consisting of hourly and synpoptic observation. This blog will introuce the simple way to retrieve and process the raw data into Python dataframe. Selection of weather stationBased on the location of the city/sampling site, we could seach for the best/most close weather station for getting its ID. Here, Milan, Italy will be treated as the example, and the main procedures are listed as follows. The detailed information of weathe station globally can be downloaded here link. 1234567891011import pandas as pdisd_st = pd.read_csv(\"/mnt/d/Dropbox/data/geo/Meteo/NOAA/global/isd-history.csv\")## filtering based on country nameisd_st[isd_st['CTRY'] =='IT']## filtering based on city nameisd_st[isd_st[\"STATION NAME\"].astype(str).str.contains(\"MILAN\")]## Then, we could choose the perferred monitoring station, and note its \"USAF\" and \"WBAN\"## For Milan, USAF == 160800, WBAN == 99999, then the target ID will be \"160800-99999\"station_ID = 160800-99999 A more visualized way could be achieved through this link Find a station. FTP downloading and processing1234567891011121314151617181920212223242526272829303132333435363738from ftplib import FTPfrom pathlib import Pathftp = FTP(\"ftp.ncdc.noaa.gov\")ftp.login()## there are multiple dataset within NOAA ISD. Here we choose NOAA ISD-Lite, which is hourly-recoreded.ftp.cwd('/pub/data/noaa/isd-lite/2013/')# Get all filesfiles = ftp.nlst()# Download the file matched with the station IDfor file in files: if file == station_ID +'.gz':# print (file) print(\"Downloading...\" + file) ftp.retrbinary(f'RETR {file}', open(str(Path(r'./isd_data') / file), 'wb').write)# # ftp.close()def calc_rh(T,TD): RH = 100*(np.exp((17.625*TD)/(243.04+TD))/np.exp((17.625*T)/(243.04+T))) return RHdef read_year_meteo(filepath): df= pd.read_csv(filepath,sep='\\s+',header = None) #NOTE: Trace precipitation is coded as -1 # SCALING FACTOR: 10 df.columns = ['Year','Month','Day','Hour','Air temp','Dew point','Pressure','WD','WS','Cloud','1h prep','6h prep'] df['Date'] = df['Year'].astype(str)+'-'+df[\"Month\"].apply(\"{0:0=2d}\".format)+'-'+df[\"Day\"].apply(\"{0:0=2d}\".format)+' '+df[\"Hour\"].apply(\"{0:0=2d}\".format)+':00:00' df = df[['Date','Air temp','Dew point','Pressure','WD','WS','Cloud','1h prep','6h prep'] ] df = df.replace(-9999,np.nan) for t in ['Air temp','Dew point','Pressure','WS','1h prep','6h prep']: df[t] = df[t]/10.0 df['RH'] = calc_rh(df['Air temp'],df['Dew point']) return dfmilan_2013 = read_year_meteo('./isd_data/160800-99999-2013') milan_2013.to_csv(\"./milan_weather_data_hourly.csv\", index = False) Done! Reference How We Process the NOAA Integrated Surface Database Historical Weather Data Preparing the NOAA ISD, Hourly, Global Dataset for Time-series Databases Getting Weather Data in 3 Easy Steps","link":"/2022/07/10/26.%5BDataset%5D-Easy-Steps-For-NOAA-Weather-Data/"},{"title":"Plotting FLEXPART trajectories in 3D terrain map","text":"This blog post demonstrates how to plot FLEXPART model trajectories on a 3D terrain map, integrating topography and transport data for enhanced visualization. The workflow includes: Loading terrain data Plotting air mass trajectories in 3D Creating illustrative particle dispersion effects 1. Load and Visualize Terrain Data123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dimport cartopy.crs as ccrsimport cartopy.feature as cfeaturefrom cartopy.io import shapereaderfrom netCDF4 import Dataset# load the terrain fileetopo_file = \"./../../../../../4.Punjab_fire/Analysis_notebook/ETOPO_2022_v1_60s_N90W180_bed.nc\" # Replace with your local ETOPO1 filenc = Dataset(etopo_file)# Extract data from ETOPO1lon = nc.variables['lon'][:]lat = nc.variables['lat'][:]elevation = nc.variables['z'][:]# Mask the ocean by setting all elevation &lt;= 0 (sea level or below) to NaNelevation_masked = np.where(elevation &lt;= 0, np.nan, elevation)# Extract the topo data to the area of interestlon_min, lon_max = 60, 89lat_min, lat_max = 22, 36lon_indices = np.where((lon &gt;= lon_min) &amp; (lon &lt;= lon_max))[0]lat_indices = np.where((lat &gt;= lat_min) &amp; (lat &lt;= lat_max))[0]subset_lon = lon[lon_indices]subset_lat = lat[lat_indices]subset_elevation = elevation_masked[np.ix_(lat_indices, lon_indices)]lon_grid, lat_grid = np.meshgrid(subset_lon,subset_lat)# Normalize elevation data for better 3D visualizationelevation_normalized = np.clip(subset_elevation/ 1000, 0, subset_elevation.max()/1000.0) # Convert to kmfrom matplotlib.colors import LinearSegmentedColormapimport shapefilefrom shapely.geometry import Polygonfrom shapely.geometry import mappingfrom mpl_toolkits.mplot3d.art3d import Line3DCollectionfrom scipy.interpolate import RegularGridInterpolatordef truncate_colormap(cmap, minval=0.2, maxval=1.0, n=100): new_cmap = LinearSegmentedColormap.from_list( f\"trunc({cmap.name},{minval:.2f},{maxval:.2f})\", cmap(np.linspace(minval, maxval, n)), ) return new_cmap# Create a truncated terrain colormap starting from greenterrain_truncated = truncate_colormap(plt.cm.terrain, minval=0.3, maxval=1.0)from matplotlib.colors import LinearSegmentedColormapimport shapefilefrom shapely.geometry import Polygonfrom shapely.geometry import mappingfrom mpl_toolkits.mplot3d.art3d import Line3DCollectionfrom scipy.interpolate import RegularGridInterpolatordef truncate_colormap(cmap, minval=0.2, maxval=1.0, n=100): new_cmap = LinearSegmentedColormap.from_list( f\"trunc({cmap.name},{minval:.2f},{maxval:.2f})\", cmap(np.linspace(minval, maxval, n)), ) return new_cmap# Create a truncated terrain colormap starting from greenterrain_truncated = truncate_colormap(plt.cm.terrain, minval=0.3, maxval=1.0) 2. Load and Plot FLEXPART TrajectoriesIn trajectory mode, the output txt format information can be seen herehttps://confluence.ecmwf.int/display/METV/FLEXPART+output Column Number Name Unit Description 1 time s The elapsed time in seconds since the middle point of the release interval 2 meanLon degrees Mean longitude position for all the particles 3 meanLat degrees Mean latitude position for all the particles 4 meanZ m Mean height for all the particles (above sea level) 5 meanTopo m Mean topography underlying all the particles 6 meanPBL m Mean PBL (Planetary Boundary Layer) height for all the particles (above ground level) 7 meanTropo m Mean tropopause height at the positions of particles (above sea level) 8 meanPv PVU Mean potential vorticity for all the particles 9 rmsHBefore km Total horizontal RMS (root mean square) distance before clustering 10 rmsHAfter km Total horizontal RMS distance after clustering 11 rmsVBefore m Total vertical RMS distance before clustering 12 rmsVAfter m Total vertical RMS distance after clustering 13 pblFract % Fraction of particles in the PBL 14 pv2Fract % Fraction of particles with PV &lt; 2 PVU 15 tropoFract % Fraction of particles within the troposphere 16* clLon_N degrees Mean longitude position for all the particles in cluster N 17* clLat_N degrees Mean latitude position for all the particles in cluster N 18* clZ_N m Mean height for all the particles in cluster N (above sea level) 19* clFract_N % Fraction of particles in cluster N (above sea level) 20* clRms_N km Total horizontal RMS distance in cluster N 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122input_file = \"./data/retroplume_raw/traj/clustered_ind/2018-03-30-example.txt\"with open(input_file, \"r\") as file: lines = file.readlines()[7:]data = [list(map(float, line.split())) for line in lines]df_traj = pd.DataFrame(data, columns=columns[:len(data[0])]) # Adjust column count based on the actual data length# 36 hr in totalkp_traj = df_traj[df_traj['receptor'] == 1.0].reset_index(drop = True)dl_traj = df_traj[df_traj['receptor'] == 2.0].reset_index(drop = True)trajectories = { \"KP\": { \"lon\": kp_traj['meanLon'], \"lat\": kp_traj['meanLat'], \"alt\": kp_traj['meanZ']/1000.0 + kp_traj['meanTopo']/1000.0, \"time_delta\": -1*(kp_traj['time'] - kp_traj['time'].iloc[0])/60/60.0, }, \"DL\": { \"lon\": dl_traj['meanLon'], \"lat\": dl_traj['meanLat'], \"alt\": dl_traj['meanZ']/1000.0 + dl_traj['meanTopo']/1000.0, 'time_delta':-1*(kp_traj['time'] - kp_traj['time'].iloc[0])/60/60.0 },}elevation_interpolator = RegularGridInterpolator( (lat, lon), elevation / 1000.0 # Convert elevation to km)def set_3d_background(ax,sel_df_bio): ax.scatter( 80.331871, 26.449923, 0.35, # Kanpur at ground level zdir=\"z\", marker=\"s\", facecolor='none', edgecolor=\"b\", s=40, alpha=1,lw = 2, label=\"Kanpur\",zorder = 3 ) ax.scatter( 77.069710, 28.679079, 0.53, # New Delhi at ground level zdir=\"z\", marker=\"s\", facecolor='none', edgecolor=\"r\", s=40, alpha=1,lw = 2, label=\"New Delhi\",zorder = 3 ) # Plot the real-world elevation data as the base map ax.plot_surface( lon_grid, lat_grid, subset_elevation/1000.0, cmap=plt.cm.gray,#terrain_truncated, rstride=10, cstride=10, edgecolor='none', alpha=0.15, ) # Customize grid appearance ax.xaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0) # Transparent X grid ax.yaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0) # Transparent Y grid ax.zaxis._axinfo[\"grid\"][\"color\"] = (1, 1, 1, 0) # Transparent Z grid ax.set_xlim([lon_min,lon_max]) ax.set_ylim([lat_min, lat_max]) ax.set_xlabel(\"Longitude\") ax.set_ylabel(\"Latitude\") ax.set_zlabel(\"Altitude (km)\")def plot_individual_day_traj(ax,traj):# colors = ['#97ff80','#679b4e','#ffe64c','#cd4040','purple','k']# time_categories = [0, 12,24, 48, 72, 96, 120] # 5 days (0-24, 24-48, etc.)# for i in range(len(time_categories) - 1):# mask = (traj[\"time_delta\"] &gt;= time_categories[i]) &amp; (traj[\"time_delta\"] &lt; time_categories[i + 1])# ax.plot(# np.array(traj[\"lon\"])[mask],# np.array(traj[\"lat\"])[mask],# np.array(traj[\"alt\"])[mask],# color=colors[i],# linewidth=3.5,# label=f\"&lt;{time_categories[i+1]} hr\",# ) norm = mcolors.Normalize(vmin=0, vmax=72) # Assuming time_delta ranges from 0 to 48 hours cmap = cm.get_cmap('Spectral_r') # Choose a continuous colormap # Plot trajectories with continuous color representation for i in range(len(traj[\"time_delta\"]) - 1): ax.plot( traj[\"lon\"][i:i+2], traj[\"lat\"][i:i+2], traj[\"alt\"][i:i+2], color=cmap(norm(traj[\"time_delta\"][i])), # Map time_delta to color linewidth=2.5,zorder =15, )# Create a 3D plotfig = plt.figure(figsize=(6, 5))norm = mcolors.Normalize(vmin=0, vmax=72) # Assuming time_delta ranges from 0 to 48 hourscmap = cm.get_cmap('Spectral_r') # Choose a continuous colormapax = fig.add_subplot(111, projection='3d')plot_individual_day_traj(ax, trajectories['DL'])set_3d_background_dl(ax,sel_df_bio)ax.set_zlim([0, 7])ax.set_title('Delhi', loc = 'left', fontweight = 'bold',y=0.85)ax.legend(ncol = 3, loc = (0.55, 0.45), fontsize = 8, frameon=True)ax.view_init(elev=21, azim=-110) # plt.show() 3.Particle Dispersion Illustration1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import numpy as npdef plot_individual_day_traj_particles(ax, traj, n_particles=10, spread=0.05,initial_spread=0.05, final_spread=1): \"\"\" Plot trajectory as particles instead of lines by generating points around the trajectory. Parameters: - ax: Matplotlib 3D axis. - traj: Dictionary with trajectory data (lon, lat, alt, time_delta). - n_particles: Number of particles to simulate per trajectory step. - spread: Standard deviation for particle dispersion around each step. \"\"\" norm = mcolors.Normalize(vmin=0, vmax=72) # Normalize time_delta cmap = cm.get_cmap('Spectral_r') # Colormap for time # Loop through each trajectory point for i in range(len(traj[\"time_delta\"])): current_spread = np.interp( traj[\"time_delta\"][i], [0, max(traj[\"time_delta\"])], [initial_spread, final_spread] ) # Generate random offsets for particles lon_offsets = np.random.normal(traj[\"lon\"][i], current_spread, n_particles) lat_offsets = np.random.normal(traj[\"lat\"][i], current_spread, n_particles) alt_offsets = np.random.normal(traj[\"alt\"][i], current_spread / 2, n_particles) # Less spread for altitude # Get color for the current time_delta color = cmap(norm(traj[\"time_delta\"][i])) # Plot the particles ax.scatter( lon_offsets, lat_offsets, alt_offsets, color=color, alpha=0.45, s=2, # Small points for particles )# Create a 3D plotfig = plt.figure(figsize=(6, 5))norm = mcolors.Normalize(vmin=0, vmax=72) # Assuming time_delta ranges from 0 to 48 hourscmap = cm.get_cmap('Spectral_r') # Choose a continuous colormapax = fig.add_subplot(111, projection='3d')plot_individual_day_traj_particles(ax, trajectories[\"DL\"], n_particles=200, spread=0.25)set_3d_background_dl(ax,df_pun)ax.set_zlim([0, 7])ax.set_title('Delhi', loc = 'left', fontweight = 'bold',y=0.85)# ax.legend(ncol = 3, loc = (0.3, 0.7), fontsize = 8, frameon=True)ax.view_init(elev=17, azim=-110) # sm = cm.ScalarMappable(cmap=cmap, norm=norm)axcolor = fig.add_axes([0.275, 0.6, 0.15, 0.015]) # Custom position and size for the colorbar axiscbar = plt.colorbar(sm, cax=axcolor, orientation='horizontal') # Create horizontal colorbarcbar.set_label('Air Mass Transport Time (hr)', labelpad=-40, fontsize=9) plt.savefig(\"./Backward_trajectories_example_20181125_pollution_cases_particle_simulation_version.png\", dpi =400)plt.show()","link":"/2024/10/05/30.FLEXPART-trajectories-in-3D-terrain-map/"},{"title":"The work transformed from shapefile to geojson file","text":"Geojson provides several advantages compared to shapefiles: lightweight, text-based, and easily readable format that can be easily shared, transmitted and used on the web. It supports a wider range of data types compared to shapefile and can be used across multiple platforms and programming languages. Additionally, geojson files have smaller file sizes, making them easier to store and process. These benefits make geojson a popular choice for geographic information data storage and exchange. 1. Python-based GeoJson manipulation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657## 1. Search specific data from raw data and load as Shapely objectfname = './india_state_geo.json'with open(fname) as f: data = json.load(f)for i, c in enumerate(data['features']): if c['properties']['NAME_1'] in ['Haryana', 'Uttar Pradesh', 'Punjab']: print (i,c['properties']['NAME_1'] )hary_shp = shape(data['features'][12]['geometry'])## 2. Mask 2-d array by shapely objectimport timestart = time.time()from warnings import filterwarningsfilterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')xx_, yy_ = np.meshgrid(lon_sa,lat_sa)mask_hary = shapely.vectorized.contains(hary_shp, xx_,yy_)## 3. Merge of multiple selected polygons from shapely.geometry import shape, MultiPolygonimport json### 3.1 Load the GeoJSON data into a Python objectwith open(fname) as f: data = json.load(f)### 3.2 Convert the polygon features into shapely objectspolygons = [shape(feature['geometry']) for feature in data['features']]### 3.3 Merge the polygons using the union methodpoly_index = [3,4,15,19,21,22,23,24,31,34]merged_polygon = polygons[poly_index[0]]for i,polygon in enumerate(polygons): if i in poly_index[1:]: merged_polygon = merged_polygon.union(polygon)### 3.4 Create a MultiPolygon object if necessaryif type(merged_polygon) != MultiPolygon: merged_polygon = MultiPolygon([merged_polygon])## 4. Transforming Shapefile to Geojsonimport fionaimport jsonfname2 = './example.shp'# Open the shapefile using fionawith fiona.open(fname2) as src: # Create a new GeoJSON file with open( 'example.json', \"w\") as output: # Write the GeoJSON representation of the shapefile to the file output.write(json.dumps(list(src)))## 5. Merge A with Bwith open('A.json') as f: A = json.load(f)with open('B.json') as f: B = json.load(f)merged_polygon = A.union(shape(B[0]['geometry'])) 2. Javascript-based Geojson visualizationcontinue","link":"/2023/02/23/28.%5BCode%5DFrom-Shapefile-to-Geojson/"},{"title":"OrbiTrack Dev Log 2: Dynamic M/Z Calibration","text":"After reading m/z data from Orbitrap raw files, it is essential to perform m/z calibration based on known reference masses of internal calibrants. For long-term measurements, especially those spanning days or weeks, dynamic calibration is criticalâ€”m/z drift can occur over time, and calibration parameters from Day 1 may not hold by Day 5. The function below allows automated chunk-wise recalibration across retention time. Dynamic M/Z Calibration FunctionThe function below performs dynamic m/z calibration using known calibrant ions. It splits the full dataset by identifying where the calibrant signals are strongest (typically their peak retention times), and fits a linear correction model within each chunk. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495## Module 2 MS calibrationdef calculate_ppm(observed_mass, ideal_mass): ppm = ((observed_mass - ideal_mass) / ideal_mass) * 1e6 return ppmdef find_peak_near_mass_after_mzcal(merged_spectrum, target_mass, tolerance_ppm): # Calculate the tolerance in m/z tolerance = target_mass * tolerance_ppm / 1e6 # Find peaks within the tolerance range peaks_within_tolerance = merged_spectrum[ (merged_spectrum['corrected_m/z'] &gt;= target_mass - tolerance) &amp; (merged_spectrum['corrected_m/z'] &lt;= target_mass + tolerance) ] return peaks_within_tolerancedef find_peak_near_mass(merged_spectrum, target_mass, tolerance_ppm): # Calculate the tolerance in m/z tolerance = target_mass * tolerance_ppm / 1e6 # Find peaks within the tolerance range peaks_within_tolerance = merged_spectrum[ (merged_spectrum['m/z'] &gt;= target_mass - tolerance) &amp; (merged_spectrum['m/z'] &lt;= target_mass + tolerance) ] return peaks_within_tolerancedef find_calibrant_matches(merged_spectrum, ideal_calibrants_mass, tolerance_ppm): calibrant_matches = [] for ideal_mass in ideal_calibrants_mass: tolerance = ideal_mass * tolerance_ppm / 1e6 # Tolerance based on ppm peaks_within_tolerance = merged_spectrum[ (merged_spectrum['m/z'] &gt;= ideal_mass - tolerance) &amp; (merged_spectrum['m/z'] &lt;= ideal_mass + tolerance) ] if not peaks_within_tolerance.empty: # Store the actual masses and their RTs for _, peak in peaks_within_tolerance.iterrows(): calibrant_matches.append((ideal_mass, peak['m/z'], peak['rt'])) return calibrant_matchesdef find_rt_for_maximum_mz(merged_spectrum, ideal_calibrants_mass, tolerance_ppm,overlapped_scan): calibrant_matches = find_calibrant_matches(merged_spectrum, ideal_calibrants_mass, tolerance_ppm) if not calibrant_matches: print('No calibrants found in the spectrum.') return [] max_ideal_mass = max(ideal_calibrants_mass) max_calibrant_matches = [(ideal_mass, actual_mass, rt) for ideal_mass, actual_mass, rt in calibrant_matches if ideal_mass == max_ideal_mass] max_calibrant_rts = sorted([match[2] for match in max_calibrant_matches]) # For overlapped mz scan, choose the second scan's rts (be careful!) if overlapped_scan == 1: max_calibrant_rts = max_calibrant_rts[1::2] return sorted(max_calibrant_rts) # Sort the RTsdef correct_mz_dynamically(merged_spectrum, ideal_calibrants_mass, tolerance_ppm, overlapped_scan): merged_spectrum = merged_spectrum.reset_index(drop = True) max_calibrant_rts = find_rt_for_maximum_mz(merged_spectrum, ideal_calibrants_mass, tolerance_ppm, overlapped_scan) chunk_boundaries = list(sorted(([merged_spectrum['rt'].min()] + max_calibrant_rts + [merged_spectrum['rt'].max()]))) model_params = [] # processing for each chunk split by the rts of the maximum m/z for i in range(len(chunk_boundaries) - 1): rt_start = chunk_boundaries[i] rt_end = chunk_boundaries[i + 1] chunk = merged_spectrum[(merged_spectrum['rt'] &gt;= rt_start) &amp; (merged_spectrum['rt'] &lt; rt_end)] actual_masses = {} for ideal_mass in ideal_calibrants_mass: tolerance = ideal_mass * tolerance_ppm / 1e6 peaks_within_chunk = chunk[ (chunk['m/z'] &gt;= ideal_mass - tolerance) &amp; (chunk['m/z'] &lt;= ideal_mass + tolerance) ] if not peaks_within_chunk.empty: actual_masses[ideal_mass] = np.mean(peaks_within_chunk['m/z']) # Step 5: Apply calibration if at least one calibrant is found if actual_masses: ideal_actual_pairs = np.array([(actual_masses[m], m) for m in actual_masses]) weights = np.abs(1 - (ideal_actual_pairs[:, 0] / ideal_actual_pairs[:, 1])) # Weight by proximity to ideal mass model = LinearRegression() model.fit(ideal_actual_pairs[:, 0].reshape(-1, 1), ideal_actual_pairs[:, 1], sample_weight=weights) # Apply the calibration to the current chunk chunk_rows = (merged_spectrum['rt'] &gt;= rt_start) &amp; (merged_spectrum['rt'] &lt; rt_end) merged_spectrum.loc[chunk_rows, 'corrected_m/z'] = model.predict(merged_spectrum.loc[chunk_rows, 'm/z'].values.reshape(-1, 1)) slope = model.coef_[0] intercept = model.intercept_ model_params.append({'rt_start': rt_start, 'rt_end': rt_end, 'slope': slope, 'intercept': intercept}) return merged_spectrum, model_params Execution and Output ParametersTo address potential m/z drift over long measurement periods, I developed a dynamic calibration procedure that allows for chunk-wise m/z correction using varying fitting parameters. This approach avoids the limitations of applying a single static equation to the entire dataset and ensures more accurate mass alignment throughout the run. A. Chunking by Retention Time of Maximum Calibrant The full time series is segmented into chunks based on the retention time (RT) of the maximum calibrant ion.Each chunk corresponds to a distinct time window, accounting for potential overlap to preserve peak continuity.Note: In our untargeted direct infusion study, the retention time refers to the time stamp of each specific MS1 scan. B. Calibrant-Based Correction Per Chunk For each time chunk: The actual m/z values of selected calibrant ions are extracted. A linear regression is performed between the measured m/z values and their corresponding ideal masses. This model is then used to recalibrate all ions in that chunk, shifting their m/z values to more accurate positions. Before the calibration 123456merged_spectrum_mzcal, model_params = correct_mz_dynamically( merged_spectrum, ideal_calibrants_mass, calibration_tolerance_ppm, # Predefined tolerance based on the typical mass error between actual and ideal calibrant values, I typically set to 20 ppm overlapped_scan_option) We then inspect the variation in calibration parameters (slope and intercept) across segments. 123456789101112131415161718slope_ = []intercept_ = []for c in model_params: slope_.append(c['slope']) intercept_.append(c['intercept'])fig = make_subplots(rows=1, cols=2)fig.add_trace(go.Scatter(x=np.arange(len(slope_)), y=slope_, mode='lines+markers'), row=1, col=1)fig.update_yaxes(title_text=\"Slope\", col=1, row=1)fig.add_trace(go.Scatter(x=np.arange(len(intercept_)), y=intercept_, mode='lines+markers'), row=1, col=2)fig.update_yaxes(title_text=\"Intercept\", col=2, row=1)fig.update_xaxes(title_text=\"Chunk\", showline=True, linecolor='black')fig.update_layout( height=400, width=900, title_text=\"Dynamic Fitting Parameters Across Retention Time\", plot_bgcolor='white', showlegend=False)fig.show() Check Calibration Accuracy on Known IonsWe visualize the distribution of m/z values for five reference ions before and after correction. This comparison confirms the improvement in accuracy. 123456789101112131415161718192021222324252627282930subplot_titles = ['Na2I', 'Na3I2', 'Na4I3','Na2[15]NO3', 'Na3[34]SO4']masses = [172.88347, 322.77771, 472.67196, 108.9638, 166.9163]colors = ['#447f44', '#cd4040', '#636efa', '#ddc927', '#9271dd']subplot_titles = []for i in range(len(masses)): ppm_differences = calculate_ppm(np.array(peak_results[masses[i]]['corrected_m/z']), masses[i]) mean_ppm = np.mean(ppm_differences) subplot_titles.append(f\"{['Na2I', 'Na3I2', 'Na4I3', 'Na2[15]NO3', 'Na3[34]SO4'][i]}: {mean_ppm:.2f} ppm\")fig = make_subplots(rows=1, cols=5, subplot_titles=subplot_titles)for i in range(len(masses)): fig.add_trace( go.Histogram(x=peak_results[masses[i]]['corrected_m/z'], marker=dict(color='blue')), row=1, col=i+1 ) fig.add_trace( go.Histogram(x=peak_results[masses[i]]['m/z'], marker=dict(color='red', opacity=0.75)), row=1, col=i+1 ) fig.add_vline(x=masses[i], line_width=2, line_dash=\"dash\", line_color=\"grey\", row=1, col=i+1) fig.update_xaxes(title_text=\"m/z\", range=[masses[i]-0.0025, masses[i]+0.0025], col=i+1, row=1) fig.update_yaxes(title_text=\"Count\", col=i+1, row=1)fig.update_layout( height=400, width=1500, title_text=\"Calibrant Mass Distribution Before and After Calibration\", plot_bgcolor='white', showlegend=False)fig.show() Here is an example of the post-check on those calibrants as well as those important ions (analytes).","link":"/2024/10/22/33.%5BMS%5DOrbiTrack_Dev_Log_2_MZ_calibration/"},{"title":"è¯»ä¹¦ç¬”è®°-æ„å¤§åˆ©æ–‡åŒ–ç®€å²","text":"ã€Šæ„å¤§åˆ©æ–‡åŒ–ç®€å²ã€‹ç‹å†›ï¼Œç‹è‹å¨œè‘—ï¼Œå¤–ç ”ç¤¾ï¼Œ2009 çŸ¥è¯†ç‚¹æ‘˜å½•ä¸æ€»ç»“ï¼Œé…å›¾é™¤ç‰¹åˆ«è¯´æ˜å¤–ï¼Œå‡æ¥æºäºç½‘ç»œã€‚ 1. å¤ä»£æ–‡æ˜1.1 å¤å¸Œè…Šæ–‡æ˜å¸Œè…Šæ˜¯è¥¿æ–¹æ–‡åŒ–çš„æ‘‡ç¯®ï¼Œç›´æ¥å½±å“è¥¿æ–¹ä¸–ç•Œçš„ç”Ÿæ´»æ–¹å¼å’Œæ€ç»´æ–¹æ³•ï¼Œè‰ºæœ¯æ–‡å­¦çš„å†…å®¹å’Œè¡¨ç°å½¢å¼å‡æ¥æºäºæ­¤ã€‚ å…‹é‡Œç‰¹æ–‡æ˜ 19ä¸–çºªæœ«ï¼Œè‹±å›½è€ƒå¤å­¦å®¶äºšç‘ŸÂ·ä¼Šæ–‡æ–¯çˆµå£«å‘ç°å…‹è¯ºè¨æ–¯é—å€ã€‚ å…‹é‡Œç‰¹å²›ä½äºå¸Œè…Šï¼Œå°äºšå’ŒåŸƒåŠä¹‹é—´ï¼Œå²›ä¸Šæœ¨æä¸°å¯Œï¼Œä¾¿äºé€ èˆ¹å¼€å±•æµ·ä¸Šè´¸æ˜“ï¼Œ5å¤©åˆ°è¾¾åŸƒåŠã€‚ ç±³è¯ºæ–¯æ–‡æ˜ (2000BC-1400BC)ï¼Œåç§°æºäºå¤å¸Œè…Šç¥è¯å…‹é‡Œç‰¹å›½ç‹ç±³è¯ºæ–¯çš„åå­—ã€‚å…‹é‡Œç‰¹æ–‡æ˜æœ‰ä¸¤ç§æ–‡å­—ï¼Œç”²ç§çº¿å½¢æ–‡å­—å’Œä¹™ç§çº¿å½¢æ–‡å­—ï¼Œå‰è€…è‡³ä»Šæ— äººç ´è¯‘ï¼Œåè€…å³ä¸ºå¤å¸Œè…Šæ–‡å­—ã€‚ è¿ˆé”¡å°¼æ–‡æ˜ 1400BC, ç«å±±å’Œæµ·å•¸ï¼Œå…‹é‡Œç‰¹æ–‡æ˜æ¹®ç­ã€‚å±…ä½åœ¨å¸Œè…ŠåŠå²›å—éƒ¨çš„è¿ˆé”¡å°¼äººï¼Œè·ç¦»ä»…100kmï¼Œå é¢†å²›å±¿ï¼Œç»§æ‰¿è¯¥æ–‡æ˜ã€‚ 1600BC-1200BC åˆ°è¾¾é¼ç››ï¼Œæ­¦å£«é˜¶çº§ï¼ŒåŸå¸‚è®¾æœ‰ç‹åŸï¼Œä¸»è¦ä»äº‹å†œç‰§ä¸šã€‚ è´¸æ˜“å¼€å§‹å‘å±•è‡³é»‘æµ·åœ°åŒºï¼Œå…¥å£åŸå¸‚ç‰¹æ´›ä¼Šæˆä¸ºè´¸æ˜“é˜»ç¢ï¼Œç‰¹æ´›ä¼Šæˆ˜äº‰çš„çœŸæ­£åŸå›  ï¼ˆè®°è½½äºè·é©¬ã€Šä¼Šåˆ©äºšç‰¹ã€‹ï¼‰ é»‘æš—æ—¶ä»£ 1200BC, å¸Œè…ŠåŒ—éƒ¨å¤šåˆ©äºšäººï¼ˆDoriï¼‰å—ä¸‹ï¼ˆå¸Œè…Šç¥è¯å¤§åŠ›ç¥èµ«æ‹‰å…‹å‹’æ–¯çš„åä»£ï¼Œç»„ç»‡ä¸¥å¯†ï¼Œè‹±å‹‡å–„æˆ˜ï¼‰ï¼ŒåŠ¿åŠ›æ›¾è¿œåŠå…‹é‡Œç‰¹å²›ã€‚å…¶å¯¹æ–¯å·´è¾¾å½±å“è¾ƒå¤§ï¼Œä½¿è¯¥åŸå§‹ç»ˆä¿æŒå†›äº‹å¯¡å¤´æ”¿æ²»ç»Ÿæ²»çš„ç‰¹å¾ã€‚ å¤šåˆ©äºšäººæ–‡æ˜ç¨‹åº¦ç›¸å¯¹è¾ƒä½ï¼Œä¸€èˆ¬è®¤ä¸ºå…¶å…¥ä¾µåæ•°ç™¾å¹´æ˜¯å¤å¸Œè…Šæ–‡æ˜çš„é»‘æš—æ—¶ä»£ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯å…¶å°†é‡è¦çš„å†¶é“æŠ€æœ¯å¼•å…¥å¸Œè…Šã€‚ é‡è¦çš„ç›¸å…³ç¥è¯ ç±³è¯ºæ–¯å›½ç‹çš„ç‰›å¤´äººå„¿å­ç±³è¯ºé™¶æ´›æ–¯ï¼Œè¢«å…³é—­åœ¨è¿·å®«ä¸­ã€‚é›…å…¸æ¯å¹´éœ€è¦è´¡çŒ®ç«¥ç”·ç«¥å¥³å„7åã€‚é›…å…¸å›½ç‹çˆ±ç´ï¼ˆEgoï¼ŒåŸƒå‹¾æ–¯ï¼‰å„¿å­å¿’ä¿®æ–¯ï¼ˆTescoï¼‰åœ¨ç±³è¯ºæ–¯å¥³å„¿é˜¿é‡Œé˜¿å¾·æ¶…ï¼ˆAriannaï¼‰çš„å¸®åŠ©ä¸‹è¯›æ€ç±³è¯ºé™¶æ´›æ–¯ã€‚ å¿’ä¿®æ–¯å’Œçˆ¶äº²çº¦å®šï¼Œè‹¥æ€æ­»ç±³è¯ºé™¶æ´›æ–¯è¿”èˆªå°†é»‘å¸†æŒ‚ä¸ºç™½å¸†ã€‚å¿’ä¿®æ–¯å¸¦é˜¿é‡Œé˜¿å¾·æ¶…è¿”å›é›…å…¸ï¼ŒåŠè·¯å°†å…¶é—å¼ƒã€‚å¿’ä¿®æ–¯å¿˜è®°æ¢ç™½å¸†ï¼ŒåŸƒå‹¾æ–¯è¯¯ä»¥ä¸ºå„¿å­å·²æ­»ï¼Œè·³æµ·è‡ªæ€ã€‚çˆ±ç´æµ·ï¼ˆMar Egeoï¼‰çš„ç”±æ¥ã€‚ å¿’ä¿®æ–¯èµ°å‡ºè¿·å®«çš„çº¿å›¢ç”±ä»£è¾¾æ´›æ–¯ï¼ˆè¿·å®«çš„è®¾è®¡è€…ï¼‰æä¾›ã€‚ç±³è¯ºæ–¯å›½ç‹å°†ä»£è¾¾ç½—æ–¯çˆ¶å­å…³æŠ¼ã€‚ä¸¤äººç”¨ç±³æ‹‰åˆ¶ä½œç¿…è†€ï¼Œå„¿å­ä¼Šå¡æ´›æ–¯ï¼ˆIcarusï¼‰å¿ƒé«˜æ°”å‚²é£å¾—å¤ªé«˜ï¼ŒåŒç¿¼èåŒ–æººæ¯™ã€‚ å¿’ä¿®æ–¯è¿”èˆªçš„èˆ¹åœ¨é›…å…¸ä¿ç•™å¾ˆä¹…ï¼Œä¸æ–­ç”¨æ–°æœ¨æ–™æ›¿æ¢æ›´æ–°ï¼Œä¿æŒä¸»ä½“ç»“æ„ä¸å˜ã€‚åœ¨è¥¿æ–¹æ–‡åŒ–ï¼Œä¿æŒä¸»ä½“ï¼Œä¿®æ”¹å±€éƒ¨çš„è¡Œä¸ºå› æ­¤è¢«ç§°ä¸ºâ€œå¿’ä¿®æ–¯çš„èˆ¹â€ï¼ˆla nave di Teseoï¼‰ã€‚ ã€Šä¼Šåˆ©äºšç‰¹ã€‹å–æè‡ªç‰¹æ´›ä¼Šæˆ˜äº‰ã€‚ç‰¹æ´›ä¼Šå°ç‹å­å¸•é‡Œæ–¯æ èµ°æ–¯å·´è¾¾å›½ç‹å¢¨æ¶…æ‹‰ä¿„æ–¯å¦»å­æµ·ä¼¦ï¼ˆElenaï¼‰ã€‚å¢¨æ¶…æ‹‰ä¿„æ–¯å¯»æ±‚è¿ˆé”¡å°¼å›½ç‹é˜¿è¿¦é—¨å†œçš„æ”¯æŒï¼Œè”åˆå„å¸Œè…Šç‹å›½å‘ç‰¹æ´›ä¼Šå®£æˆ˜ï¼Œ9å¹´æ”»åŸä¸åˆ©ã€‚ç¬¬10å¹´ï¼Œé˜¿è¿¦é—¨å†œä¸é˜¿å–€ç‰æ–¯å‘ç”Ÿæ¿€çƒˆå†²çªï¼ˆæˆ˜åˆ©å“åˆ†é…ä¸å‡ï¼‰ï¼Œåè€…æ‹’ç»å‡ºæˆ˜ï¼Œå¸Œè…Šè”å†›æŸå¤±æƒ¨é‡ã€‚é˜¿å–€ç‰æ–¯å¥½å‹å¸•ç‰¹æ´›å…‹é‡Œæ–¯æŠ«ä¸Šå‰è€…æˆ˜ç”²ï¼Œè¢«ç‰¹æ´›ä¼Šå›½ç‹é•¿å­èµ«å…‹æ‰˜è€³æ€æ­»ã€‚å¥½å‹æ®’å‘½ï¼Œé˜¿å–€ç‰æ–¯æ–©æ€èµ«å…‹æ‰˜è€³ï¼Œä½†æƒ¹æ¼äº†èµ«å…‹æ‰˜è€³çš„ä¿æŠ¤ç¥é˜¿æ³¢ç½—ã€‚é˜¿æ³¢ç½—å°„ä¸­é˜¿å–€ç‰æ–¯è„šè¸µï¼Œè‡´å…¶æ­»äº¡ã€‚ é˜¿å–€ç‰æ–¯ä¹‹è¸µï¼ˆTallone di Achilleï¼‰æ„ä¸ºâ€œè‡´å‘½çš„å¼±ç‚¹â€ï¼Œæœªå…¥å†¥æ²³æµ¸æ³¡ã€‚ ã€Šå¥¥å¾·èµ›ã€‹å¦ä¸€éƒ¨è·é©¬å²è¯—ï¼Œè®²è¿°ç‰¹æ´›ä¼Šæˆ˜äº‰åï¼Œè‹±é›„å¥¥å¾·ä¿®æ–¯ï¼ˆOdisseaï¼‰è‰°éš¾è¿”ä¹¡ï¼ˆ10å¹´ï¼‰ã€‚å¥¥å¾·ä¿®æ–¯æ˜¯ä¼Šå¡”åˆ»ï¼ˆItacaï¼‰å›½ç‹ï¼Œæœ¨é©¬è®¡è®¾è®¡è€…ã€‚ç‰¹æ´›ä¼Šçš„ä¿æŠ¤ç¥ä»¬æ†æ¨å¥¥å¾·ä¿®æ–¯ï¼Œåœ¨å…¶è¿”ä¹¡é€”ä¸­ä¸æ–­åŠ å®³ã€‚ç‹¬çœ¼å·¨äººæ³¢å•æ–æ‘©æ–¯ï¼ˆæµ·ç¥æ³¢å¡å†¬ä¹‹å­ï¼‰-&gt; é£ç‹è‰¾å¥¥ç½—æ–¯ -&gt;æµ·å¦–èµ›å£¬-&gt; æµ·æ€ªæ–¯åº“æ‹‰å’Œå¡å¾‹å¸ƒç‹„æ–¯-&gt;ç¥å¥³å¡å•æ™®ç´¢7å¹´æŒ½ç•™ã€‚å¥¥å¾·ä¿®æ–¯å¦»å­ç€æ¶…ç½—ç€ï¼ˆPenelopeï¼‰è‹¦ç­‰ä¸ˆå¤«20å¹´ï¼Œæ‹’ç»æ— æ•°çš„æ±‚å©šè€…ã€‚åœ¨ä¸ˆå¤«æ³æ— éŸ³ä¿¡çš„æƒ…å†µä¸‹ï¼Œå¥¹è¢«è¿«è¦åœ¨æ±‚å©šè€…ï¼ˆä¹Ÿæ˜¯è§Šè§ç‹ä½è€…ï¼‰ä¸­é€‰æ‹©ä¸€ä½æ–°çš„é…å¶ã€‚å¥¥å¾·ä¿®æ–¯æ°å·§è¿”å›ï¼Œä¹”è£…æˆä¹ä¸ï¼Œæ··å…¥ç‹å®«ï¼Œå‘å¦»å­çš„æ±‚å©šè€…ä»¬æå‡ºæŒ‘æˆ˜ï¼Œæ¯”è¯•ç®­æ³•ã€‚æˆ˜èƒœæ‰€æœ‰çš„å¯¹æ‰‹ï¼Œä¸å¦»å„¿ç›¸è®¤ï¼Œä¿ä½è´¢äº§å’Œç‹ä½ã€‚ 1.2 å¸Œè…ŠåŸé‚¦å›½å’Œå¸Œè…Šæ®–æ°‘åœ°å› åŠå²›å¤šå±±ï¼Œäº¤é€šä¸ä¾¿ï¼Œå„äººç±»èšå±…åŒºç›¸å¯¹ç‹¬ç«‹ï¼Œå¹¶é€æ¸å½¢æˆç‹¬ç«‹é‚¦å›½ï¼Œä¸”å‡ ä¹å…¨éƒ¨ä½äºæ²¿æµ·ã€‚å„åŸé‚¦åŠå…¶å‘¨è¾¹å†œæ‘æ„æˆä¸€ä¸ªâ€Polisâ€ï¼Œæ‹¥æœ‰ç‹¬ç«‹æ”¿åºœï¼Œæ³•å¾‹ï¼Œå†›é˜Ÿã€‚åŸé‚¦å†…å…¬å…±äº‹åŠ¡è¢«ç§°ä¸ºâ€œpoliticusâ€ã€‚å„å›½æ”¿æ²»ç‹¬ç«‹ï¼Œä½†è¯­è¨€æ–‡åŒ–ç›¸åŒ/é€šï¼Œè¢«è§†ä¸ºåŒä¸€ä¸ªæ°‘æ—ã€‚ 2. ä¸­ä¸–çºªæ–‡æ˜ç¥åœ£ç½—é©¬å¸å›½ä¸åŸºç£æ•™æ–‡åŒ–ç¥ç½—è¯ç”Ÿ ï¼ˆ800 ADï¼‰ åŸºç£æ•™å·²åœ¨æ¬§æ´²ä¸åŒæ°‘æ—é—´ç›¸å½“æ™®åŠï¼Œç½—é©¬æ•™ä¼šä¿æŒå´‡é«˜å¾®ä¿¡ã€‚åŠ æ´›æ—å®¶æ—çš„æ³•å…°å…‹ç‹æŸ¥ç†éœ€è¦ç½—é©¬ä¸»æ•™çš„æ”¯æŒï¼Œæœ‰åŠ©äºç»´æŒç»Ÿæ²»ï¼›åŒæ ·ï¼Œç½—é©¬ä¸»æ•™éœ€è¦æŸ¥ç†ä¿æŠ¤ã€‚ å…¬å…ƒ799å¹´ï¼Œæ•™çš‡åˆ©å¥¥ä¸‰ä¸–è¢«ç½—é©¬äººæ°‘é©±é€ï¼ŒæŸ¥ç†ç«‹å³å—ä¸‹æ”¯æ´ï¼Œæ¢å¤æƒä½ã€‚800 ADï¼Œæ•™çš‡æˆäºˆæŸ¥ç†çš‡å† ï¼Œä»¥ä¸Šå¸åä¹‰å®£å¸ƒå…¶ä¸ºâ€œç½—é©¬äººçš„çš‡å¸â€ æŸ¥ç†å¤§å¸çš„åŠ å†•ä»ªå¼ ä¸œç½—é©¬å¸å›½è®¤ä¸ºå¸ä½è¢«ç¯¡å¤ºã€‚è‡ªæ­¤ï¼Œç½—é©¬æ•™ä¼šå®Œå…¨æ‘†è„±æ‡‚ç½—é©¬å¸å›½çš„æ§åˆ¶ï¼Œåƒ­è¶Šäº†åŠ å†•çš‡å¸çš„æƒåŠ›ï¼Œå¹¶å°†æƒåŠ›ç”±ç¥æƒå»¶ä¼¸è‡³æ”¿æƒï¼Œç¡®ç«‹å¯¹æ„å¤§åˆ©ä¸­éƒ¨åœ°åŒºçš„ç»Ÿæ²»ã€‚ æŸ¥ç†é‡æ–°ç»Ÿä¸€æ¬§æ´²ï¼Œå²ç§°â€œæŸ¥ç†å¤§å¸â€ï¼ˆå¦è¯‘â€œæŸ¥ç†æ›¼â€ï¼Œâ€œæ›¼â€æ„ä¸ºâ€œä¼Ÿå¤§çš„â€ï¼‰ï¼Œå…¶åˆ›ç«‹å¸å›½è¢«ç§°ä¸ºâ€œç¥åœ£ç½—é©¬å¸å›½â€ï¼Œç¥åœ£ä»£è¡¨å¸å›½å…·æœ‰çš„åŸºç£æ•™æ€§è´¨ï¼Œç½—é©¬è¡¨æ˜å¯¹ç½—é©¬å¸å›½æ­£ç»Ÿæ€§çš„ç»§æ‰¿ã€‚ æ¬§æ´²å¤§éƒ¨åˆ†åœ°åŒºå®ç°æ”¿æ²»ç»Ÿä¸€ï¼Œä¼ ç»Ÿçš„å¤ç½—é©¬æ–‡åŒ–ä¸åŸºç£æ•™æ–‡åŒ–å’Œæ—¥è€³æ›¼æ–‡åŒ–èåˆï¼Œæ¬§æ´²å»¶ç»­æ•°ç™¾å¹´çš„æ”¿æ²»ä½“åˆ¶â€”å°å»ºåˆ¶ï¼Œä¹Ÿé€æ­¥å½¢æˆã€‚ å°å»ºåˆ¶ å¤§åœ°ä¸»æ‹¥æœ‰é™„åº¸çš„æ­¦å£«ã€‚äºŒè€…ä¾æ®â€œçŒ®èº«â€ä»ªå¼ï¼ˆhommageï¼‰ç¡®ç«‹ä¸»ä»†å…³ç³»ï¼Œæ­¦å£«å¯è·å¾—åœŸåœ°ï¼Œç£¨åŠç­‰æ”¶ç›Šï¼ŒåŒæ—¶è´Ÿæœ‰æ•ˆå¿ ä¸»äººçš„ä¹‰åŠ¡ã€‚ å¤§åœ°ä¸»éƒ¨åˆ†æˆ–å‡ºèº«äºè¥¿ç½—é©¬å¸å›½çš„å…ƒè€é™¢å…ƒè€å®¶æ—ï¼ˆå‰æœè´µæ—ï¼‰ï¼Œæˆ–å‡ºèº«äºè›®æ—æ­¦å£«é¦–é¢†ï¼ˆå¾æœè€…ï¼‰ã€‚ä»–ä»¬å½¢æˆçš„æ–°ç¤¾ä¼šé˜¶çº§ï¼Œè¢«ç§°ä¸ºâ€œæœ›æ—é˜¶çº§â€ï¼Œå³ä¸­ä¸–çºªè´µæ—é˜¶çº§ï¼ˆæ‹‰ä¸è¯­nobilisï¼‰ã€‚ æŸ¥ç†è¢«åŠ å†•åï¼Œå®šéƒ½äºšç›ï¼Œè¿™ä¹Ÿæ„å‘³æ–°ç½—é©¬ä¸å†ä»¥ç¯åœ°ä¸­æµ·ä½œä¸ºä¸»è¦å‘å±•æ–¹å‘ï¼Œè€Œæ˜¯è½¬å…¥ç»è¥æ¬§æ´²çš„ä¸­éƒ¨å’ŒåŒ—éƒ¨ã€‚ ç¥ç½—ç–†åŸŸè¿‡äºå¹¿å¤§è€Œäº¤é€šä¸å˜ï¼Œä¸­å¤®å¾æœéš¾ä»¥ç›´æ¥ç®¡ç†ï¼Œæ•…æŸ¥ç†å¤§å¸é‡‡å–æ—©å·²å­˜åœ¨çš„åˆ†å°æ–¹å¼ï¼Œå»ºç«‹å¸å›½çš„åˆ†å°åˆ¶åº¦ã€‚ å¸å›½çš„åœŸåœ°åˆ†ä¸ºæ•°åå—å°åœ°ï¼Œåˆ†å°äºéƒ¨ä¸‹ã€‚è¿™äº›äººæˆä¸ºå¤§å°å»ºä¸»ï¼Œè¢«ç§°ä½œä¼¯çˆµã€ä¾¯çˆµã€å…¬çˆµã€‚åœ¨æ³•å…°å…‹è¯­ä¸­ï¼š å…¬çˆµä¸€è¯ä¸ºduxï¼Œæ¥è‡ªæ‹‰ä¸è¯­ï¼Œæ„ä¸ºé¦–é¢†ï¼Œå…¶å°åœ°æœ€å¹¿ï¼Œåœ°ä½æœ€æ˜¾èµ« ä¼¯çˆµä¸€è¯ä¸ºconteï¼Œæ¥è‡ªæ‹‰ä¸è¯­çš„comiteï¼Œæ„ä¸ºæˆ˜å‹ï¼Œå…¶å°åœ°è¾ƒå°ï¼Œåœ°ä½ä½äºå…¬çˆµï¼› ä¾¯çˆµä¸€è¯ä¸ºmarchisï¼Œæ¥è‡ªæ—¥è€³æ›¼è¯­çš„markaï¼Œæ„ä¸ºè¾¹ç•Œåœ°åŒºå°åœ°çš„ç»Ÿæ²»è€…ã€‚ æ¯å¹´5æœˆï¼Œçš‡å¸å¬é›†å°å»ºä¸»å¤§ä¼šï¼Œé¢å¸ƒé‡è¦å†³å®šï¼Œå½¢æˆå›½å®¶çš„æ³•å¾‹ã€‚åŒæ—¶ï¼Œçš‡å¸ç»å¸¸æ´¾å·¡æŒ‰ä½¿åˆ°å„åœ°å·¡è§†ï¼ˆåˆºå²ï¼‰ï¼Œç›‘å¯Ÿæ³•å¾‹æ‰§è¡Œæƒ…å†µå’Œå°å»ºä¸»æ˜¯å¦å¿ äºçš‡å¸å·¡æŒ‰ä½¿ã€‚æœ‰æ—¶ï¼Œçš‡å¸è¿˜äº²ç‡å·¡æŒ‰ä½¿è§†å¯Ÿå„å°åœ°ã€‚ å°å»ºä¸»åœ¨å„è‡ªå°åœ°é«˜åº¦è‡ªç”±ï¼Œè§†è‡ªå·±ä¸ºå°åœ°çš„â€œå›ä¸»â€ï¼Œè±¢å…»å¤§æ‰¹éª‘å£«ï¼Œå‘å·æ–½ä»¤ï¼Œæ¨ªå¾æš´æ•›ï¼Œæ¬ºå‹å°åœ°å±…æ°‘ã€‚ æŸ¥ç†å»ºç«‹å°å»ºåˆ¶åº¦çš„ä¸€ä¸ªé‡è¦ç›®çš„æ˜¯æ»¡è¶³å†›äº‹éœ€æ±‚ï¼Œå³çš‡å¸å¯éšæ—¶è°ƒé…å…¶é™„åº¸ï¼ˆå°å»ºé¢†ä¸»ï¼‰ï¼Œä»¥åŠå…¶é™„åº¸çš„é™„åº¸ï¼ˆå°å»ºä¸»çš„éª‘å£«ï¼‰ã€‚ä½†è¯¥æœºåˆ¶å­˜åœ¨è‡´å‘½é—®é¢˜ï¼Œå³å»ºç«‹åœ¨çš‡å¸ä¸å°å»ºä¸»ä¸ªäººçš„é—å­˜å…³ç³»ä¸Šã€‚éšæ—¶é—´æ¨ç§»ï¼Œè¿™ç§å…³ç³»å¾ˆå®¹æ˜“è¢«ç ´åï¼šå°å»ºä¸»ç»å¸¸ä½¯è£…å¿˜è®°èª“è¨€ï¼Œå½“çš‡å¸å®£å¬æ—¶ï¼Œå€Ÿæœºè¦æ±‚èµèµï¼Œå¦åˆ™æŒ‰å…µä¸åŠ¨ã€‚å¸å›½æ‰©å¼ é˜¶æ®µï¼ŒæŸ¥ç†æ–°å¤ºå–çš„åœŸåœ°å¯æ»¡è¶³å°å»ºä¸»ä¸æ–­è†¨èƒ€çš„æ¬²æœ›ï¼›æŸ¥ç†ç»§æ‰¿è€…å´æ— æ³•å¤ºå–æ–°çš„ç–†åœŸï¼Œåªèƒ½åˆ†å°è‡ªå·±çš„åœŸåœ°ï¼Œå¯¼è‡´çš‡å¸åŠ¿åŠ›è¡°å¼±ï¼Œä¸§å¤±äº†ä¸­å¤®æ”¿æƒçš„ç»Ÿæ²»æƒå¨ã€‚ å…¬å…ƒ9ä¸–çºªä¸­å¶ï¼Œéšç€ä¸­å¤®æ”¿æƒçš„ä¸æ–­å‰Šå¼±ï¼Œçš‡å¸é€æ¸å¤±å»äº†å¯¹å°å»ºä¸»çš„æ§åˆ¶ï¼›å°å»ºä¸»çš„åŠ¿åŠ›å¢å¼ºï¼Œä¸æ–­æ‰©å¤§çš„å°åœ°å˜æˆäº†ä»–ä»¬çš„ä¸–è¢­ç§äº§ã€‚å°å»ºä¸»è¶Šæ¥è¶Šä¸æœä»çš‡å¸çš„å‘½ä»¤ï¼Œæ¬§æ´²å‡ºç°äº†â€œå°å»ºçš„æ— æ”¿åºœçŠ¶æ€â€ï¼Œå¹¶ä¸€ç›´å»¶ç»­åˆ°10ä¸–çºªä¸­å¶ã€‚è¿™ä¸€ä¸ªä¸–çºªæ°æ°æ˜¯é˜¿æ‹‰ä¼¯äººå’Œè›®æ—äººå†åº¦ä¾µæ‰°æ¬§æ´²çš„æ—¶æœŸã€‚ è½¯å¼±çš„ä¸­å¤®æ”¿æƒæ— åŠ›æŠµæŠ—å¤–æ¥ä¾µç•¥ï¼Œäººæ°‘å¯¹å…¶ä¸§å¤±äº†ä¿¡å¿ƒã€‚å°å»ºä¸»ä¾¿è¶æœºåŠ å¼ºè‡ªå·±çš„åŠ›é‡ï¼ŒæŠŠå®«æ®¿æ”¹é€ æˆåŸå ¡ä»¥æŠµå¾¡æ¥çŠ¯ä¹‹æ•Œã€‚äººä»¬å› æƒ§æ€•ä¾µç•¥è€…çš„è‚†æ„å± æ€ï¼Œçº·çº·å¯»æ±‚å°å»ºä¸»çš„ä¿æŠ¤ï¼›ä¸ºäº†ç”Ÿå‘½å®‰å…¨ï¼Œä»–ä»¬ç”˜æ„¿å¤±å»è‡ªç”±æˆä¸ºå°å»ºä¸»çš„å†œå¥´ã€‚è¿™æ ·ï¼Œå°å»ºä½“åˆ¶åœ¨æ¬§æ´²ç¤¾ä¼šä¾¿ç‰¢ç‰¢åœ°æ‰ä¸‹äº†æ ¹ã€‚ ç‘å£«é˜¿å°”é«˜å·çš„å“ˆå¸ƒæ–¯å ¡åŸå ¡ï¼ˆæ‹æ‘„äº2021å¹´4æœˆ25æ—¥ï¼‰ å†æ¬¡çš„åˆ†è£‚ä¸æ··ä¹± 814 ADï¼ŒæŸ¥ç†å¤§å¸å„¿å­â€œè™”è¯šè€…â€è·¯æ˜“ä¸€ä¸–ï¼ˆLudovico il Pioæˆ–Luigi Iï¼‰ç»§ä½ï¼Œä»–å°†å¸å›½é¢†åœŸåˆ†å°ç»™3ä¸ªå„¿å­ã€‚å›´ç»•é¢†åœŸå’Œçš‡æƒçš„æˆ˜äº‰ç›´è‡³843 ADï¼Œæœ€ç»ˆç­¾è®¢å‡¡å°”ç™»æ¡çº¦ï¼Œå½¢æˆä¸‰ä¸ªç‹å›½ï¼Œå³æ„å¤§åˆ©ç‹å›½ã€æ—¥è€³æ›¼ç‹å›½å’Œæ³•å…°è¥¿ç‹å›½ã€‚ é•¿å­™æ´›æ³°å°”ï¼ˆLotarioï¼‰æ‰¿è¢­å¸å·ï¼Œä½†æœ‰åæ— å®ï¼Œä»»æ„å¤§åˆ©å›½ç‹ï¼Œå…¶é¢†åœŸåŒ…æ‹¬ä½åœ°å›½å®¶ï¼Œé˜¿å°”è¨æ–¯å’Œæ´›æ—ï¼Œå‹ƒè‰®ç¬¬ï¼Œæ™®ç½—æ—ºæ–¯ï¼Œæ„å¤§åˆ©åŒ—éƒ¨åŠäºšç›ï¼Œç½—é©¬ä¸¤åº§åŸå¸‚ï¼›855 ADï¼Œæ´›æ³°å°”å»ä¸–ï¼Œä¸­æ³•å…°å…‹å¸å›½åˆ†è£‚ï¼ˆç›´è‡³19ä¸–çºªï¼‰ã€‚ â€œæ—¥è€³æ›¼äººâ€è·¯æ˜“ï¼ˆLudovico il Germanicoï¼‰ä»»æ—¥è€³æ›¼å›½ç‹ï¼Œé¢†åœŸåŒ…æ‹¬æ„å¤§åˆ©ä»¥åŒ—ï¼Œè±èŒµæ²³ä»¥ä¸œï¼ŒåŠè±èŒµæ²³ä»¥è¥¿çš„è±èŒµå…°åœ°åŒºï¼› â€œç§ƒå¤´æŸ¥ç†â€ï¼ˆCarlo il Calvoï¼‰ä»»æ³•å…°è¥¿å›½ç‹ï¼Œé¢†åœŸåŒ…æ‹¬ç½—è®·æ²³ä»¥è¥¿æ‰€æœ‰é¢†åœŸã€‚ ã€Šå‡¡å°”ç™»æ¡çº¦ã€‹ç­¾è®¢åçš„åŠ æ´›æ—å¸å›½ç‰ˆå›¾ å—æ–¹ï¼ŒæŒæ¡å…ˆè¿›èˆªæµ·æŠ€æœ¯çš„é˜¿æ‹‰ä¼¯äººæˆä¸ºåœ°ä¸­æµ·éœ¸ä¸»ï¼Œå¤ºå–ç§‘è¥¿å˜‰å’Œè¥¿è¥¿é‡Œï¼Œå¹¶ä»¥æ­¤ä¸ºè·³æ¿ä¸æ–­ä¾µè¢­æ„å¤§åˆ©å’Œæ³•å…°è¥¿æ²¿æµ·ï¼Œä¿˜è™ç”·å¥³ä½œä¸ºå¥´éš¶ã€‚å…¬å…ƒ845å¹´ï¼Œé˜¿æ‹‰ä¼¯äººæ´—åŠ«äº†ç½—é©¬åŸã€‚å…¬å…ƒ10ä¸–çºªï¼Œä»–ä»¬æ·±å…¥æ¬§æ´²è…¹åœ°çš„é˜¿å°”å‘æ–¯å±±è„‰ï¼Œæ§åˆ¶è¿æ¥æ„å¤§åˆ©å’Œæ³•å…°è¥¿çš„äº¤é€šè¦é“æ•°åå¹´ä¹‹ä¹…ï¼ŒåŠ«æ è¿‡å¾€å•†äººå’Œæ—…è¡Œè€…çš„è´¢ç‰©ã€‚ åŒ—æ–¹ï¼Œå…¬å…ƒ8ä¸–çºªæœ«ï¼Œæ¥è‡ªåŒ—æ¬§ä¸¹éº¦ç­‰åœ°å¾—è¯ºæ›¼äººä¸¹éº¦äººï¼Œå¼€å§‹ä¸æ–­ä¾µæŸ“æ¬§æ´²å…¶ä»–å›½å®¶ï¼Œä»è‹±æ ¼å…°ç›´è‡³é»‘æµ·åœ°åŒºã€‚å…¬å…ƒ10ä¸–çºªï¼Œè¯ºæ›¼äººç›´æŠµæ³•å…°è¥¿é¦–éƒ½å·´é»ï¼ŒæŸ¥ç†ä¸‰ä¸–ä¸å…¶é¦–é¢†ç½—æ´›ï¼ˆæ­¥è¡Œè€…ç½—å¤«ï¼Œå› èº«æè¿‡äºé«˜å¤§ï¼Œæ²¡æœ‰è½½ä»–çš„é©¬ï¼Œåªèƒ½æ­¥è¡Œï¼‰ç­¾è®¢ã€Šåœ£-å…‹è±å°”-åŸƒæ™®ç‰¹æ¡çº¦ã€‹ï¼Œè·å¾—å¡çº³æ²³å£çš„å¢æ˜‚åœ°åŒºï¼Œè·å¾—å…¬çˆµå¤´è¡”ã€‚å…¬å…ƒ911å¹´ï¼Œè¯ºæ›¼åº•å…¬å›½å»ºç«‹ï¼Œå¹¶çšˆä¾åŸºç£æ•™ã€‚ 11ä¸–çºªï¼Œå¤§æ‰¹è¯ºæ›¼éª‘å£«å—ä¸‹æ„å¤§åˆ©ï¼Œå‡»è´¥æ•™çš‡å›½å†›é˜ŸåŠç»Ÿæ²»æ„å¤§åˆ©åŠå²›å—éƒ¨çš„æ‹œå åº­å’Œä¼¦å·´ç¬¬äººï¼Œé©±é€äº†æ’’æ‹‰é€Šäººï¼Œå¤ºå–äº†åå¸•å°¼äºšã€å¡æ‹‰å¸ƒåˆ©äºšã€æ™®åˆ©äºšç­‰åœ°åŒºã€‚ å…¬å…ƒ1059å¹´ï¼Œè¯ºæ›¼äººçš„é¦–é¢†åœ­æ–¯å¡å¾·ï¼ˆGuiscardoï¼‰ä¸æ•™çš‡å°¼å¤æ‹‰äºŒä¸–è¾¾æˆæ”¿æ•™åè®®ï¼Œè¢«æ•™çš‡åŠ å°ä¸ºæ™®åˆ©äºšä¸å¡æ‹‰å¸ƒåˆ©äºšå…¬çˆµï¼Œå…¶å¾æœè¡Œä¸ºè¢«ç½—é©¬æ•™å»·æ‰€è®¤å¯ï¼›åŒæ—¶ä»–æ‰¿è¯ºè¿›å†›è¥¿è¥¿é‡Œï¼Œé©±é€é˜¿æ‹‰ä¼¯äººï¼Œä½¿è¯¥å²›é‡å›åŸºç£æ•™çš„æ€€æŠ±ã€‚åæ¥ï¼Œåœ¨åœ­æ–¯å¡å¾·çš„å…„å¼Ÿé²æ°ç½—ä¸€ä¸–ï¼ˆRuggero Iï¼‰çš„ç‡é¢†ä¸‹ï¼Œç»è¿‡30å¹´çš„æ¿€æˆ˜ï¼Œè¯ºæ›¼äººç»ˆäºæ‰“è´¥äº†é˜¿æ‹‰ä¼¯äººï¼Œäº1091å¹´å é¢†äº†è¥¿è¥¿é‡Œå²›ã€‚å…¬å…ƒ1130å¹´ï¼Œé²æ°ç½—äºŒä¸–ï¼ˆRuggero IIï¼‰è¢«åŠ å†•ä¸ºè¥¿è¥¿é‡Œå›½ç‹ï¼Œå…¼é¢†æ™®åˆ©äºšä¸å¡æ‹‰å¸ƒåˆ©äºšå…¬å›½ã€‚ ä¸œæ–¹ï¼Œå…¬å…ƒ9ä¸–çºªï¼ŒåŒˆç‰™åˆ©äººçš„ç¥–å…ˆé©¬æ‰å°”äººç”±é¥è¿œçš„ä¸œæ–¹è‰åŸè¿å¾™è‡³æ¬§æ´²ï¼Œä¾µå…¥æ—¥è€³æ›¼ã€æ³•å…°è¥¿å’Œæ„å¤§åˆ©ï¼›å…¬å…ƒ955å¹´ï¼ŒåŒˆäººè¢«æ—¥è€³æ›¼å›½ç‹å¥¥æ‰˜ä¸€ä¸–ï¼ˆOttone Iï¼‰æ‰“è´¥ï¼Œè¢«è¿«é€€å›åŒˆç‰™åˆ©ï¼Œä¹‹åçšˆä¾äº†åŸºç£æ•™ã€‚ æ—¥è€³æ›¼ç¥åœ£ç½—é©¬å¸å›½çš„å»ºç«‹ 936 ADï¼Œ è¨å…‹æ£®å…¬çˆµå¥¥æ‰˜ä¸€ä¸–ç™»ä¸Šæ—¥è€³æ›¼ç‹ä½ï¼Œ955 ADæ‰“è´¥é©¬æ‰å°”äººå…¥ä¾µè€…ï¼Œæˆä¸ºåŸºç£æ•™å«å£«ã€‚961 ADï¼Œå—æ•™çš‡é‚€è¯·ï¼Œå—ä¸‹ç»´æŠ¤æ•™çš‡åˆ©ç›Šï¼Œæ§åˆ¶æ„å¤§åˆ©åŒ—éƒ¨ã€‚962 ADï¼Œå—æ•™çš‡åŠ å†•ä¸ºâ€œæ—¥è€³æ›¼ç¥åœ£ç½—é©¬å¸å›½çš‡å¸â€ ï¼ˆä¸æŸ¥ç†æ›¼å¤§å¸ä¸åŒï¼Œè™½åŒæ‰¿æ‹…ä¿å«åŸºç£æ•™çš„ä¹‰åŠ¡ï¼Œä½†å¸å›½ç–†åŸŸç¼©å‡è‡³æ—¥è€³æ›¼åœ°åŒºï¼ˆå¾·å¥¥ï¼‰å’Œæ„å¤§åˆ©åŒ—éƒ¨ï¼‰ã€‚ 955 AD, é©¬æœ­å„¿äººåœ¨ä»Šå¾·å›½å¥¥æ ¼æ–¯å ¡é™„è¿‘æ¨ªæ¸¡è±èŒµæ²³ å¥¥æ‰˜ä¸€ä¸–åŠ å†•åè¯•å›¾æ§åˆ¶ç½—é©¬æ•™å»·ï¼Œå‘å¸ƒã€Šå¥¥æ‰˜ç‰¹æƒã€‹å£°æ˜çš‡å¸å…·æœ‰æ‰¹å‡†æ•™çš‡é€‰ä¸¾ç»“æœçš„æƒåŠ›ï¼Œç¥æƒä¾é™„äºæ”¿æƒã€‚è‡ªæ­¤è¥¿æ¬§æ•°ç™¾å¹´æ•™çš‡ä¸çš‡å¸çš„æƒæ–—å†å²å¼€å§‹ä¸Šæ¼”ã€‚ â€œä¸»æ•™â€”ä¼¯çˆµåˆ¶â€ä¸å…‹å•å°¼æ”¹é© è€ƒè™‘åˆ°å°å»ºé¢†ä¸»å¯èƒ½ä¸é¡ºä»çš‡å¸ï¼Œå¥¥æ‰˜ä¸€ä¸–å°†åœŸåœ°åˆ†å°ç»™ä¸»æ•™ï¼Œå§”æ‰˜å…¶ç®¡ç†åœ°æ–¹äº‹åŠ¡ã€‚ç”±äºä¸»æ•™æ˜¯ç¥èŒäººå‘˜ï¼Œæ— å­å—£ï¼Œæ— æ³•ä¼ ä½ï¼Œæ•…å»ä¸–åï¼Œå°åœ°è¿”è¿˜å›ä¸»ã€‚çš‡å¸å’Œæ•™çš‡éƒ½å¸Œæœ›åŒæ—¶æŒæ¡æ”¿æƒå’Œç¥æƒï¼Œçš‡å¸è¯•å›¾å¹²é¢„æ•™ä¼šçš„å®—æ•™äº‹åŠ¡ï¼Œå¦‚è‡ªå·±å‘æ•™ä¼šæ¨èä¸»æ•™äººé€‰ï¼Œæ•™çš‡ä¹Ÿè¯•å›¾å¹²é¢„å¸å›½çš„æ”¿æ²»äº‹åŠ¡ï¼Œè®©è‡ªå·±äººæˆä¸ºä¼¯çˆµã€‚å› æ­¤ï¼ŒäºŒè€…ä¹‹é—´å¿…ç„¶ä¼šå‡ºç°ä¸å¯è°ƒå’Œçš„çŸ›ç›¾å’Œæ–—äº‰ã€‚ â€œä¸»æ•™â€”ä¼¯çˆµåˆ¶â€æœ‰åˆ©äºå·©å›ºçš‡æƒï¼Œå´ä¸¥é‡åœ°å½±å“äº†æ•™ä¼šã€‚äººä»¬è°‹æ±‚é«˜çº§æ•™èŒï¼Œä¸ä¸ºå®—æ•™ç†æƒ³ï¼Œè€Œä¸ºç‰Ÿå–æƒåŠ›ä¸è´¢å¯Œï¼Œæ•™ä¼šä¸€æ­¥æ­¥è…åå •è½ã€‚è‡ª11ä¸–çºªä¸­å¶èµ·ï¼Œä¸€äº›è™”è¯šçš„æ•™å£«å¥‹èµ·åæŠ—ï¼Œå‘¼åæ•™ä¼šæ”¹é©ï¼Œä¸¥ç¦çš‡å¸å’Œå°å»ºä¸»å¹²é¢„æ•™åŠ¡ï¼Œéå®—æ•™äººå£«ä¸å¾—å‚ä¸é€‰ä¸¾å’Œä»»å‘½æ•™èŒ ï¼ˆTips: ä¿®é“é™¢ç›´æ¥å—æ•™çš‡ä¿æŠ¤ï¼ŒåœŸåœ°è´¢äº§å½’äºç¦„é©¬æ•™å»·ï¼Œå°å»ºä¸»æ— æ³•éšæ„ä¾µçŠ¯ï¼Œæ•…åœ¨æ”¹é©ä¸­å…·æœ‰å¼ºå¤§çš„æˆ˜æ–—åŠ›ï¼‰ã€‚ ä½äºæ³•å›½é©¬å­”é™„è¿‘çš„å…‹å•å°¼ä¿®é“é™¢ï¼Œå…‹å•å°¼æ”¹é©è¿åŠ¨çš„å‘èµ·åœ° æ ¼åˆ—é«˜åˆ©ä¸ƒä¸–ä¸äº¨åˆ©å››ä¸– 1073å¹´ï¼Œå…‹å•å°¼ä¿®é“é™¢çš„ä»£è¡¨äººç‰©æ ¼åˆ—é«˜åˆ©æˆä¸ºæ•™çš‡ã€‚å…¬å…ƒ1075å¹´ï¼Œæ ¼åˆ—é«˜åˆ©ä¸ƒä¸–å‘å¸ƒæ••ä»¤ï¼šæ•™çš‡é«˜äºçš‡å¸ï¼Œæœ‰æƒè¯„åˆ¤çš‡å¸å’Œå›½ç‹çš„è¡Œä¸ºæ­£ç¡®ä¸å¦ï¼Œå¿…è¦æ—¶æœ‰æƒç½¢é»œçš‡å¸æˆ–å›½ç‹ã€‚æ­¤æ••ä»¤å°†æ”¿â€œä¸€åˆ‡æƒåŠ›æ¥è‡ªäºä¸Šå¸â€è§£é‡Šä¸ºä¸€åˆ‡æƒåŠ›æ¥è‡ªäºä¸Šå¸åœ¨äººé—´çš„ä»£è¡¨â€”â€”æ•™çš‡ï¼Œç›¸å½“äºå¯¹ç¥ç½—å¸å›½çš‡å¸å®£æˆ˜ã€‚ 1076å¹´ï¼Œäº¨åˆ©å››ä¸–ï¼ˆEnrico IVï¼‰çš‡å¸æ— æ³•å¿å—è€»è¾±ï¼Œå¬é›†æ•ˆå¿ çš‡å¸çš„ä¸»æ•™å¼€ä¼šï¼Œå®£å¸ƒç½¢é»œæ ¼åˆ—é«˜åˆ©ä¸ƒä¸–æ•™çš‡ï¼Œé‡ç”³çš‡å¸è‡³é«˜æ— ä¸Šçš„åœ°ä½ã€‚æ ¼åˆ—é«˜åˆ©ä¸ƒä¸–éšå³å®£å¸ƒå¼€é™¤äº¨åˆ©å››ä¸–æ•™ç±ï¼ˆç­‰åŒäºå‰¥å¤ºç»Ÿæ²»æƒï¼‰ã€‚å¤§å°å»ºä¸»å¼€å§‹è§£é™¤ä¸çš‡å¸çš„ä¹‰åŠ¡ï¼ˆåŸºç£å¾’æ— éœ€æ‰¿æ‹…å¯¹éåŸºç£å¾’å›ä¸»çš„ä¹‰åŠ¡ï¼‰ï¼Œçº·çº·åå›ã€‚äº¨åˆ©å››ä¸–è¢«è¿«åœ¨1æœˆå‰å¾€æ„å¤§åˆ©ä¸­éƒ¨çš„å¡è¯ºè¨ï¼ˆCanossaï¼‰åŸå ¡ï¼Œèº«ç©¿åƒ§è¢åœ¨å¤–é™å€™3æ—¥ï¼Œè·å¾—æ•™çš‡å®½æ•ï¼Œå¹¶æ¢å¤æ•™ç±ã€‚ äº¨åˆ©å››ä¸–åœ¨å¡è¨è¯ºåŸå ¡å‰å‘æ ¼é‡Œé«˜åˆ©ä¸ƒä¸–å¿æ‚” ä¹‹åï¼Œäº¨åˆ©å››ä¸–å·åœŸé‡æ¥ï¼Œæ‹˜æ•æ ¼é‡Œé«˜åˆ©ä¸ƒä¸–ï¼Œå…³æŠ¼äºç½—é©¬çš„åœ£å¤©ä½¿å¤å ¡ï¼Œå¹¶é‡æ–°ä»»å‘½æ•™çš‡ä¸ºå…¶åŠ å†•ã€‚å æ®æ„å¤§åˆ©å—æ–¹çš„è¯ºæ›¼äººåœ­æ–¯å¡å¾·å¹²é¢„ï¼Œæ ¼é‡Œé«˜åˆ©å¾—ä»¥é‡Šæ”¾ï¼Œä½†è½¯ç¦äºæ„å¤§åˆ©å—éƒ¨çš„è¨å‹’è¯ºç›´è‡³å»ä¸–ã€‚ äº¨åˆ©å››ä¸–å’Œæ ¼é‡Œé«˜åˆ©å»ä¸–åï¼Œå¸å›½ä¸æ•™å»·ä¹‹äº‰å»¶ç»­æ•°åå¹´ã€‚1122ADï¼ŒåŒæ–¹åœ¨æ²ƒå°”å§†ç­¾è®¢ã€Šæ²ƒå°”å§†æ–¯åè®®ã€‹ï¼Œç¡®å®šï¼šä¸»æ•™ç”±æ•™å£«ç»„æˆçš„é€‰ä¸¾ä¼šè®®æ¨é€‰ï¼Œæ—¥è€³æ›¼ä¸»æ•™çš„é€‰ä¸¾ä¼šè®®é¡»åœ¨çš‡å¸æˆ–å…¶ä»£è¡¨è…ä¸´çš„æƒ…å†µä¸‹è¿›è¡Œï¼Œå¦‚æœä¸»æ•™å…¼ä»»å°å»ºä¸»ï¼Œçš‡å¸åˆ™æœ‰æƒä¸¾è¡Œä¸»æ•™çš„å™çˆµä»ªå¼ï¼Œæˆäºˆå…¶è±¡å¾ä¸–ä¿—æƒåŠ›çš„æƒæ–ã€‚ä¸¤ä¸ªä»ªå¼çš„å…ˆåé¡ºåºæä¸ºé‡è¦ï¼Œå› ä¸ºå…ˆè€…æœ‰æƒç¡®å®šâ€œä¸»æ•™â€”â€”ä¼¯çˆµâ€çš„äººé€‰ï¼Œè€Œåè€…åªæœ‰å¦å®šæƒï¼›ä¸ºäº†é¿å…å†²çªï¼Œåè€…ä¸€èˆ¬å‡äºˆä»¥è®¤å¯ã€‚åœ¨ä¸¤ä¸ªä»ªå¼çš„å…ˆåé¡ºåºä¸Šï¼Œæ•™å»·ä¹Ÿä¸å¸å›½è¾¾æˆäº†å¦¥åï¼šåœ¨æ„å¤§åˆ©ç­‰åœ°ï¼Œæˆäºˆä¸»æ•™â€”â€”ä¼¯çˆµä¸–ä¿—æƒåŠ›çš„ä»ªå¼åº”åœ¨æˆäºˆå®—æ•™ç¥æƒä¹‹å6ä¸ªæœˆä¸¾è¡Œï¼Œåœ¨æ—¥è€³æ›¼ï¼Œçš‡å¸åˆ™å¯ä»¥å…ˆä¸¾è¡Œä¸»æ•™â€”â€”ä¼¯çˆµçš„å™çˆµä»ªå¼ï¼Œæˆäºˆä¸»æ•™â€”â€”ä¼¯çˆµç›¸å£°ä¸–ä¿—æƒåŠ›çš„æƒæ–ã€‚ä¸¤ç§ä¸åŒçš„è§„å®šé€ æˆäº†ä¸¤ç§ä¸åŒçš„ç»“æœï¼šåœ¨æ„å¤§åˆ©ï¼Œæ•™çš‡çš„æƒåŠ›è¶Šæ¥è¶Šå¤§ï¼Œè€Œåœ¨æ—¥è€³æ›¼ï¼Œçš‡å¸çš„æƒåŠ›åˆ™å¤§äºæ•™çš‡çš„æƒåŠ›ã€‚ã€Šæ²ƒå°”å§†æ–¯åè®®ã€‹å®£å‘Šæ¿€çƒˆçš„å¯¹æŠ—å¾—ä»¥ç¼“å’Œã€‚å†å²è¯æ˜ï¼Œè¯¥åè®®æœ€å¤§å—ç›Šè€…æ˜¯ç½—é©¬æ•™å»·ï¼Œå®ƒè¿›ä¸€æ­¥åŠ å¼ºäº†æ•™å»·åœ¨æ•´ä¸ªå¤©ä¸»æ•™ä¼šä¸­çš„ç»Ÿæ²»åœ°ä½ã€‚ æ²ƒå°”å§†æ–¯ä¸»æ•™åº§å ‚å†…æ™¯ï¼ˆæ‹æ‘„äº2022å¹´8æœˆ2æ—¥ï¼‰ã€‚å…¶ä¸»ç¥­å›ä¸‹æ–¹å®‰æ”¾æœ‰åº·æ‹‰å¾·äºŒä¸–ç­‰çš‡å¸æˆ–ä¸»æ•™çš„çµæŸ©ã€‚åœ¨æ­¤ä¸æ•™å®—ç­¾è®¢ã€Šæ²ƒå°”å§†æ–¯åè®®ã€‹çš„äº¨åˆ©äº”ä¸–åˆ™å®‰è‘¬äºæ–½æ´¾å°”ä¸»æ•™åº§å ‚ æ ¼é‡Œé«˜åˆ©çš„æ”¹é©ä½¿æ•™çš‡åœ¨æ•™ä¼šæŒæ¡ç»å¯¹æƒåŠ›ï¼Œå»ºç«‹æ£®ä¸¥çš„ç­‰çº§åˆ¶åº¦ï¼Œä¾æ¬¡ä¸ºæ•™çš‡ã€æ¢æœºä¸»æ•™(å³çº¢è¡£ä¸»æ•™ï¼Œæ¢æœºå›¢æœ‰æƒé€‰ä¸¾ä¸‹ä¸€ä»»æ•™å®—ï¼Œæ•™å®—ä¹Ÿç”±æ¢æœºå›¢æˆå‘˜ä¸­é€‰å‡º)ã€å¤§ä¸»æ•™ã€ä¸»æ•™ã€æœ¬å ‚ç¥ç”«ã€åŠ©ç¥­ç­‰ã€‚æ¢æœºä¸»æ•™äº«æœ‰æ•™çš‡çš„é€‰ä¸¾æƒå’Œè¢«é€‰ä¸¾æƒã€‚æ­¤ç»„ç»‡ç»“æ„å»¶ç»­è‡³ä»Šã€‚åŒæ—¶ï¼Œä¸œç½—é©¬å¸å›½æ‰€æ§åˆ¶çš„ä¸œæ¬§ä¸æ‰¿è®¤æ•™çš‡æƒå¨ï¼Œè§†å›å£«å¦ä¸å ¡å®—ä¸»æ•™ä¸ºç²¾ç¥é¢†è¢–ã€‚ä¸œæ–¹æ•™ä¼šæœ‰ç€ä¸€å¥—ä¸è¥¿æ–¹æ•™ä¼šä¸åŒçš„ç¥­ç¥€ç¤¼ä»ªå’Œå¯¹ç¥å­¦çš„ç†è§£ã€‚æ‰€ä»¥ï¼Œå½“ç½—é©¬æ•™çš‡è¯•å›¾æˆä¸ºæ•´ä¸ªåŸºç£æ•™ä¸–ç•Œçš„æœ€é«˜é¢†è¢–æ—¶ï¼Œä¸œè¥¿æ–¹æ•™ä¼šçš„åˆ†è£‚åœ¨æ‰€éš¾å…ã€‚å…¬å…ƒ1054å¹´ï¼Œç½—é©¬æ•™çš‡åˆ©å¥¥ä¹ä¸–ä¸å›å£«å¦ä¸å ¡å®—ä¸»æ•™è‰²è·¯æ‹‰é‡Œä¹Œç›¸äº’é©é™¤å¯¹æ–¹çš„æ•™ç±ï¼Œä¸œè¥¿æ–¹æ•™ä¼šå½»åº•å†³è£‚ ã€‚ ä¸­ä¸–çºªå°å»ºç­‰çº§è´µæ— (aristocracy)ã€åƒ§ä¾£ (monk) å’Œå†œæ°‘ (peasant) è´µæ—åŒ…æ‹¬çš‡å¸ã€å›½ç‹ã€å°å»ºä¸»ã€éª‘å£«ï¼Œä¸»è¦ä»äº‹å†›äº‹æ´»åŠ¨ï¼Œä¿å«å’Œæ‰©å¼ å›½å®¶æˆ–å°åœ°çš„é¢†åœŸã€‚äº‹å®ä¸Šï¼Œä¸­ä¸–çºªçš„è´µæ—å°±æ˜¯éª‘å£«ï¼Œçš‡å¸å’Œå›½ç‹æ˜¯æœ€å¤§çš„éª‘å£«ï¼Œå°å»ºä¸»æ˜¯æ‹¥æœ‰å°åœ°å’Œçˆµä½çš„éª‘å£«ã€‚ç»å¤§å¤šæ•°æ™®é€šéª‘å£«æ²¡æœ‰å°åœ°ï¼Œä¸ºé¢†ä¸»æœåŠ¡ï¼ŒåŒæ—¶å—é¢†ä¸»ä¿æŠ¤ã€‚ä¸ºäº†è±¢å…»å¤§æ‰¹éª‘å£«ï¼Œå°å»ºä¸»å¿…é¡»æ®‹é…·åœ°å‰¥å‰ŠåŠ³åŠ¨è€…ã€‚ å°å»ºä¸»ä¸ºé˜²å¾¡éœ€è¦ï¼Œä¿®å»ºåŸå ¡ã€‚å°åœ°å±…æ°‘åœ¨æˆ˜ä¹±æ—¶å¯è¿›å…¥é¿éš¾ã€‚å‘¨è¾¹å±…æ°‘ä¸ºå¯»æ±‚ä¹±ä¸–ä¸­çš„å®‰å…¨ï¼Œå°†åœŸåœ°çŒ®ç»™åŸå ¡ä¸»ï¼Œç§°ä¸ºå°åœ°å±…æ°‘ã€‚å°å»ºä¸»åœŸåœ°ã€æƒåŠ›ã€å¨ä¿¡ç”±æ­¤ä¸æ–­æ‰©å¼ ã€‚å°å°å»ºä¸»æ¸´æœ›æˆ˜äº‰ï¼Œå–å¾—ç«£å·¥ï¼Œè·å¾—ä¸°å¯Œçš„æˆ˜åˆ©å“å’Œæ›´å¤§çš„å°åœ°ã€‚å¤§å°å»ºä¸»å¥”æ³¢äºå›½ç‹æˆ–çš‡å¸çš„å®«å»·ä¸å°åœ°ä¹‹é—´ï¼Œåœ¨å®«å»·ä¸­ä»–ä»¬æ˜¯è‡£å­ï¼Œåœ¨å„è‡ªçš„å°åœ°ï¼Œä»–ä»¬å´æ˜¯åç¬¦å…¶å®çš„â€œå›ä¸»â€ã€‚åŸå ¡ä¸­è®¾æœ‰æ³•åº­ï¼Œå¯ä»¥å¤„ç†å°åœ°å±…æ°‘ä¹‹é—´çš„çº çº·ï¼Œå¯¹å·çªƒã€æŠ¢åŠ«ç­‰è½»ç½ªï¼Œå¯åšå‡ºç½šæ¬¾ã€æ–ç½šã€é­ç½šã€æ‹˜ç¦ç­‰åˆ¤å†³ï¼›å›å›½ã€å‡¶æ€ç­‰é‡ç½ªï¼Œåˆ™é¡»ç”±å›½ç‹æˆ–çš‡å¸æŒ‡å®šçš„æ³•å®˜å®¡ç†ã€‚ å…¬å…ƒ10ä¸–çºªåï¼Œéšé©¬æŒ(ä¿æŠ¤æˆ˜é©¬ä¸æ˜“å—ä¼¤)å’Œé©¬é•«(åŒæ‰‹æ­¦å™¨)çš„æ™®éé‡‡ç”¨ï¼Œæˆ˜é©¬æˆä¸ºåç¬¦å…¶å®çš„æˆ˜äº‰æœºå™¨ã€‚å› æ­¤ï¼ŒéšæŠ€æœ¯å‘å±•ï¼Œå¯æŠ«æŒ‚ä¸æ­¥å…µç›¸åŒçš„é‡ç”²ï¼Œé…å¤‡é‡å‹æ­¦å™¨ï¼Œå¹¶æ‹¥æœ‰â€œé•¿æªå†²å‡»æˆ˜æœ¯â€çš„é‡éª‘å…µåœ¨å†›é˜Ÿä¸­æ¯”é‡è¶Šæ¥è¶Šå¤§ã€‚ ç†è®ºä¸Šï¼Œä»»ä½•æ‹¥æœ‰æˆ˜é©¬å’Œæ­¦å™¨è£…å¤‡ä¸”å—è¿‡åŸºç£æ•™æ´—ç¤¼çš„äººï¼Œæ— è®ºå‡ºèº«ï¼Œéƒ½å¯ä»¥è¢«è±¢å…»ä»–çš„å°å»ºä¸»åŠ å°ä¸ºéª‘å£«ï¼Œæˆä¸ºå°å»ºä¸»çš„å°è‡£ï¼ˆæ³•å›½æ–‡å­¦â€œæ­¦åŠŸæ­Œâ€ï¼‰ã€‚ä½†å¹³æ°‘å¾ˆéš¾ä¸ä»äº‹åŠ³ä½œè€Œä¸“é—¨è®­ç»ƒï¼Œå¹¶è´­ç½®æ˜‚è´µçš„éª‘å£«è£…å¤‡ã€‚æ•…éª‘å£«å¤šå‡ºèº«è´µæ—ï¼Œè‡ªå¹¼å­¦ä¹ éª‘æœ¯ï¼Œç»ƒä¹ ä½¿ç”¨å…µå™¨ã€‚å‡ ä¹æ¯ä¸€ä¸ªå¤§å°å»ºä¸»çš„åŸå ¡ä¸­éƒ½å¼€è®¾éª‘å£«å­¦æ ¡ã€‚å°å°å»ºä¸»å’Œéª‘å£«çš„å­å¼Ÿ7å²æ—¶ä¾¿è¢«é€åˆ°é‚£é‡Œæ¥å—æ•™è‚²ï¼Œä»ä¾ç«¥åšèµ·ï¼Œå…¼åšå®¶åŠ¡æˆ–é™ªä¼´å¤«äººã€‚12å²æ—¶å¼€å§‹ä¹ æ­¦ï¼Œæˆä¸ºåŸå ¡ä¸»çš„æ­£å¼ä¾ä»ï¼Œå¹¶éšå…¶ç‹©çŒï¼Œå‚åŠ æ¯”æ­¦å’Œå¾æˆ˜ã€‚å½“ä¸»äººç¡®è®¤ä»–å·²ç»å®Œå…¨å…·å¤‡äº†éª‘å£«çš„æ¡ä»¶æ—¶ï¼Œä¾¿ä¸¾è¡ŒåŠ å°ä»ªå¼ã€‚ä¸»æ•™é¦–å…ˆä¸ºå—å°éª‘å£«çš„çŸ›ã€å‰‘ã€ç›”ã€ç›¾ã€é©¬åˆºç­‰ç‰©å“ç¥ˆç¦ï¼Œæ¥ç€å—å°éª‘å£«æŠ«æŒ‚ä¸Šè¿™äº›è£…å¤‡ï¼Œæœ—è¯µç¥ˆç¥·è¯å’Œèª“è¯ï¼Œè¡¨ç¤ºæ‰¿æ‹…éª‘å£«ä¹‰åŠ¡ã€‚æœ€åï¼Œä¸»æ•™æˆ–åŸå ¡ä¸»èµ°åˆ°ä¸‹è·ªå—å°çš„éª‘å£«é¢å‰ï¼Œåœ¨å…¶é¢ˆéƒ¨æˆ–è‚©éƒ¨é‡æ‹ä¸€ä¸‹ï¼Œåº„é‡åœ°å®£å¸ƒâ€œæˆ‘ä»¥ä¸Šå¸çš„åä¹‰åŠ å°ä½ ä¸ºéª‘å£«â€ã€‚å½“æ–°éª‘å£«é…å‰‘æ—¶ï¼Œä¸»æ•™/åŸå ¡ä¸»ä¼šç”¨å‰‘è½»å‡»å…¶è„¸é¢Šï¼Œè¢«ç§°ä¸ºdubbingï¼Œå³é…éŸ³ã€‚ éª‘å£«ä»¬åœ¨åŸå ¡å†…æ¯”è¯•å‰‘æ³•ï¼ˆæ‹æ‘„äºç‘å£«ä¼¦èŒ¨å ¡ï¼Œ2022å¹´9æœˆ11æ—¥) æ—¥è€³æ›¼ç¥åœ£ç½—é©¬å¸å›½å»ºç«‹ä¹‹åï¼Œå¤§è§„æ¨¡å¯¹å¤–æˆ˜äº‰ç»“æŸï¼Œä½†éª‘å£«å°šæ­¦ä¹ ä¿—å¹¶æœªæ”¹å˜ï¼Œå°å»ºä¸»äº‰æƒå¤ºåˆ©å†²çªä»ç„¶å­˜åœ¨ï¼Œæ•…éª‘å£«é—´äº‰æ–—ç»§ç»­ã€‚ ä¸ºä¿æŠ¤å®¶æ—çš„è´¢äº§å’ŒæƒåŠ¿ï¼Œä½¿å°åœ°å’Œå°å·ä¸–ä»£ç›¸ä¼ ï¼Œå°å»ºä¸»å»ä¸–æ—¶åªèƒ½æŠŠå°åœ°å’Œçˆµä½ä¼ ç»™ä¸€ä¸ªå„¿å­ï¼Œå› æ­¤å‡ºç°äº†é•¿å­ç»§æ‰¿åˆ¶ï¼Œå³é•¿å­ç»§æ‰¿å°å»ºä¸»çš„çˆµä½å’Œå°åœ°ï¼Œå…¶ä»–çš„å„¿å­åªèƒ½é€‰æ‹©æˆä¸ºåƒ§ä¾£æˆ–éª‘å£«ã€‚åƒ§ä¾£å‡­å®¶æ—çš„åŠ¿åŠ›æ”¯æŒå‘å±•è¾ƒå¿«ï¼Œä¸€äº›äººå¯èƒ½æˆä¸ºä¸»æ•™ï¼Œç”šè‡³æ•™çš‡ï¼Œå…¶æˆå°±åˆä¼šå¤§å¤§æå‡å®¶æ—ç¤¾ä¼šåœ°ä½ï¼ˆå¦‚ç¾ç¬¬å¥‡æ—äº§ç”Ÿå››ä½æ•™çš‡ï¼‰ã€‚éª‘å£«åˆ™å¾€å¾€ä¸ºå¦ä¸€å°å»ºä¸»æœåŠ¡ï¼Œæˆä¸ºå°è‡£ã€‚æ—¢ä¸èƒ½æˆä¸ºåƒ§ä¾£åˆæ— ç¼˜å°è‡£çš„è´µæ—å­å¼Ÿï¼Œåˆ™å¾€å¾€æˆå¸®ç»“ä¼™åœ°åœ¨ç¤¾ä¼šä¸Šæ¸¸è¡ï¼Œæ‰“æ¶æŠ¢æ ï¼Œå¯¹ç¤¾ä¼šé€ æˆæå¤§ç ´åã€‚ä¸ºç»´æŠ¤ç¤¾ä¼šç§©åºï¼Œæ•™ä¼šè¿›è¡Œå¹²é¢„ï¼Œè¯•å›¾å¼•å¯¼ä»–ä»¬èµ°ä¸Šæ­£è·¯ï¼šå…¬å…ƒ11ä¸–çºªåæœŸï¼Œæˆç«‹äº†æ—¨åœ¨å°†çº¨ç»”å­å¼Ÿæ”¹é€ æˆä¸Šå¸å«å£«çš„æ•™ä¼šéª‘å£«å›¢ï¼Œå°†è¿™äº›æ¸¸æ‰‹å¥½é—²ã€æ¶ä¹ æˆæ€§çš„è´µæ—å­å¼ŸåŠ å°ä¸ºéª‘å£«ï¼Œå…¶å¿…é¡»æ”¹é‚ªå½’æ­£ï¼Œå®£èª“ä¿æŠ¤ç¥èŒäººå‘˜ã€ç©·äººã€å¦‡å¥³å’Œå„¿ç«¥ç­‰å¼±è€…ï¼Œæƒ©æ¶æ‰¬å–„ï¼Œæå«ä¸Šå¸çš„ç‹å›½ã€‚ åƒ§ä¾£åƒ§ä¾£åŒ…æ‹¬ä¿—é—´åƒ§ä¾£ï¼ˆclero secolareï¼‰å’Œéšä¿®åƒ§ä¾£ï¼ˆclero regolareï¼‰ä¸¤ç±»ï¼Œæœ€é«˜ä»£è¡¨ä¸ºç½—é©¬æ•™çš‡ã€‚ä¿—é—´åƒ§ä¾£åˆ†å¸ƒäºå„åœ°æ•™å ‚ï¼Œè¢«ç§°ä½œç¥ç”«ï¼Œæœä»å„åœ°ä¸»æ•™çš„é¢†å¯¼ï¼Œä¸æ°‘ä¼—ç›´æ¥æ¥è§¦ï¼Œå¸®åŠ©æ°‘ä¼—è§£å†³ä¸–ä¿—ç”Ÿæ´»é—®é¢˜ï¼Œå–å¾—æ°‘ä¼—ä¿¡ä»»ï¼Œå¼•å¯¼å…¶çµé­‚å¾—ä»¥æ•‘èµã€‚ä¸»æ•™ç”Ÿæ´»åœ¨åŸå¸‚ï¼Œæˆä¸ºä¸­ä¸–çºªåŸå¸‚çœŸæ­£ç»Ÿæ²»è€…ï¼Œä¸ä»…ç®¡ç†æ•™ä¼šè´¢äº§ï¼Œè¡Œä½¿å¸æ³•æƒï¼Œè¿˜æŒæ§å†›é˜Ÿã€‚ æ²ƒå°”å§†æ–¯æ•™å ‚åŠå…¶é™„å±å»ºç­‘æ¨¡å‹ï¼ˆæ‹æ‘„äº2022å¹´8æœˆ2æ—¥ï¼‰ã€‚æ•™å ‚åŒ—ä¾§é™„å±å»ºç­‘ä¸ºä¸»æ•™å®«æ®¿ï¼Œä¸æ•™å ‚ä¾§å»Šç›´æ¥ç›¸è¿ï¼Œä¸»æ•™ä¸ºåŸå¸‚å®é™…ç»Ÿæ²»è€… éšä¿®åƒ§ä¾£ç”Ÿæ´»åœ¨æ·±å±±è€æ—ä¸­çš„ä¿®é“é™¢ï¼Œè¢«ç§°ä½œä¿®é“å£«ï¼Œä»–ä»¬æœä»ä¿®é“é™¢é™¢é•¿çš„é¢†å¯¼ï¼Œä¸¥æ ¼åœ°éµå®ˆä¿®é“é™¢çš„è§„ç« ï¼Œç¥ˆç¥·ä¸Šå¸ï¼Œä»äº‹ç”Ÿäº§ï¼ŒæŠŠè‡ªå·±çš„ä¸€ç”Ÿå®Œå…¨å¥‰çŒ®ç»™ä¸Šå¸ã€‚åœ¨ä¸­ä¸–çºªæ—©æœŸçš„æ··æˆ˜ä¸­ï¼Œéšä¿®åƒ§ä¾£å¸®åŠ©äº†æ°‘ä¼—ï¼Œå¹¶ä¸ºä¿æŠ¤æ–‡åŒ–åšå‡ºäº†å·¨å¤§çš„è´¡çŒ®ï¼Œå› è€Œèµ¢å¾—äº†ä¿¡å¾’ä»¬çš„ä¿¡ä»»å’Œæ”¯æŒï¼Œä¿®é“é™¢çš„é‡è¦æ€§å’Œè´¢å¯Œæ€¥å‰§å¢é•¿ï¼Œéšä¿®åƒ§ä¾£çš„ç¤¾ä¼šåœ°ä½ä¹Ÿè¶Šæ¥è¶Šé«˜ã€‚æ§åˆ¶åƒ§ä¾£ç­‰çº§çš„é«˜çº§æ•™èŒä¸€èˆ¬ç”±è´µæ—å‡ºèº«çš„äººæ‹…ä»»ï¼Œå› ä¸ºåªæœ‰åœ¨æœ‰æƒåŠ¿çš„å®¶æ—çš„æ”¯æŒä¸‹ï¼Œæ•™å£«æ‰æœ‰å¯èƒ½å½“é€‰ä¿®é“é™¢é™¢é•¿ã€ä¸»æ•™å’Œæ•™çš‡ã€‚ å¾·å›½æ¯›å°”å¸ƒéš†ä¿®é“é™¢å†…å½¹å·¥(lay brother)èšä¼šçš„æˆ¿é—´ï¼Œç°å·²è¾Ÿä¸ºéŸ³ä¹å…ï¼ˆæ‹æ‘„äº2022å¹´6æœˆ5æ—¥ï¼‰ åƒ§ä¾£ç­‰çº§æœ¬åº”æŒæ§ç¥æƒï¼Œä½†å®é™…ä¸Šï¼Œå´åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ§åˆ¶ç€ä¸­ä¸–çºªæ¬§æ´²ç¤¾ä¼šçš„ä¸–ä¿—æƒåŠ›ï¼Œå®ƒä¸è´µæ—ç­‰çº§ä¸€èµ·æ„æˆäº†æ¬§æ´²å°å»ºä¸­ä¸–çºªçš„ç»Ÿæ²»é˜¶çº§ å†œæ°‘å†œæ°‘çš„ç»å¤§å¤šæ•°æ˜¯å†œå¥´ï¼Œæˆ˜ä¹±å¹´ä»£ä¸ºè‡ªä¿ä¸å¾—ä¸å¯»æ±‚å°å»ºä¸»æˆ–æ•™ä¼šä¿æŠ¤ï¼Œä¸§å¤±ä¸ªäººåœŸåœ°åŠäººèº«è‡ªç”±ï¼ŒåŠ³å½¹ä¸–ä»£ç›¸ä¼ (serfdom å†œå¥´åˆ¶)ï¼Œä½†äººèº«ä¸å¯ä¹°å–ã€‚ å°å»ºä¸»ã€ä¸»æ•™å’Œä¿®é“é™¢çš„åœŸåœ°è¢«åˆ†æˆè®¸å¤šåº„å›­ï¼Œç”±ä¸»ç®¡è´Ÿè´£ç®¡ç†ï¼Œä¸€èˆ¬åˆ†ä¸ºä¸‰ç±»ï¼š â€œé¢†ä¸»è€•åœ°â€ï¼ŒåŒ…æ‹¬ç”°åœ°ã€è‘¡è„å›­ã€æœå›­ç­‰ï¼Œç”±ä½ƒå†œï¼ˆå†œå¥´ï¼‰è€•ç§ï¼Œå…¶æ”¶è·å…¨éƒ¨å½’é¢†ä¸»æ‰€æœ‰ï¼› â€œä½ƒå†œè€•åœ°â€ï¼Œç”±å°å—ç”°åœ°å’Œè‘¡è„å›­ç»„æˆï¼Œä¹Ÿç”±ä½ƒå†œè€•ç§ï¼Œé¢†ä¸»ä»æ”¶æˆä¸­æŠ½å–ç§Ÿç¨ï¼› â€œå…¬åœ°â€ï¼Œç”¨ä½œç‰§åœºçš„è‰åœ°å’Œæ ‘æ—ï¼Œé¢†ä¸»å’Œä½ƒå†œéƒ½å¯ä»¥ä½¿ç”¨ï¼Œä½†ä½ƒå†œè¦æ”¯ä»˜ä¸€å®šçš„ä½¿ç”¨è´¹ã€‚ é™¤ä¸ºé¢†ä¸»æ— å¿åŠ³ä½œå¹¶ç¼´ç¨å¤–ï¼Œä½ƒå†œè¿˜è¦æ‰¿æ‹…å…¶ä»–åç›®ç¹å¤šçš„èµ‹ç¨å’ŒåŠ³å½¹ï¼Œå¦‚ï¼šæˆ˜äº‰ç¨ã€å©šå«ç¨ã€ç£¨æˆ¿ç¨ã€å…»è·¯ç¨ã€è¿‡æ¡¥ç¨ã€ç æŸ´ç¨ã€é—äº§ç¨ç­‰ã€‚æ­¤å¤–ï¼Œè¿˜è¦å‘æ•™ä¼šç¼´çº³â€œä»€ä¸€ç¨â€ï¼ˆæ¬§æ´²åŸºç£æ•™ä¼šä¾æ®ã€Šåœ£ç»ã€‹æ‰€è½½å†œç‰§äº§å“ååˆ†ä¹‹ä¸€â€œå±äºä¸Šå¸â€çš„è¯´æ³•ï¼Œå‘å±…æ°‘å¾ç¨ï¼‰ã€‚ é™¤å†œå¥´å¤–ï¼Œå°å»ºä¸»çš„é¢†åœ°ä¸Šè¿˜ç”Ÿæ´»ç€è®¸å¤šå¥´éš¶ï¼Œæ²¡æœ‰äººèº«è‡ªç”±ï¼Œæ˜¯å°å»ºä¸»çš„ç§äººè´¢äº§ã€‚å¥´éš¶å¯ä»¥å¨¶å¦»ç”Ÿå­ï¼Œæˆç«‹å®¶åº­ï¼Œä½†ä»–ä»¬æœ¬äººã€å­å¥³ã€ç”Ÿäº§å·¥å…·ï¼Œä¹ƒè‡³ç”Ÿæ´»ç”¨å“éƒ½å½’å°å»ºä¸»æ‰€æœ‰ã€‚ä¸­ä¸–çºªå¥´éš¶çš„ç”Ÿå­˜æ¡ä»¶æ¯”å¤ç½—é©¬å¥´éš¶è¦å¥½ä¸€äº›ï¼Œä¸€èˆ¬éƒ½æ¥å—è¿‡åŸºç£æ•™æ´—ç¤¼ã€‚ç”±äºæ•™ä¼šç¦æ­¢è™å¾…åŸºç£å¾’ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…äº†å¥´éš¶å¤„å¢ƒçš„æ¶åŒ–ã€‚å¥´éš¶åˆ†ä¸ºâ€œå±…å®¶å¥´â€å’Œâ€œå¥´ä»†â€ä¸¤ç±»ã€‚â€œå±…å®¶å¥´â€ä¸ºå°å»ºä¸»è€•ç§åœŸåœ°ï¼ŒæœåŠ³å½¹ï¼Œå°å»ºä¸»ä¸ºä»–ä»¬æä¾›ä½æˆ¿å’ŒåœŸåœ°ï¼Œä»–ä»¬çš„ç”Ÿå­˜æ¡ä»¶ä¸ä½ƒå†œç›¸å·®æ— å‡ ã€‚â€œå¥´ä»†â€ä¸€èˆ¬ç”Ÿæ´»åœ¨å°å»ºä¸»çš„å®¶ä¸­ï¼Œä¸»è¦æœä¾å°å»ºä¸»åŠå…¶å®¶äººã€‚æœ‰äº›å¿ƒè…¹å¥´ä»†è¿˜ä¼šè¢«å°å»ºä¸»ä»»å‘½ä¸ºåº„å›­ä¸»ç®¡ï¼Œä»£å…¶ç®¡ç†åº„å›­äº‹åŠ¡ã€‚ ä¸­ä¸–çºªæˆ˜ä¹±ä¸æ–­ï¼Œæµ·ç›—å’Œå±±è´¼è‚†è™ã€‚å› è¿è¾“å›°éš¾ï¼Œé£é™©å¤§ï¼Œè¿œé€”è´¸æ˜“åŸºæœ¬ä¸­æ–­ã€‚å°åœ°çš„ç»æµå‡ ä¹æ˜¯å°é—­çš„ï¼Œå”¯ä¸€çš„å•†å“äº¤æ¢æ´»åŠ¨æ˜¯å°åœ°ä¸Šçš„é›†å¸‚è´¸æ˜“ï¼Œæ¢å–é£Ÿç›åŠé“åˆ¶å†œå…·ç­‰ç”Ÿæ´»å’Œç”Ÿäº§èµ„æ–™ã€‚ æ–¯ç‰¹æ‹‰æ–¯å ¡æ•™å ‚é™„è¿‘çš„é›†å¸‚ (æ‹æ‘„äº2021å¹´12æœˆ26æ—¥ï¼‰ æœ‰å…³å†œä¸š/é›†å¸‚çš„çŸ¥è¯†è¡¥å…… å¾·å›½ï¼Œå…ˆæœ‰æ•™å ‚åæœ‰åŸå¸‚ã€‚ç¥ç½—å‘ä¸œæ‰©å¼ ï¼Œå¼ºåˆ¶ä¼ æ•™ã€‚è¯¸ä¾¯èµèµåœŸåœ°äºæ•™å£«ï¼Œå»ºç«‹ä¼ æ•™æ®ç‚¹ï¼ˆä¸»æ•™é¢†åœ°ï¼‰ã€‚æ ¸å¿ƒä½ç½®å»ºè®¾å°æ•™å ‚ï¼Œå‘¨è¾¹åœŸåœ°åˆ†ç»™æ„¿æ„å¼€å¦çš„å†œæ°‘ï¼Œå¹¶æ”¶å–ç²®é£Ÿä¸ºç”°ç§Ÿã€‚æ•™ä¼šè·å¾—æ”¶å…¥åï¼Œæ‹›å‹ŸçŸ³åŒ ã€æœ¨åŒ è¿›ä¸€æ­¥æ‰©å»ºæ•™å ‚ï¼Œé€æ¸ï¼Œé™„è¿‘å‡ºç°é“åŒ é“ºï¼Œä»“åº“ï¼Œè´§åœºï¼Œå°é…’é¦†ç­‰ã€‚æœåœ£è€…ã€æ—…æ¸¸è€…å’Œè¡Œå•†ä¹Ÿå¼€å§‹å‡ºç°ï¼ŒåŸå¸‚è¯ç”Ÿã€‚ å¾·å›½çš„çŠä¸å…¶ä»–å›½å®¶ä¸åŒã€‚10ä¸–çºªï¼Œé‡çŠå‘æ˜ï¼Œä¾¿äºæ·±è€•ï¼Œå¾·æ„å¿—äººç»ˆäºæ‰¾åˆ°é€‚åˆè‡ªå·±åœŸå£¤å’Œæ°”å€™çš„è€•ä½œæ–¹å¼ã€‚ä½†çŠåˆ€å¤ªæ·±ï¼Œéœ€è¦4å¤´ç‰›ï¼Œæ•…æ‘æ°‘é›†ä½“åŠ³ä½œã€‚ä¸€æ¡å„æ²Ÿè¿ç»µï¼Œå„äººç”°åœ°äº§æƒæ— æ³•åˆ†æ¸…ï¼Œå› æ­¤é›†ä½“åŠ³ä½œã€åå•†å¹¶ç”±æ‘é‡Œå¾·é«˜æœ›é‡çš„è€äººå†³æ–­ã€‚è‡ªæ­¤ï¼Œå¾·å›½é€æ¸å½¢æˆäº†å°Šé‡æƒå¨ï¼Œæœä»é›†ä½“ï¼Œé‡è§†åä½œçš„ä¼ ç»Ÿã€‚ ä¸­ä¸–çºªåŸºç£æ•™æ–‡åŒ–çŠ¶å†µæ–‡åŒ–æŒæ¡åœ¨è´µæ—ï¼ˆçš‡å¸ã€å›½ç‹ã€å°å»ºä¸»å’Œéª‘å£«ï¼‰ä¸åƒ§ä¾£ä¸¤å¤§ç»Ÿæ²»é˜¶çº§æ‰‹ä¸­ã€‚æ”¿æ²»å’Œå†›äº‹çš„ä¸­å¿ƒåœ¨åŸå ¡ï¼Œæ–‡åŒ–ä¸­å¿ƒåœ¨ä¿®é“é™¢ã€‚æˆ˜ç«ä½¿å¾—ä¸­ä¸–çºªçš„åŸå¸‚ä¸§å¤±å¤ç½—é©¬æ—¶æœŸçš„ç¹è£ï¼Œä½äºä¹¡æ‘å’Œå±±åŒºçš„åŸå ¡ä¸ä¿®é“é™¢ä¸­æ¶Œå…¥å¹¸å­˜ä¸‹æ¥çš„å¸‚æ°‘ï¼Œä»¥è·å–ä¿æŠ¤ã€‚ä¸»æ•™å´ä¾ç„¶åšå®ˆåœ¨åŸå¸‚çš„æ•™å ‚ä¸­ï¼Œåœ¨è‡ªå·±ç®¡è¾–çš„æ•™åŒºé‡Œç»§ç»­è¡Œä½¿æƒåŠ›ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…äº†åŸå¸‚çš„ç­äº¡ã€‚æ­¤æ—¶çš„åŸå¸‚å¾’æœ‰å…¶åï¼Œå®é™…ä¸Šå·²ç»ä¹¡æ‘åŒ–äº†ï¼ˆä¸€èˆ¬ä»…æœ‰ä¸¤ä¸‰åƒå±…æ°‘ï¼‰ã€‚ åŸå ¡å†…çš„è´µæ—å°šæ­¦è½»æ–‡ï¼Œä¿®é“é™¢æˆä¸ºæ–‡åŒ–ä¼ æ’­çš„ä¸­å¿ƒã€‚æ‰€æœ‰æ¸´æœ›è·å¾—æ–‡åŒ–çŸ¥è¯†çš„äººéƒ½è¦åœ¨ä¿®é“é™¢é‡Œå­¦ä¹ ï¼Œæ¥å—åŸºç£æ•™çš„æ–‡åŒ–æ•™è‚²ï¼ŒåŒ…æ‹¬æ‹‰ä¸è¯­ã€å†å²ã€ç®—æœ¯ã€å‡ ä½•åŠå„ç±»è‡ªç„¶ç§‘å­¦ï¼Œå¦‚æ¤ç‰©å­¦ã€çŸ¿ç‰©å­¦ã€åŒ»å­¦ç­‰ã€‚çš‡å¸ã€å›½ç‹å’Œå°å»ºä¸»èº«è¾¹çš„æ–‡èŒå®˜å‘˜ä¹Ÿå¿…ç„¶ç”±æŒæ¡æ–‡åŒ–çš„åƒ§ä¾£æ‹…ä»»ã€‚ æ•™ä¼šå®Œå…¨æ§åˆ¶æ–‡åŒ–ï¼Œä½¿å…¶å½»åº•åœ°åŸºç£æ•™åŒ–ã€‚æ•™ä¼šç¯¡æ”¹ç¿çƒ‚è¾‰ç…Œçš„å¤å¸Œè…Šå’Œå¤ç½—é©¬æ–‡åŒ–ï¼Œä¸ºä¼ æ•™æœåŠ¡ï¼Œç¥å­¦ä¹Ÿé¡ºç†æˆç« åœ°æˆä¸ºè‡³é«˜æ— ä¸Šçš„å­¦ç§‘ï¼Œâ€œä¿¡åˆ™æ˜â€æˆä¸ºä¸­ä¸–çºªå“²å­¦æ€æƒ³çš„åŸºç¡€ã€‚åŸºç£æ•™ä¼šå¤§åŠ›å®£æ‰¬ä¸Šå¤©é‡äºäººé—´ã€æ¥ä¸–é‡äºä»Šç”Ÿã€çµé­‚é‡äºè‚‰ä½“çš„ç¥ç§˜ä¸»ä¹‰ åœ£äº‹ï¼ˆsacramentoï¼‰äº¦ç§°â€œåœ£ç¤¼â€ï¼Œæ˜¯åŸºç£æ•™çš„é‡è¦ç¤¼ä»ªã€‚å¤©ä¸»æ•™å’Œä¸œæ­£æ•™éƒ½è®¤ä¸ºâ€œåœ£äº‹â€å…±æœ‰7ä»¶ï¼Œå³æ´—ç¤¼ã€åšæŒ¯ã€å‘Šè§£ã€åœ£ä½“åœ£äº‹ï¼ˆåœ£é¤ï¼‰ã€ç»ˆå‚…ã€æˆç¥èŒç¤¼å’Œå©šé…ã€‚æ–°æ•™ä¸€èˆ¬åªæ‰¿è®¤â€œæ´—ç¤¼â€å’Œâ€œåœ£é¤â€ä¸ºåœ£äº‹ã€‚ æ´—ç¤¼äº¦ç§°åœ£æ´—ï¼Œæ˜¯åŸºç£æ•™çš„å…¥æ•™ä»ªå¼ï¼Œåˆ†ä¸ºâ€œæ³¨æ°´æ´—ç¤¼â€å’Œâ€œæµ¸ç¤¼â€ä¸¤ç§ã€‚è¡Œæ³¨æ°´æ´—ç¤¼æ—¶ï¼Œä¸»æŒæ´—ç¤¼è€…åœ¨å—æ´—è€…é¢ä¸Šæ´’å°‘è®¸æ¸…æ°´ï¼Œæ°´é¡ºé¢å¤´æµä¸‹ï¼ŒåŒæ—¶å¿µè¯µè§„å®šçš„æ´—ç¤¼è¯ã€‚è¡Œæµ¸ç¤¼æ—¶ï¼Œä¸»æŒæ´—ç¤¼è€…å¿µè¯µè§„å®šçš„æ´—ç¤¼è¯ï¼Œå¼•é¢†å—æ´—è€…å…¨èº«æµ¸æ³¡åœ¨æ°´æ± ä¸­ç‰‡åˆ»ã€‚æ—©æœŸåŸºç£æ•™ä¼šç»å¸¸ä¸¾è¡Œâ€œæµ¸ç¤¼â€ï¼Œæœ‰äº›å¤è€çš„æ•™å ‚é™„è¿‘è¿˜è®¾æœ‰ä¸“é—¨çš„æµ¸ç¤¼æ± ã€‚8ä¸–çºªä»¥åï¼Œå› â€œæµ¸ç¤¼â€å¾ˆä¸æ–¹ä¾¿ï¼Œå°¤å…¶å¯¹ä½“å¼±ç—…æ®‹è€…æ›´æ˜¯å›°éš¾ï¼Œè¥¿éƒ¨æ•™ä¼šè¡Œâ€œæ³¨æ°´æ´—ç¤¼â€è€…æ—¥å¢ï¼Œ12ä¸–çºªä»¥åï¼Œå¤©ä¸»æ•™ä¼šå…¨éƒ¨é‡‡ç”¨â€œæ³¨æ°´æ´—ç¤¼â€ã€‚ä¸œæ­£æ•™å’Œæ–°æ•™çš„æŸäº›æ•™æ´¾ï¼ˆå¦‚â€œæµ¸ç¤¼ä¼šâ€ç­‰ç»„ç»‡ï¼‰è‡³ä»Šä»ç„¶ä¸¾è¡Œâ€œæµ¸ç¤¼â€ã€‚åŸºç£æ•™è®¤ä¸ºï¼Œæ´—ç¤¼ï¼ˆBattesimoï¼‰æ˜¯è€¶ç¨£åŸºç£å®šç«‹çš„åœ£äº‹ï¼Œå®ƒå¯ä»¥èµ¦å…å…¥æ•™è€…çš„â€œåŸç½ªâ€å’Œâ€œæœ¬ç½ªâ€ï¼ˆpeccato personaleåˆç§°â€œç°çŠ¯ç½ªâ€ï¼Œæ„ä¸ºé™¤â€œåŸç½ªâ€ä¹‹å¤–ï¼Œäººä»¬è¿èƒŒä¸Šå¸æ„å¿—æ‰€çŠ¯ä¸‹çš„ç½ªï¼‰ï¼Œèµ‹äºˆå…¶â€œæ©å® â€å’Œâ€œå°å·â€ï¼Œä½¿å…¶æˆä¸ºæ•™å¾’ï¼Œå¹¶æœ‰æƒé¢†å—å…¶ä»–åœ£äº‹. åšæŒ¯ â€œåšæŒ¯â€ï¼ˆConfermazioneï¼‰äº¦ç§°â€œåšæŒ¯ç¤¼â€æˆ–â€œåšä¿¡ç¤¼â€ï¼Œæ˜¯åŸºç£æ•™çš„é‡è¦åœ£äº‹ä¹‹ä¸€ã€‚æ´—ç¤¼åç»è¿‡ä¸€æ®µæ—¶é—´ï¼Œå…¥æ•™è€…è¦æ¥å—â€œåšæŒ¯ç¤¼â€ï¼Œè¿™æ˜¯ä¸€ç§â€œæŒ‰æ‰‹ç¤¼â€ï¼Œç”±ä¸»æ•™æ–½è¡Œã€‚è¡Œç¤¼æ—¶ï¼Œä¸»æ•™æŠŠæ‰‹æŒ‰åœ¨é¢†å—è€…å¤´ä¸Šï¼Œå¿µè¯µè§„å®šçš„ç¤¼ä»ªè¯ã€‚æ•™ä¼šè®¤ä¸ºï¼Œè¯¥ç¤¼ä»ªèƒ½å¤Ÿä½¿â€œåœ£çµâ€é™ä¸´è‡³å—ç¤¼è€…çš„èº«ä¸Šï¼Œä»¥åšå®šå…¶ä¿¡ä»°ï¼ŒæŒ¯å¥‹å…¶çµé­‚ï¼Œæ•…ç§°å…¶ä¸ºâ€œåšæŒ¯â€ã€‚æ®ã€Šä½¿å¾’è¡Œä¼ ã€‹è®°è½½ï¼Œåœ£å½¼å¾—ã€åœ£çº¦ç¿°å’Œåœ£ä¿ç½—éƒ½æ›¾ä¸ºä¿¡å¾’è¡Œè¿‡â€œæŒ‰æ‰‹ç¤¼â€ï¼Œä½¿ä»–ä»¬å¾—åˆ°â€œåœ£çµâ€ã€‚ä¸»æ•™è¢«è®¤ä¸ºæ˜¯ä½¿å¾’çš„ç»§æ‰¿äººï¼Œæ•…â€œæŒ‰æ‰‹ç¤¼â€ä¸€èˆ¬åº”ç”±ä¸»æ•™æ–½è¡Œã€‚æ–°æ•™æœ‰äº›å®—æ´¾ä¹Ÿè¡Œâ€œåšæŒ¯ç¤¼â€ï¼Œä½†å¹¶ä¸è®¤ä¸ºè¯¥ç¤¼ä»ªæ˜¯åŸºç£äº²è‡ªè®¾ç«‹ï¼Œæ•…ä¸ç§°å…¶ä¸ºåœ£äº‹ã€‚ 3.å‘Šè§£ äº¦ç§°â€œåŠç¥å·¥â€ï¼Œä¿—ç§°â€œå¿æ‚”â€ã€‚åŸºç£æ•™è®¤ä¸ºï¼Œä¸ºäº†èµ¦å…æ•™å¾’é¢†æ´—åå¯¹ä¸Šå¸æ‰€çŠ¯ä¸‹çš„ç½ªè¿‡ï¼Œä½¿ä»–ä»¬é‡æ–°è·å¾—ä¸Šå¸çš„æ©å® ï¼Œè€¶ç¨£åŸºç£äº²è‡ªå®šç«‹äº†è¿™ä¸€åœ£äº‹ã€‚è¡Œå‘Šè§£ï¼ˆConfessioneï¼‰ç¤¼æ—¶ï¼Œæ•™å¾’å‘ç¥ç”«è¯´æ˜å¯¹ä¸Šå¸æ‰€çŠ¯ä¸‹çš„ç½ªè¿‡ï¼Œå¹¶è¡¨ç¤ºå¿æ‚”ï¼›ç¥ç”«åˆ™åº”å¯¹æ•™å¾’çš„å¿æ‚”ä¿å¯†ï¼Œå¹¶å‘æ•™å¾’é˜æ˜å¦‚ä½•è¡¥èµç½ªè¿‡ï¼Œå–å¾—ä¸Šå¸çš„å®½èµ¦ã€‚ 4.åœ£ä½“åœ£äº‹ äº¦ç§°â€œç¥äº¤åœ£ç¤¼â€ã€‚å¤©ä¸»æ•™ç§°å…¶ä¸ºâ€œåœ£ä½“åœ£äº‹â€ï¼ˆEucarestiaï¼‰ï¼Œå…¶ç¤¼ä»ªè¢«ç§°ä½œâ€œå¼¥æ’’â€ï¼ˆMessaï¼‰ï¼›ä¸œæ­£æ•™ç§°å…¶ä¸ºâ€œåœ£ä½“è¡€â€ï¼ˆDivina Liturgiaï¼‰ï¼›æ–°æ•™ç§°å…¶ä¸ºâ€œåœ£é¤â€ï¼ˆSanta cenaï¼‰ã€‚æ®ã€Šåœ£ç»Â·æ–°çº¦ã€‹è®°è½½ï¼Œè€¶ç¨£åœ¨ä¸ä½¿å¾’å…±è¿›æœ€åæ™šé¤æ—¶ï¼Œå¯¹é¢é¥¼å’Œçº¢é…’åšäº†ç¥ç¥·ï¼Œç„¶åå°†å…¶åˆ†ç»™ä½¿å¾’é£Ÿç”¨ï¼Œç§°é¢é¥¼æ˜¯è‡ªå·±çš„èº«ä½“ï¼Œçº¢é…’æ˜¯è‡ªå·±çš„è¡€æ¶²ï¼Œè±¡å¾ä»–ä¸ºäº†å…é™¤ä¼—äººçš„ç½ªè¿‡è€Œèˆå¼ƒè‡ªå·±çš„èº«èº¯å’Œé²œè¡€ï¼Œå¹¶å‘½ä»¤å…¶åä¸–é—¨å¾’æ–½æ­¤ç¤¼ä»ªï¼Œä»¥çºªå¿µä»–ä¸ºè§£æ•‘äººç±»è€ŒçŒ®èº«ã€‚è¡Œæ­¤ç¤¼ä»ªæ—¶ï¼Œä¸»æŒç¤¼ä»ªçš„ç¥èŒäººå‘˜é¦–å…ˆç¥ç¥·é¢é¥¼å’Œè‘¡è„é…’ï¼Œç„¶åç”±ä¿¡å¾’é¢†é£Ÿã€‚ 5.ç»ˆå‚… åœ¨æ•™å¾’ä¸´ç»ˆå‰ï¼Œç¥ç”«ç”¨ä¸»æ•™ç¥åœ£è¿‡çš„æ©„æ¦„æ²¹æ¶‚æŠ¹å…¶è€³ã€ç›®ã€å£ã€é¼»å’Œæ‰‹è¶³ï¼Œå¹¶è¯µå¿µä¸€æ®µç¥ˆç¥·ç»æ–‡ã€‚åŸºç£æ•™è®¤ä¸ºï¼Œé€šè¿‡ç»ˆå‚…ï¼ˆEstrema Unzioneï¼‰å¯ä»¥èµ‹æ©å® äºå—å‚…è€…ï¼Œå‡è½»å…¶èº«ä½“ä¸ç²¾ç¥çš„ç—›è‹¦ï¼Œèµ¦å…å…¶ç½ªè¿‡ã€‚ 6.æˆç¥èŒç¤¼ äº¦ç§°â€œæˆåœ£èŒç¤¼â€æˆ–â€œæ´¾ç«‹ç¤¼â€ï¼ˆOrdinazioneï¼‰ã€‚åŸºç£æ•™ä¼šå¯¹ç¥èŒäººå‘˜æˆäºˆç¥èŒæ—¶æ‰€ä¸¾è¡Œçš„ç¤¼ä»ªï¼Œå„æ•™æ´¾æ‰€é‡‡ç”¨çš„æ–¹å¼ä¸å°½ç›¸åŒã€‚ 7.å©šé… åœ¨æ•™å ‚ä¸­ï¼Œç”±ç¥ç”«ä¸»æŒï¼ŒæŒ‰ç…§æ•™ä¼šçš„è§„å®šä¸¾åŠç»“å©šä»ªå¼ï¼ˆmatrimonioï¼‰ã€‚ä»ªå¼çš„ä¸»è¦å†…å®¹æœ‰ï¼šç¥ç”«è¯¢é—®ç”·å¥³åŒæ–¹æ˜¯å¦æ„¿æ„ç»“ä¸ºå¤«å¦»ï¼›å¾—åˆ°è‚¯å®šçš„å›ç­”ä¹‹åï¼Œç¥ç”«è¯µå¿µè§„å®šçš„ç¥ˆç¥·ç»æ–‡ï¼Œå¹¶æ ¹æ®ã€Šç›çª¦ç¦éŸ³ã€‹ä¸­çš„ä¿¡æ¡å®£å¸ƒï¼šâ€œå‡¡å¤©ä¸»æ‰€ç»“åˆçš„ï¼Œäººä¸å¯æ‹†æ•£â€ï¼Œ(11)ç„¶åå‘å©šå§»åŒæ–¹ç¥ç¦ã€‚æ–°æ•™ä¸è§†å…¶ä¸ºåœ£äº‹ï¼Œä½†æœ‰é‚€è¯·ç‰§å¸ˆè¯å©šçš„ä¹ æƒ¯. â€“ ç›¸å…³é“¾æ¥Map of the Ancient World - World History Encyclopedia FOLLOWING HADRIAN Hadrianâ€™s travels What was the ceremony for making a knight?","link":"/2022/03/01/25.%5BReading%5D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%84%8F%E5%A4%A7%E5%88%A9%E6%96%87%E5%8C%96%E7%AE%80%E5%8F%B2/"},{"title":"Create your first web app: Interactive data panel for visualizatoin correlations","text":"In this tutorial, I will demonstrate how to create a simple, interactive web app that visualizes correlations between variables. We will also walk you through the necessary environment setup and deployment steps. Environment SetupFirst, create a new conda environment and install the required packages: 1234conda create --name new_panel_env python=3.8conda activate new_panel_env## those package version are tested by myself with no conflictconda install -c conda-forge panel bokeh==2.4.3 holoviews==1.14.8 pandas==1.2.4 hvplot==0.8.2 Plotting Script123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import panel as pnimport holoviews as hvimport pandas as pdimport numpy as npimport hvplot.pandashv.extension('bokeh')# 1. Read the datadf_merge = pd.read_csv(\"./data/df_merge.csv\")factor_name = [c for c in df_merge.columns if 'factor_' in c]var_ = factor_name + [...]# Replace 'if tag bellow after columns definitiondf_var = df_merge[var_]df_var = df_var.replace('&lt;LD', np.nan)df_var = df_var.replace('&lt;DL', np.nan)df_var = df_var.dropna(axis=1, how='all')df_corr = df_var.corr()corr = df_corr # .abs()# Mask the upper triangle of the heatmapcorr.values[np.triu_indices_from(corr, 0)] = np.nantick_label = [...]# 2. Create heatmapheatmap = hv.HeatMap((corr.columns, corr.index, corr)) \\ .opts( tools=['hover'], height=1400, width=1400, fontsize=12, toolbar='above', colorbar=False, cmap='RdBu_r', invert_yaxis=True, xrotation=90, xlabel='', ylabel='', title='Correlation heatmap' )# Define tap stream with heatmap as sourcetap_xy = hv.streams.Tap(source=heatmap, x='As', y='K')# Calculate correlation plot based on tapdef tap_corrplot(x, y): # Drop missing values if there are any df_notnull = df_merge[['time_x', x, y]].dropna(how='any') scatter1 = hv.Scatter((df_notnull[x], df_notnull[y]), x, y, label='scatterplot') \\ .opts( tools=['hover'], height=700, width=900, fontsize=12, size=5, alpha=0.2, ylim=(df_notnull[y].min(), df_notnull[y].max()), color='#30a2da', framewise=True, ) timeseries1 = df_merge.hvplot(x='time_x', y=x, label=x, ylabel='Value').opts( height=700, width=900 ) timeseries2 = df_merge.hvplot(x='time_x', y=y, label=y, ylabel='Value').opts( height=700, width=900 ) return hv.Layout([scatter1, timeseries1 * timeseries2]).cols(1)tap_dmap = hv.DynamicMap(tap_corrplot, streams=[tap_xy])app_bar = pn.pane.Markdown(\"# Correlation Matrix Checker\")app = pn.Column( app_bar, pn.Spacer(height=10), pn.Row( heatmap, pn.Column( tap_dmap, ), ),)app.servable(title='Correlation matrix checker') Save the above code in a Python file named app.py. This app has a heatmap of correlations between variables, which serves as the backdrop for an interactive scatterplot and time series based on your selections. Running the Web AppFinally, to run your web app, open a terminal in the folder containing the app.py script and execute the following command: panel serve app.py --show Your web app will open in your default web browser with an interactive visualization panel showing a correlation matrix of the dataset, the scatter plots, and time series of two variables based on your selection. In summary, this tutorial showed you how to build a simple and interactive web app for visualizing correlations between variables. We covered the steps for setting up the appropriate environment, creating visualization scripts, and running the web application. You can use these techniques to analyze complex datasets and produce clear visualizations that aid in your understanding and interpretation of the data.","link":"/2023/08/09/29.%5BCode%5DWeb_App_Development_Interactive_Data_Panel/"},{"title":"è¯»ä¹¦ç¬”è®°-å¼ ç¬‘å®‡æ–‡æ˜ä¸‰éƒ¨æ›²","text":"ã€Šäº§ä¸šä¸æ–‡æ˜ã€‹ ç¬¬ä¸€ç« æ€»ç»“ä¸»é¢˜ï¼šæŠ€æœ¯æ”¹å˜ç¤¾ä¼šå¹¶éç›´æ¥çº¿æ€§å‘ç”Ÿï¼Œè€Œæ˜¯å¿…é¡»é€šè¿‡â€œæ¼æ–—â€”å–‡å­â€æ¨¡å‹çš„æ£€éªŒâ€”â€”å³äº§ä¸šåŒ–ã€å•†ä¸šåŒ–æ£€éªŒï¼Œæ‰èƒ½çœŸæ­£æ”¹å˜ä¸–ç•Œã€‚ æŠ€æœ¯æœ¬èº«æ— æ³•è‡ªåŠ¨æ”¹å˜ç¤¾ä¼šï¼Œå¿…é¡»ç»è¿‡â€œå•†ä¸šåŒ–/äº§ä¸šåŒ–â€æ¼æ–—ç­›é€‰ äº‹å®æ”¯æ’‘ï¼š å¸Œç½—å‘æ˜æ±½è½¬çƒã€è‡ªåŠ¨é—¨ã€è‡ªåŠ¨å”®è´§æœºç­‰è¶…å‰æœºæ¢°ï¼Œä½†å› ç¤¾ä¼šç¼ºä¹å•†ä¸šéœ€æ±‚ä¸äº§ä¸šåº”ç”¨ï¼Œä»…æˆä¸ºç¥åº™çš„å¥‡æŠ€å¼‚å·§ï¼Œæ— å¹¿æ³›åº”ç”¨ã€‚ å·´åŠªÂ·ç©†è¨ä¸‰å…„å¼Ÿã€åŠ æ‰åˆ©å‘æ˜è‡ªåŠ¨æ¼”å¥è£…ç½®ã€å¤§è±¡é’Ÿç­‰å¤æ‚æœºæ¢°ï¼Œä½†ä¾é™„ç¥èŒæœºæ„æˆ–æš´åŠ›é›†å›¢ï¼Œæœªèƒ½æ¨åŠ¨ç”Ÿäº§é©å‘½ã€‚ å¤ä»£ç¤¾ä¼šèµ„æºé›†ä¸­åœ¨æš´åŠ›é›†å›¢ä¸å®—æ•™é›†å›¢æ‰‹ä¸­ï¼Œç¼ºä¹å¹¿æ³›æ¶ˆè´¹èƒ½åŠ›ä¸é«˜åŠ³åŠ¨åŠ›æˆæœ¬ï¼Œå› æ­¤æ–°æŠ€æœ¯éš¾ä»¥æ™®åŠåº”ç”¨ã€‚ ç¤¾ä¼šç»æµç»“æ„å†³å®šäº†æŠ€æœ¯èƒ½å¦åº”ç”¨ä¸æ‰©æ•£ äº‹å®æ”¯æ’‘ï¼š å¤ç½—é©¬ã€ä¼Šæ–¯å…°ç¤¾ä¼šä¸­å¤§é‡å¥´éš¶åŠ³åŠ¨ä½¿èµ·é‡æœºç­‰æœºæ¢°å‘æ˜ç¼ºä¹å®é™…åº”ç”¨åœºæ™¯ã€‚ ä¸­å›½ç“·çª‘ï¼ˆæ–œä¸Šæ’æ°”ï¼‰é‡è§†èŠ‚çœç‡ƒæ–™è€Œå¿½ç•¥å·¥äººç¦ç¥‰ï¼Œåæ˜ å·¥äººåŠ³åŠ¨åŠ›æå…¶å»‰ä»·ã€‚ è‹±å›½ç“·çª‘ï¼ˆç›´ä¸Šæ’æ°”ï¼‰çƒ­é‡æµªè´¹å¤§ï¼Œä½†åŠ³åŠ¨åŠ›ä»·å€¼è¾ƒé«˜ï¼Œå·¥ä¸šé©å‘½å‰å¤œè‹±æ ¼å…°å·²å‡ºç°é«˜å·¥èµ„ç»æµæ¨¡å¼ã€‚ äº§ä¸šç¯å¢ƒå’Œç»æµæ¿€åŠ±ï¼Œæ˜¯æŠ€æœ¯å‘æ˜èƒ½å¦å¤§è§„æ¨¡åº”ç”¨çš„å…³é”® äº‹å®æ”¯æ’‘ï¼š è’¸æ±½æœºçš„æ™®åŠä¸æ˜¯å› ä¸ºç§‘å­¦ç†è®ºæœ¬èº«ï¼Œè€Œæ˜¯å› ä¸ºç…¤ç‚­è´µã€äººåŠ›è´µï¼Œè’¸æ±½æœºèƒ½èŠ‚çœæˆæœ¬ï¼Œç¬¦åˆå¸‚åœºéœ€æ±‚ã€‚ å·¥ä¸šé©å‘½çœŸæ­£å¯åŠ¨ï¼Œä¸é å•ä¸€ç§‘å­¦å‘ç°ï¼Œè€Œé å¤§é‡æ—¥å¸¸éœ€æ±‚å åŠ ï¼Œæ¿€åŠ±å·¥ç¨‹å¸ˆä¸æ–­æ”¹è‰¯ã€å¾®åˆ›æ–°ã€‚ çº½å¡é—¨ã€ç“¦ç‰¹èƒ½æˆåŠŸï¼Œæ˜¯å› ä¸ºä»–ä»¬æ”¹è¿›æŠ€æœ¯èƒ½åœ¨å½“æ—¶è·å¾—å®é™…ç›ˆåˆ©ï¼Œè€Œéä»…ä¾èµ–çš‡å®¤ã€å­¦é™¢èµåŠ©ã€‚ ç§‘æŠ€æ¨åŠ¨ç¤¾ä¼šæ¼”åŒ–ï¼Œå¿…é¡»è­¦æƒ•â€œç†æ€§ä¸»ä¹‰çš„è‡ªè´Ÿâ€ äº‹å®æ”¯æ’‘ï¼š å„’å®¶ç¤¾ä¼šã€å¤å¸Œè…Šç¤¾ä¼šéƒ½å­˜åœ¨è½»è§†å®ç”¨æŠ€æœ¯çš„æ–‡åŒ–ï¼Œä½†çœŸæ­£é˜»ç¢æŠ€æœ¯æ™®åŠçš„ï¼Œæ˜¯ç»æµä¸ç¤¾ä¼šç»“æ„ã€‚ 20ä¸–çºªè‹è”OGASè®¡åˆ’è¯•å›¾é€šè¿‡è®¡ç®—æœºè®¡åˆ’ç»æµæ”¹å˜ç¤¾ä¼šï¼Œä½†æŠ€æœ¯æœ¬èº«æ— æ³•æ‹¯æ•‘åƒµåŒ–ä½“åˆ¶ï¼ŒOGASå¤±è´¥ã€‚ çº³ç²¹åˆ©ç”¨å­Ÿå¾·å°”é—ä¼ å­¦å‘å±•ä¼˜ç”Ÿå­¦ï¼Œè¯´æ˜æŠ€æœ¯æœ¬èº«ä»·å€¼ä¸­æ€§ï¼Œå¦‚ä½•åº”ç”¨å–å†³äºç¤¾ä¼šç¯å¢ƒã€‚ è‡ªç”±å¸‚åœºæ˜¯æ‰©å¤§â€œæ¼æ–—â€å®½åº¦ã€ä¿ƒè¿›æ–°æŠ€æœ¯å­˜æ´»ä¸å‘å±•çš„æœ€ä½³æœºåˆ¶ äº‹å®æ”¯æ’‘ï¼š 17ä¸–çºªè‹±å›½é€šè¿‡è‡ªç”±è´¸æ˜“ã€å›½å€ºå¸‚åœºå»ºç«‹ä¿¡ç”¨ç§©åºï¼Œæ”¯æ’‘äº†èµ„æœ¬å¸‚åœºä¸é«˜ç§‘æŠ€äº§ä¸šçš„ç¹è£ã€‚ è‹±å›½ä»å°è§„æ¨¡å•†è´¸æ­£å¢é•¿å¼€å§‹ï¼Œé€æ¸ç§¯ç´¯å‡ºå¯ä»¥æ”¯æ’‘é«˜å·¥èµ„ã€é«˜æŠ•èµ„å›æŠ¥ç‡çš„äº§ä¸šç¯å¢ƒï¼Œä¸ºå·¥ä¸šé©å‘½æ‰“ä¸‹åŸºç¡€ã€‚ æ€»ä½“ç»“è®ºç§‘æŠ€åˆ›æ–°åœ¨å¤æ‚ç¤¾ä¼šä¸­æ— æ³•ç®€å•ç›´æ¥æ¨åŠ¨å˜é©ï¼Œåªæœ‰ç»è¿‡äº§ä¸šåŒ–ã€å•†ä¸šåŒ–ï¼ˆæ¼æ–—æ£€éªŒï¼‰ï¼Œç¬¦åˆç¤¾ä¼šå®é™…éœ€æ±‚ï¼Œæ‰èƒ½ä»æ— æ•°åˆ›æ–°ä¸­è„±é¢–è€Œå‡ºï¼Œå¼•å‘â€œå–‡å­æ•ˆåº”â€ï¼Œå¤§è§„æ¨¡æ‰©å±•ã€å½»åº•æ”¹å˜ç¤¾ä¼šã€‚ ç¬¬äºŒç« æ€»ç»“ä¸»é¢˜ï¼šè’¸æ±½æœºé©å‘½å¹¶éå¶ç„¶çˆ†å‘ï¼Œè€Œæ˜¯ç”±å•†ä¸šç¹è£ã€ç”Ÿæ´»ç»†èŠ‚éœ€æ±‚å˜åŒ–ã€è‡ªç„¶èµ„æºä¼˜åŠ¿å’Œç¤¾ä¼šæ¿€åŠ±æœºåˆ¶å…±åŒé©±åŠ¨ï¼Œåœ¨å¾®å°ç”Ÿæ´»å˜åŒ–ä¸­ç§¯ç´¯èµ·å·¨å¤§æŠ€æœ¯çªç ´åŠ¨åŠ›ã€‚ ç‡ƒæ–™å±æœºæ˜¯è’¸æ±½æœºé©å‘½çš„ç”Ÿæ´»æºå¤´ äº‹å®æ”¯æ’‘ï¼š 16ä¸–çºªï¼Œä¼¦æ•¦äººå£ç”±5.5ä¸‡äººå¢è‡³20ä¸‡äººï¼Œç‡ƒæ–™éœ€æ±‚æ¿€å¢ã€‚ æœ¨æŸ´ä¾›åº”å› è¿è¾“é™åˆ¶ï¼ˆæ³°æ™¤å£«æ²³ã€æ°´é™†è¿åŠ›ç“¶é¢ˆï¼‰è€Œä¸è¶³ï¼Œæœ¨æŸ´ä»·æ ¼é£™å‡ï¼Œæœ¨ç‚­ç”šè‡³æ¯”ç…¤ç‚­è´µä¸¤å€ã€‚ ç…¤ç‚­å› èƒ½é‡å¯†åº¦é«˜ã€è¿è¾“æ•ˆç‡é«˜ï¼Œé€æ¸æ›¿ä»£æœ¨æŸ´æˆä¸ºä¸»è¦ç‡ƒæ–™ï¼ˆ1550å¹´ä»¥åç…¤ä»·ä¼˜åŠ¿æ˜¾è‘—ï¼‰ã€‚ ç…¤ç‚­ä¾›ç»™çš„åœ°ç†å¶ç„¶æ€§ï¼Œå¥ å®šäº†è‹±å›½çš„èƒ½æºåŸºç¡€ äº‹å®æ”¯æ’‘ï¼š çº½å¡æ–¯å°”åœ°åŒºç…¤å±‚è£¸éœ²åœ°è¡¨ï¼Œå®¹æ˜“å¼€é‡‡ã€‚ 16ä¸–çºªä¸­åæœŸï¼Œçº½å¡æ–¯å°”ç…¤äº§é‡ä»15000å¨æ¿€å¢åˆ°1625å¹´çš„40ä¸‡å¨ã€‚ ç›ä¸½ä¸€ä¸–å–æ¶ˆè´µæ—å¯¹ç…¤çŸ¿çš„å„æ–­ï¼Œä¿ƒè¿›èµ„æœ¬æµå…¥å’Œç…¤ç‚­å¤§è§„æ¨¡å¼€å‘ã€‚ æ’æ°´éš¾é¢˜å‚¬ç”Ÿå¯¹æŠ½æ°´æŠ€æœ¯çš„åˆšéœ€ï¼Œç›´æ¥å¯¼è‡´è’¸æ±½æœºå‡ºç° äº‹å®æ”¯æ’‘ï¼š ç…¤çŸ¿é‡‡æ·±é‡åˆ°ä¸¥é‡çŸ¿äº•é€æ°´é—®é¢˜ï¼ˆå¦‚æ²³æ°´å€’çŒäº‹æ•…ï¼‰ã€‚ ä¼ ç»Ÿä¿®å»ºæ’æ°´æ¸ æ–¹æ³•é€æ¸å¤±æ•ˆï¼Œå¿…é¡»å¯»æ‰¾æ–°æŠ€æœ¯ã€‚ æ‰˜é©¬æ–¯Â·çº½å¡é—¨ï¼ˆç‰§å¸ˆå…¼é“åŒ ï¼‰1712å¹´å‘æ˜çº½å¡é—¨è’¸æ±½æœºï¼Œé¦–æ¬¡é€šè¿‡è’¸æ±½é©±åŠ¨æ´»å¡æŠ½æ°´ï¼Œè§£å†³äº†çŸ¿äº•æ’æ°´éš¾é¢˜ã€‚ ç§‘æŠ€åˆ›æ–°éœ€è¦é‡‘é’±æ¿€åŠ±ï¼Œè€Œä¸æ˜¯å•é çš‡å®¶å­¦ä¼šæˆ–è´µæ—èµåŠ© äº‹å®æ”¯æ’‘ï¼š çš‡å®¶å­¦ä¼šè™½æå‡ºçŸ¿äº•æ’æ°´éœ€è¦ç§‘æŠ€ï¼Œä½†æœªèƒ½å®é™…è§£å†³é—®é¢˜ã€‚ ç…¤è€æ¿å‡ºäºå®é™…åˆ©ç›Šéœ€æ±‚ï¼Œæ„¿æ„ä¸ºè’¸æ±½æœºæ”¯ä»˜é«˜é¢è´¹ç”¨ï¼Œå½¢æˆç¨³å®šå•†ä¸šæ¨¡å¼ï¼ˆæŒ‰ç…¤çŸ¿äº§èƒ½æ”¶å–é•¿æœŸä½¿ç”¨è´¹ç”¨ï¼‰ã€‚ 1733å¹´è‹±å›½å·²æœ‰100å°è’¸æ±½æœºï¼Œ1800å¹´å¢è‡³2500å°ã€‚ æ—¥å¸¸ç”Ÿæ´»éœ€æ±‚ï¼ˆå¦‚çƒ¹é¥ªï¼‰é©±åŠ¨äº†ææ–™é©å‘½ï¼Œä¸ºè’¸æ±½æœºæ”¹è¿›å¥ åŸº äº‹å®æ”¯æ’‘ï¼š ç…¤ç‚­ç«åŠ›å¼ºï¼Œä¼¦æ•¦å®¶åº­å¿…é¡»æ›´æ¢æ›´è€ç”¨ã€æ›´è½»è–„çš„é”…å…·ã€‚ äºšä¼¯æ‹‰ç½•Â·è¾¾æ¯”1709å¹´æˆåŠŸç”¨ç„¦ç‚­ç‚¼å‡ºç°é“¸é“ï¼Œé‡äº§è€ç”¨é”…å…·ã€‚ è¾¾æ¯”å®¶æ—å››ä»£æ¨åŠ¨äº†ä»é“¸é“é”…â†’å·¥ä¸šé“¸é“ææ–™â†’é“è·¯ã€æ¡¥æ¢å»ºè®¾çš„ææ–™é©å‘½ã€‚ ç“¦ç‰¹è’¸æ±½æœºçš„çªç ´æºäºèƒ½æºæ•ˆç‡é©å‘½ä¸ææ–™è¿›æ­¥çš„ç§¯ç´¯ äº‹å®æ”¯æ’‘ï¼š ç“¦ç‰¹1765å¹´å‘æ˜åˆ†ç¦»å¼å†·å‡å™¨ï¼Œè’¸æ±½æœºç…¤è€—é™ä½50%ã€‚ ä¾èµ–è¾¾æ¯”ç„¦ç‚­ç‚¼é“æŠ€æœ¯ï¼Œæ‰èƒ½åˆ¶é€ é«˜æ•ˆè€çƒ­æ°”ç¼¸ã€‚ ç“¦ç‰¹å¼è’¸æ±½æœº1780å¹´ä»£èµ·æ™®åŠï¼Œæˆä¸ºåŠ¨åŠ›æºï¼Œä¸å†å±€é™äºæŠ½æ°´ï¼Œæ‰©å±•åˆ°åˆ¶é€ ä¸šã€çººç»‡ä¸šã€‚ æ€»ä½“ç»“è®ºè’¸æ±½æœºé©å‘½ä¸æ˜¯å•ä¸€æŠ€æœ¯å¤©æ‰çš„äº§ç‰©ï¼Œè€Œæ˜¯ç”±èƒ½æºéœ€æ±‚ã€ç”Ÿæ´»ç»†èŠ‚ã€ææ–™é©å‘½ã€æ¿€åŠ±æœºåˆ¶å’Œåœ°ç†ä¼˜åŠ¿å¤šé‡å› ç´ åœ¨å¾®è§‚å±‚é¢é•¿æœŸç§¯ç´¯ä¸é€‰æ‹©ä¸‹è‡ªç„¶å­•è‚²çš„å¿…ç„¶ç»“æœã€‚","link":"/2024/10/11/31.%5BReading%5D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%BA%A7%E4%B8%9A%E4%B8%8E%E6%96%87%E6%98%8E/"},{"title":"OrbiTrack Dev Log 1: Overview and MS1 Data Parsing for Orbitrap","text":"The ultra-high resolution of Orbitrap mass spectrometry enables an unprecedented level of chemical detail. In my current research, we employ a direct-infusion Orbitrap approach (EESI inlet, Felipe et al, 2019) for non-targeted analysis. However, due to the lack of dedicated tools tailored for this application, I developed a new software package called OrbiTrack. This tool supports two core workflows: (1) TOF peak guidance: Guiding ion fitting in EESI-TOF using high-resolution chemical information acquired at 120k resolution from Orbitrap. (2) Directly utilization of Orbitrap MS1 data by integrating full time-series MS1 outputs, enabling online untargted molecular analysis without chromatographic sepeartion. This blog series will introduce the OrbiTrack framework step by step, covering the conceptual foundations and sharing the original scripts for each component: Raw data parsing (covered in this post) Dynamic m/z calibration link Ion filtering and clustering link Chemical formula assignment link TOF integration (optional, used when leveraging Orbitrap-resolved ions to support TOF peak fitting) link Feedback and suggestions are highly welcome. Feel free to leave a comment below or contact me via email. In this first post, I will introduce Part 1: loading Orbitrap raw MS1 data by user-defined time period. Raw data reading12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182## Module 1. MS reading def get_last_retention_time(file_path): \"\"\" Function to read a file and extract the last retention time. Args: - file_path (str): Path to the file. Returns: - last_retention_time (float): The last retention time found in the file. \"\"\" last_retention_time = None # Initialize as None with open(file_path, 'r') as f: for line in f: if line.startswith(\"I\"): # Look for the lines with retention time info parts = line.strip().split('\\t')[1:] if len(parts) == 2: key, value = parts if key == \"RetTime\": last_retention_time = float(value) # Update with the latest retention time return last_retention_timedef get_retention_time_indices(file_path, start_retention_time, end_retention_time): retention_time_indices = [] current_segment_index = -1 # Initialize segment index retention_time_value = [] with open(file_path, 'r') as f: for line in f: if line.startswith(\"S\"): # Check for segment start current_segment_index += 1 # Increment segment index elif line.startswith(\"I\"): parts = line.strip().split('\\t')[1:] if len(parts) == 2: key, value = parts if key == \"RetTime\": retention_time = float(value) if start_retention_time &lt;= retention_time &lt;= end_retention_time: retention_time_indices.append(current_segment_index) retention_time_value.append(retention_time) return retention_time_indices,retention_time_valuedef read_ms1_segments_between_retention_times(file_path, retention_time_indices): segments = [] current_segment = [] current_segment_index = -1 with open(file_path, 'r') as f: for line in f: if line.startswith(\"S\"): current_segment_index += 1 if current_segment_index in retention_time_indices: if current_segment: segments.append(current_segment) current_segment = [] elif line.strip() and current_segment_index in retention_time_indices: current_segment.append(line.strip().split()) if current_segment: segments.append(current_segment) return segmentsdef save_segments_as_dataframes(segments,thres): dataframes = [] for idx, segment in enumerate(segments): retention_time = float(segment[0][2]) df = pd.DataFrame(segment[3:], columns=[\"m/z\", \"intensity\", \"unknown1\", \"unknown2\"]) # Convert columns to numeric (excluding the first two columns) df.iloc[:, 2:] = df.iloc[:, 2:].apply(pd.to_numeric) # Keep only the first two columns df = df.iloc[:, :2] df['m/z'] = df['m/z'].astype(float) df['intensity'] = df['intensity'].astype(float) df = df[df['intensity']&gt;thres] df = df.reset_index(drop = True) df['rt'] = retention_time # Save the DataFrame dataframes.append(df) return dataframesdef merge_segments(dataframes): merged_df = pd.concat(dataframes, ignore_index=True) return merged_df 2. Data inputFor TOF peak list fitting, we incorporate the raw TOF spectrum output from Tofware. This dataset is used to: Evaluate the fitting accuracy of Orbitrap-assigned ions, and Supplement ions that may be missing or undetectable in Orbitrap. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ========== 1. INPUT FILE ==========file_path = \"./../../../../../Africa/Angola/Data/Raw/Orbitrap/Angola_mixed_samples_20250620020218.ms1\"# ========== 2. Orbitrap Real Time Series ==========end_time = '2025-06-20 16:40:00' # Define end time for the time seriesstart_time = pd.to_datetime(end_time) - pd.Timedelta(minutes=get_last_retention_time(file_path)) # Calculate start time based on the file retention time# ========== 3. Pre-filtering Threshold for Noisy Ions ==========pre_filtering_threshold = 1000 # Default threshold set to 1000 to filter out noisy ions# ========== 4. Processing Mode Selection ==========# Options: \"full_ts\" for full time series or \"subset\" for a specific peak list subset# mode = 'full_ts' # Use full time series mode# start_retention_time, end_retention_time = 0, get_last_retention_time(file_path) # Set for full time seriesmode = 'sub_set'start_retention_time, end_retention_time = 3.51, 13#get_last_retention_time(file_path) # Define subset range (retention times of the place we are interested)# ========== 5. Parameters for M/Z Calibration ==========ideal_calibrants_mass = [ 172.88347, 322.77771, 472.67196, 108.9638, 166.9163] # 5 calibrants for m/z calibrationcalibration_tolerance_ppm = 20 # Set calibration tolerance to 20 ppmoverlapped_scan_option = 1 # Handle overlapping regions in neighboring segment scans (1 to enable handling)# ========== 6. Parameters for Filtering and Clustering ==========# Pre-filtering based on \"Air-beam\" (Na2I range)Na2I_lower_bound, Na2I_upper_bound = 0, 50 * 1E6 # Filter to remove bad air-beam ions# Data size threshold for KDE clustering chunkdata_size_threshold = 500 # Minimum data size for performing KDE clustering# KDE Parameters (for clustering noisy ions)kde_bandwidth = 0.0005 # KDE bandwidth for peak density estimationkde_percentile = 5 # Remove ions below the 5th percentile of density score (to filter out rare/noisy ions)# DBSCAN Clustering Parameterseps_value = 0.5 # Epsilon value for DBSCAN (maximum clustering distance in ppm)clustering_thre = 0.025 # Retain clusters appearing more than 2.5% of Na2I showing frequency# ========== 7. EESI-TOF reference ==========# tof_spectrum = pd.read_excel(\"./../Input/EESI-TOF_reference/EESI_LTOF_raw_curve_Punjab.xlsx\")# tof_add_ion_threshold = 50 # 50 ppm means there is no Orbitrap ions within 50 ppm window of a EESI-TOF identifed peak apex# inorganic_ion_file = './../Input/EESI-TOF_reference/EESI_TOF_common_inorganic_ions.csv'# tof_peak_list_template = './../Input/EESI-TOF_reference/ToFware_peak_list_template.txt'# ========== 8. Peak List Output and refining parameters ==========peak_list_prefix = './output/ToF_peak_list/20250720_Angola_Orbitrap+TOF_peak_list'refining_distance,refining_ratio = 25, 4 3. Reading data within user-defined time periodThe segments refer to raw data extracted from the Orbitrap .ms1 text files, based on user-defined start and end retention times. Users can specify the desired time window for processing and optionally read multiple time segments to be merged in later steps. 123456789101112131415161718retention_time_indices,rt_values = get_retention_time_indices(file_path, start_retention_time, end_retention_time)segments = read_ms1_segments_between_retention_times(file_path, retention_time_indices)df_segments = save_segments_as_dataframes(segments, pre_filtering_threshold)merged_spectrum = merge_segments(df_segments)merged_spectrum = merged_spectrum.sort_values(by = 'm/z', ascending=True)merged_spectrum = merged_spectrum.reset_index(drop=True)``` The `showing_freq`, representing the recurrence frequency of a primary ion within the defined time window, is crucial for subsequent analysis. It reflects how consistently an ion appears and can serve as a basis for filtering other less stable or noise-like signals.```pythontolerance_ppm = 20 ## need to re-adjust based on the actual datapeak_results = {}masses = [172.88347, 322.77771, 472.67196,622.566700,108.9638,166.9163]#for mass in masses: peaks = find_peak_near_mass(merged_spectrum, mass, tolerance_ppm) peak_results[mass] = peaksshowing_freq = len(peak_results[masses[0]]) print (showing_freq) We can also evaluate how closely the measured m/z values of calibrants match their theoretical values before applying calibration. This step is useful for pre-defining the tolerance_ppm parameter. 1234567891011121314151617181920212223242526subplot_titles = ['Na2I', 'Na3I2', 'Na4I3','Na2[15]NO3', 'Na3[34]SO4']masses = [172.88347, 322.77771, 472.67196,108.9638,166.9163][0:] colors = ['#447f44', '#cd4040', '#636efa', '#ddc927', '#9271dd','#2d85cc'] subplot_titles = []for i in range(len(masses)): ppm_differences = calculate_ppm(np.array(peak_results[masses[i]]['m/z']), masses[i]) mean_ppm = np.mean(ppm_differences) subplot_titles.append(f\"{['Na2I', 'Na3I2', 'Na4I3', 'Na2[15]NO3', 'Na3[34]SO4'][i]}: {mean_ppm:.2f} ppm\")fig = make_subplots(rows=1, cols=5, subplot_titles=subplot_titles)for i in range(len(masses)): fig.add_trace( go.Histogram(x=peak_results[masses[i]]['m/z'], marker=dict(color=colors[i % len(colors)])), row=1, col=i+1 ) fig.add_vline(x=masses[i], line_width=2, line_dash=\"dash\", line_color=\"grey\", row=1, col=i+1) fig.update_xaxes(title_text=\"m/z\", showline=True, linecolor='black', range=[masses[i]-0.0025, masses[i]+0.0025], col=i+1, row=1) fig.update_yaxes(title_text=\"Count\", showline=True, linecolor='black', col=i+1, row=1)fig.update_layout( height=400, width=1500, title_text=\"Calibrant Raw M/Z Distribution\", plot_bgcolor='white', showlegend=False)fig.show()","link":"/2024/10/21/32.%5BMS%5DOrbiTrack_Dev_Log_1_Parsing_Orbitrap_data/"},{"title":"OrbiTrack Dev Log 3: Ion Filtering and Clustering","text":"After m/z calibration, the next step is to align ions detected across different retention times that likely originate from the same chemical species but appear at slightly shifted m/z values. To ensure robust clustering, itâ€™s important to first remove rare or noisy ions that could interfere with pattern recognition. Basic principleWe apply two types of filtering strategies onto Orbitrap MS1 data to isolate meaningful signals. Density-Based Filtering (KDE)Kernel Density Estimation (KDE) is used to evaluate the local density around each m/z value: Low-density points are likely to represent noise or isolated random signals and are filtered out using a user-defined percentile threshold (default: 5%). To balance accuracy and computational performance, the full m/z range is dynamically split into smaller chunks (e.g., 500 points), and KDE is applied to each chunk independently.(Note: Since KDE has a time complexity of O(nÂ²), chunking significantly reduces the computational burden. See slides below for more details.) Ion Clustering (DBSCAN)The remaining m/z values after KDE filtering are grouped using the DBSCAN algorithm: DBSCAN identifies ion clusters based on spatial proximity, enabling detection of true ion peaks even with minor m/z drifts over time. After clustering, a frequency threshold (e.g., â‰¥2% of scans) is applied to exclude unstable or sporadic clusters. For example, if a stable calibrant ion appears consistently in 100 scans, meaningful signals are expected to recur in at least a small fraction (e.g., â‰¥2 scans). This step removes random spikes that lack temporal consistency. Note: The clustering_thre threshold is critical and should be carefully tuned based on the actual dataset and scanning strategy. It is highly recommended to validate the setting by checking the fitting quality in the post-analysis stage. Code library1def correct_mz_dynamically(merged_spectrum, ideal_calibrants_mass, tolerance_ppm, overlapped_scan): histor","link":"/2024/10/23/34.%5BMS%5DOrbiTrack_Dev_Log_3_Ion_Filtering_and_Clustering/"},{"title":"OrbiTrack Dev Log 6: TOF missing peaks adding","text":"","link":"/2024/11/24/38.%5BMS%5DOrbiTrack_Dev_Log_6_Using_TOF_Fitting_Refiner/"},{"title":"OrbiTrack Dev Log 5: Chemical Formula Assignment","text":"In this post, I document the full formula assignment pipeline using MFAssignR, tailored for Orbitrap-MS data post-peak list processing (There is no need to reinvent the wheel here). The steps include quality control filtering, noise estimation via Kendrick Mass Defect (KMD) plots, isotopic filtering, recalibrant inspection, and final formula assignment. 1. Load Input and Install Required Packages12345setwd(\"./output/ToF_peak_list/\")devtools::install(\"MFAssignR\")library(MFAssignR)input_name = './output/ToF_peak_list/20250731_Punjab_Delhi_Orbitrap+TOF_peak_list'Data &lt;- read.csv(paste0(input_name, '-raw_mz.csv'))[ , c('m.z', 'intensity')] 2. Reading note: How MFAssignR filter raw peaks and assign formulasNoise filteringMFassignR uses a Kendrick Mass Defect (KMD)-based noise filtering approach (KMDNoise) before formula assignment. It analyzes raw spectra to identify background regions free from analyte signals. By slicing along two KMD lines (default intercepts: 0.05 and 0.2), it estimates the baseline noise level. Peaks below a user-defined signal-to-noise (SN) threshold (e.g., 3â€“10Ã— noise) are removed. This enhances data quality by filtering out low-intensity noise and multiply charged ions. Formula assignmentMFAssignR applies several non-optional quality assurance (QA) rules to screen out chemically invalid formulas during assignment. Below is a summary of key rules: ğŸ” Fundamental Rules Rule Description Senior Rule(Kind &amp; Fiehn, 2007) Ensures molecular formulas follow known valency and bonding constraints. Useful for identifying feasible adduct or fragment ions. Nitrogen Rule For odd vs even nominal masses: odd â†’ odd number of N atoms. Large Atom Rule Large atoms tend to fragment at weak bonds; used to predict fragmentation patterns. Max Hydrogen Rule Limits H count based on allowed bonding from other atoms. Prevents over-saturation. Max DBE Rule(Lobodin et al., 2012) Ensures formulas have chemically valid unsaturation: DBE = (2C + 2 + N âˆ’ X âˆ’ H)/2 3. Noise Estimation Using KMDKMDNoise isolates low-intensity regions via Kendrick Mass Defect linear slice filtering: 123456Noise &lt;- KMDNoise(Data)plot &lt;- Noise[[\"KMD\"]]plotKMDN &lt;- Noise[[\"Noise\"]]KMDNSNplot(Data, cut = KMDN * 3, mass = 301.1, window.x = 900, window.y = 25) Signal-to-noise (S/N) plot Spectrum after noise removal 4. Isotope Prescreening123Isotope &lt;- IsoFiltR(Data)Mono &lt;- Isotope[[\"Mono\"]]Iso &lt;- Isotope[[\"Iso\"]] 5. Initial CHO Formula Assignment1234Assign &lt;- MFAssignCHO(Mono, Iso, ionMode = \"pos\", lowMW =50, highMW = 1000, POEx= 0, Zx = 1, Mx = 2, Ex = 1, ppm_err = 3, H_Cmin = 0.3, HetCut = \"off\", NMScut = \"on\", SN = 1*KMDN) 6. Review Assignment Quality1234567Unambig1 &lt;- Assign[[\"Unambig\"]]Ambig1 &lt;- Assign[[\"Ambig\"]]Unassigned1 &lt;- Assign[[\"None\"]]MSAssign &lt;- Assign[[\"MSAssign\"]]Error &lt;- Assign[[\"Error\"]]MSgroups &lt;- Assign[[\"MSgroups\"]]VK &lt;- Assign[[\"VK\"]] 7. Identify Recalibrant Series and RecalibrateThis step uses high-confidence assigned ions (from Unambig1) to refine m/z accuracy via internal recalibration. 12345678910check &lt;- RecalList(Unambig1)Test &lt;- Recal(df = Unambig1, peaks = Mono, isopeaks = Iso, mode = \"pos\", SN = 2*KMDN, mzRange = 50, series1 = \"O5_Na_2\", series2 = \"O5_Na_3\", series3 = \"O6_Na_3\", series4 = \"O2_Na_3\", series5 = \"O3_Na_3\")Plot &lt;- Test[[\"Plot\"]]PlotMono2 &lt;- Test[[\"Mono\"]]Iso2 &lt;- Test[[\"Iso\"]]List &lt;- Test[[\"RecalList\"]] 8. Final Formula Assignment with Extended Elements1234Assign &lt;- MFAssign(Mono2, Iso2, ionMode = \"pos\", lowMW =50, highMW = 1000, POEx= 0, Zx = 1, Mx = 2, Ex = 0, Nx = 3, Sx = 3, ppm_err = 20, H_Cmin = 0.3, HetCut = \"off\", DeNovo = 300, NMScut = \"on\", SN = 0.25*KMDN) A summary of the parameters used in the MFAssign function: Parameter Value Meaning Mono2 [input] Dataframe of monoisotopic masses (from Recal step) Iso2 [input] Dataframe of isotopic masses (from Recal step) ionMode \"pos\" Specifies positive ionization mode lowMW 50 Lower limit of molecular mass to be assigned highMW 1000 Upper limit of molecular mass to be assigned POEx 0 Whether to allow odd-electron positive mode ions (0 = no) Zx 1 Charge state allowed in formula assignment Mx 2 Maximum number of sodium adducts (Na) allowed Ex 0 Amount of 13C isotopes allowed Nx 3 Maximum number of nitrogen atoms (14N) allowed Sx 3 Maximum number of sulfur atoms (32S) allowed ppm_err 20 Error tolerance for formula assignment in ppm H_Cmin 0.3 Minimum hydrogen-to-carbon (H/C) ratio HetCut \"off\" Disable high heteroatom QA filter DeNovo 300 Cutoff for de novo formula generation (masses above this value are not considered) NMScut \"on\" Enable nominal mass series QA check (Koch et al., 2007) SN 0.25*KMDN Signal-to-noise threshold for formula assignment, scaled by KMD-based noise estimate 9. Save Final Outputs1234567Unambig2 &lt;- Assign[[\"Unambig\"]]Ambig2 &lt;- Assign[[\"Ambig\"]]Unassigned2 &lt;- Assign[[\"None\"]]MSAssign &lt;- Assign[[\"MSAssign\"]]Error &lt;- Assign[[\"Error\"]]MSgroups &lt;- Assign[[\"MSgroups\"]]VK &lt;- Assign[[\"VK\"]] 123456input_name = './output/ToF_peak_list/20250731_Punjab_Orbitrap+TOF_peak_list'write.csv(Unambig2, file = paste0(input_name, '-assigned_mz.csv'))write.csv(Unassigned2,file = paste0(input_name, '-un-assigned_mz.csv'))write.csv(Ambig2, file = paste0(input_name, '-ambiguous_ion.csv'))write.csv(Iso2, file = paste0(input_name, '-isotope_mz.csv')) The assigned formula will be shown as: Reference Kind, T. &amp; Fiehn, O. (2007). Seven Golden Rules for heuristic filtering of molecular formulas obtained by accurate mass spectrometry. BMC Bioinformatics, 8, 105 Schum, S.K., Brown, L.E., &amp; Mazzoleni, L.R. (2020). MFAssignR: Molecular formula assignment software for ultrahigh resolution mass spectrometry analysis of environmental complex mixtures. Environmental Research, https://doi.org/10.1016/j.envres.2020.11011 MFAssignR github page, https://github.com/skschum/MFAssignR","link":"/2024/11/01/37.%5BMS%5DOrbiTrack_Dev_Log_5_Formula_Assignment/"},{"title":"OrbiTrack Dev Log 4: TOF missing peaks adding","text":"After m/z calibration, the next step is to align ions detected across different retention times that likely originate from the same chemical species but appear at slightly shifted m/z values. To ensure robust clustering, itâ€™s important to first remove rare or noisy ions that could interfere with pattern recognition. We apply two types of filtering strategies to isolate meaningful signals: Frequency filtering using K-means to eliminate low-frequency clusters. Density-score filtering using Kernel Density Estimation (KDE) to adaptively remove low-density regions. This pre-filtering step reduces the data size and computational load while preserving the integrity of true signal clusters, as shown in the examples below. Dynamic M/Z Calibration FunctionThe function below performs dynamic m/z calibration using known calibrant ions. It splits the full dataset by identifying where the calibrant signals are strongest (typically their peak retention times), and fits a linear correction model within each chunk. 12def correct_mz_dynamically(merged_spectrum, ideal_calibrants_mass, tolerance_ppm, overlapped_scan): ... Execution and Output ParametersRun the calibration and extract linear model parameters from each retention time segment. 123456merged_spectrum_mzcal, model_params = correct_mz_dynamically( merged_spectrum, ideal_calibrants_mass, calibration_tolerance_ppm, overlapped_scan_option) We then inspect the variation in calibration parameters (slope and intercept) across segments. 123456789101112131415161718slope_ = []intercept_ = []for c in model_params: slope_.append(c['slope']) intercept_.append(c['intercept'])fig = make_subplots(rows=1, cols=2)fig.add_trace(go.Scatter(x=np.arange(len(slope_)), y=slope_, mode='lines+markers'), row=1, col=1)fig.update_yaxes(title_text=\"Slope\", col=1, row=1)fig.add_trace(go.Scatter(x=np.arange(len(intercept_)), y=intercept_, mode='lines+markers'), row=1, col=2)fig.update_yaxes(title_text=\"Intercept\", col=2, row=1)fig.update_xaxes(title_text=\"Chunk\", showline=True, linecolor='black')fig.update_layout( height=400, width=900, title_text=\"Dynamic Fitting Parameters Across Retention Time\", plot_bgcolor='white', showlegend=False)fig.show() Check Calibration Accuracy on Known IonsWe visualize the distribution of m/z values for five reference ions before and after correction. This comparison confirms the improvement in accuracy. 123456789101112131415161718192021222324252627282930subplot_titles = ['Na2I', 'Na3I2', 'Na4I3','Na2[15]NO3', 'Na3[34]SO4']masses = [172.88347, 322.77771, 472.67196, 108.9638, 166.9163]colors = ['#447f44', '#cd4040', '#636efa', '#ddc927', '#9271dd']subplot_titles = []for i in range(len(masses)): ppm_differences = calculate_ppm(np.array(peak_results[masses[i]]['corrected_m/z']), masses[i]) mean_ppm = np.mean(ppm_differences) subplot_titles.append(f\"{['Na2I', 'Na3I2', 'Na4I3', 'Na2[15]NO3', 'Na3[34]SO4'][i]}: {mean_ppm:.2f} ppm\")fig = make_subplots(rows=1, cols=5, subplot_titles=subplot_titles)for i in range(len(masses)): fig.add_trace( go.Histogram(x=peak_results[masses[i]]['corrected_m/z'], marker=dict(color='blue')), row=1, col=i+1 ) fig.add_trace( go.Histogram(x=peak_results[masses[i]]['m/z'], marker=dict(color='red', opacity=0.75)), row=1, col=i+1 ) fig.add_vline(x=masses[i], line_width=2, line_dash=\"dash\", line_color=\"grey\", row=1, col=i+1) fig.update_xaxes(title_text=\"m/z\", range=[masses[i]-0.0025, masses[i]+0.0025], col=i+1, row=1) fig.update_yaxes(title_text=\"Count\", col=i+1, row=1)fig.update_layout( height=400, width=1500, title_text=\"Calibrant Mass Distribution Before and After Calibration\", plot_bgcolor='white', showlegend=False)fig.show()","link":"/2024/10/24/35.%5BMS%5DOrbiTrack%20Dev%20Log%204_Enhancing_Peak_Coverage_via_TOF_Integration/"},{"title":"Visualizing Chemical Structures ç»˜åˆ¶åŒ–å­¦ç‰©è´¨çš„ç»“æ„å¼","text":"For the purpose of visualizing chemical structures, I will now post a few method that can achive this goal. 1. ChemDrawChemDraw is a software that can be used to draw chemical structures (though commerical). 2. RDKitRDKit Python library can reproduce the structurs based on the smiles code. 12345678910111213141516171819202122232425from rdkit import Chemfrom rdkit.Chem import Draw# Define the correct Myosmine SMILESprecursor_smiles = \"C1CC(=NC1)C2=CN=CC=C2\" # Correct Myosmine (C9H10N2)precursor_mol = Chem.MolFromSmiles(precursor_smiles)# Define predicted fragment SMILESfragments_smiles = { \"79.0544\": \"C1CC(=NC1)\", # Benzene radical cation C6H6-H+ \"104.0495\": \"C1=CC=C(C=C1)C=[NH+]\", # Pyridine-benzyl cation \"106.0653\": \"C1=CC=C(C=N1)C=[NH+]\", # Methyl-pyridine \"132.0683\": \"CCC(=[NH+])C2=CN=CC=C2\", # Pyridine-imidazole \"147.0919\": \"C1CC(=NC1)C2=CN=CC=C2\" # Correct Myosmine (precursor)}# Convert SMILES to RDKit moleculesfragment_mols = {mz: Chem.MolFromSmiles(smiles) for mz, smiles in fragments_smiles.items()}# Draw precursor and fragmentsDraw.MolsToGridImage( [precursor_mol] + list(fragment_mols.values()), molsPerRow=3, subImgSize=(200, 200), legends=[\"Precursor\"] + list(fragments_smiles.keys())) Save the structures to SVG vector file. 12345678910111213141516from rdkit import Chemfrom rdkit.Chem import Draw# Define the moleculeprecursor_smiles = \"CC1=CN2C=CC=C2C(=N1)C\"precursor_mol = Chem.MolFromSmiles(precursor_smiles)# Generate SVG as a stringdrawer = Draw.MolDraw2DSVG(600, 600) # width, height in pixelsdrawer.DrawMolecule(precursor_mol)drawer.FinishDrawing()svg = drawer.GetDrawingText()# Save SVG to filewith open(\"C9H10N2_structure_pyrazine.svg\", \"w\") as f: f.write(svg) 3. SmileDrawerSmileDrawer is a lightweight JS library that renders chemical structures from SMILES strings in the browser. It supports canvas and SVG, with optional export and styling. I receommend this method with the nice looking and the interactive feature. Relevant link: Playground Simple example Online editor Two examples for pasting 3.1. Quick draw using Canvas12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"utf-8\" /&gt; &lt;meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\" /&gt; &lt;title&gt;Smiles Drawer Example&lt;/title&gt; &lt;meta name=\"description\" content=\"A minimal smiles drawer example.\" /&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" /&gt; &lt;link href=\"https://fonts.googleapis.com/css?family=Droid+Sans:400,700\" rel=\"stylesheet\" /&gt;&lt;/head&gt;&lt;body&gt; &lt;input id=\"example-input\" name=\"example-input\" /&gt; &lt;canvas id=\"example-canvas\" width=\"500\" height=\"500\"&gt;&lt;/canvas&gt; &lt;script src=\"https://unpkg.com/smiles-drawer@1.0.10/dist/smiles-drawer.min.js\"&gt;&lt;/script&gt; &lt;script&gt; let input = document.getElementById(\"example-input\"); let options = {}; // Initialize the drawer to draw to canvas let smilesDrawer = new SmilesDrawer.Drawer(options); // Alternatively, initialize the SVG drawer: // let svgDrawer = new SmilesDrawer.SvgDrawer(options); input.addEventListener(\"input\", function() { // Clean the input (remove unrecognized characters, such as spaces and tabs) and parse it SmilesDrawer.parse(input.value, function(tree) { // Draw to the canvas smilesDrawer.draw(tree, \"example-canvas\", \"light\", false); // Alternatively, draw to SVG: // svgDrawer.draw(tree, 'output-svg', 'dark', false); }); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.2 Exportable SVG with Save Button12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"utf-8\" /&gt; &lt;title&gt;SmilesDrawer SVG Example&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;input id=\"example-input\" name=\"example-input\" value=\"C1=CC=CC=C1\" /&gt; &lt;svg id=\"example-svg\" width=\"500\" height=\"500\" style=\"background:white; border:1px solid #ccc;\"&gt;&lt;/svg&gt; &lt;br&gt; &lt;button id=\"saveBtn\"&gt;ğŸ’¾ Save SVG&lt;/button&gt; &lt;script src=\"https://unpkg.com/smiles-drawer@2.0.1/dist/smiles-drawer.min.js\"&gt;&lt;/script&gt; &lt;script&gt; const input = document.getElementById(\"example-input\"); const svg = document.getElementById(\"example-svg\"); const svgDrawer = new SmilesDrawer.SvgDrawer({ width: 500, height: 500 }); input.addEventListener(\"input\", () =&gt; { while (svg.firstChild) svg.removeChild(svg.firstChild); // Clear old structure SmilesDrawer.parse(input.value, tree =&gt; { svgDrawer.draw(tree, svg, \"light\"); }, err =&gt; { console.error(\"SMILES parse error:\", err); }); }); // Initial draw input.dispatchEvent(new Event(\"input\")); // Save as SVG document.getElementById(\"saveBtn\").addEventListener(\"click\", () =&gt; { const serializer = new XMLSerializer(); const svgString = serializer.serializeToString(svg); const blob = new Blob([svgString], { type: \"image/svg+xml\" }); const url = URL.createObjectURL(blob); const a = document.createElement(\"a\"); a.href = url; a.download = \"molecule.svg\"; a.click(); URL.revokeObjectURL(url); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","link":"/2024/05/29/40.%5BMS%5DVisualizing-Chemical-Structures/"},{"title":"Common Function for Organics Visualization æœ‰æœºç‰©åˆ†å­ç»„æˆç»˜å›¾ä»£ç åˆ†äº«","text":"","link":"/2025/05/01/42.%5BCode%5DCommon_Functions_in_Chemical_Composition_Visualization%20copy/"},{"title":"[WRF-Chemå­¦ä¹ ç¬”è®°â‘¡]ä»…å«æ²™å°˜æºçš„æ¨¡æ‹Ÿç»ƒä¹ ","text":"æœ¬æ–‡æ˜¯æˆ‘å­¦ä¹ å®˜ç½‘ç»ƒä¹ çš„è®°å½•ï¼Œå…¶æ¨¡æ‹ŸåŒºåŸŸä¸ºåŒ—éï¼Œä¸­ä¸œä»¥åŠæ¬§æ´²éƒ¨åˆ†åœ°åŒºï¼Œæ— åµŒå¥—è®¾ç½®ã€‚å…±åˆ†ä¸º5ä¸ªç»ƒä¹ ï¼Œåˆ†åˆ«ä¸º: åˆå§‹åœºæ·»åŠ æ²™å°˜æ’æ”¾æºçš„æ¨¡æ‹Ÿ é‡‡ç”¨GOCARTå…¨çƒæ°”æº¶èƒ¶æ’æ”¾æ¸…å•çš„æ¨¡æ‹Ÿ åŠ å…¥ç”Ÿç‰©æºæ’æ”¾çš„åŒ–å­¦æ¨¡æ‹Ÿ(MEGANå¼•å…¥) åŠ å…¥æ°”æº¶èƒ¶ç›´æ¥/é—´æ¥è¾å°„æ•ˆåº”çš„æ¨¡æ‹Ÿ WRF-Chemæ•°å€¼é¢„æŠ¥å®éªŒ ä¸‹æ–‡è®°å½•Exercise 1çš„æµç¨‹å’Œæ³¨æ„äº‹é¡¹ WPSè¿è¡Œ12345678910111213141516-------cd /wrf/WPS/rm geogrid.log ungrib.log metgrid.log1. åœ°é¢é™æ€èµ„æ–™æ•°æ®ç”Ÿæˆcd geogridln -sf GEOGRID.TBL.ARW_CHEM GEOGRID.TBLcd .../geoerid.exe-------2. æ°”è±¡èµ„æ–™è¯»å–./link_grib.csh /disk2/data/ncep/fnl_2015071*./ungrib.exe &gt;&amp; ungrib.log-------3. æ•°æ®æ•´åˆmpirun -np 8 metgrid.exe------- æ³¨æ„æ­¤å¤„geogrid.exeè¾“å‡ºç»“æœä¼šåŒ…å«æ²™å°˜ä¿¡æ¯: Processing EROD Processing CLAYFRAC Processing SANDFRAC WRFè¿è¡Œåœ¨WRFV3/testæ–‡ä»¶å¤¹ä¸‹å¤åˆ¶em_realæ–‡ä»¶å¤¹ï¼Œæ–°å»ºtutoræ–‡ä»¶å¤¹ã€‚æ­¤å¤„ä¸»è¦è¿›è¡ŒWRFè¿è¡Œè¿‡ç¨‹ä¸­è¾¹ç•Œåœºå’Œåˆå§‹åœºæ–‡ä»¶çš„ç”Ÿæˆï¼Œchem_opt(åŒ–å­¦é€‰é¡¹) = 401ï¼Œ è¡¨ç¤ºä»…è€ƒè™‘æ²™å°˜ã€‚ 1234é“¾æ¥WPSçš„è¾“å‡ºç»“æœln -svf /disk2/hyfmpirun -np 16 ./real.exempirun -np 16 ./wrf.exe æˆåŠŸè¿è¡Œï¼Œä¼šåœ¨rsl.out.xxxxå‡ºç° __wrf: SUCCESS COMPLETE WRF__å­—æ®µã€‚ ç»˜åˆ¶è¾“å‡ºç»“æœåˆ©ç”¨Pythonå¯¹è¾“å‡ºæ–‡ä»¶è¿›è¡Œè¯»å–å’Œç»˜åˆ¶å›¾åƒï¼Œå¦‚ä¸‹å›¾ä¸­çš„æ¸©åº¦: å‚è€ƒèµ„æ–™ Exercise 1","link":"/2016/09/05/5.%5BModel%5D%5BWRF-Chem%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%91%A1%5D%E4%BB%85%E5%90%AB%E6%B2%99%E5%B0%98%E6%BA%90%E7%9A%84%E6%A8%A1%E6%8B%9F%E7%BB%83%E4%B9%A0/"},{"title":"Organic Emission Profiles in Interactive Plots","text":"Load Plotly JS once (in the themeâ€™s head.ejs or _partial/footer.ejs)In themes/minos/layout/_partial/head.ejs, before Interactive Spectrum: Fire vs Vehicular Emissions","link":"/2025/05/01/43.%5BCode%5DInteractive_plot_for_organics_spectra/"},{"title":"MODISå«æ˜Ÿç«ç‚¹æ•°æ®çš„ç®€æ˜“å¤„ç†ä¸å¯è§†åŒ–åˆ†æ","text":"æè¦ï¼šæ­¤æ–‡ä»‹ç»åˆ©ç”¨Pythonè¯­è¨€å¤„ç†NASA MODISç«ç‚¹æ•°æ®ï¼ˆGlobal Monthly Fire Location Productï¼ŒMCD14MLï¼‰ï¼Œå¯å®ç°çš„åŸºæœ¬åŠŸèƒ½åŒ…æ‹¬:ï¼ˆ1ï¼‰ç‰¹å®šæ—¶æœŸçš„ç«ç‚¹ä¿¡æ¯æå–;ï¼ˆ2ï¼‰ç‰¹å®šåŒºåŸŸå†…çš„ç«ç‚¹ä¿¡æ¯æå–;ï¼ˆ3ï¼‰ç«ç‚¹å¯†åº¦ç©ºé—´åˆ†å¸ƒçš„è®¡ç®—ä¸å¯è§†åŒ–è¡¨è¾¾ ç«ç‚¹æ•°æ®èƒŒæ™¯ä»‹ç»ç”Ÿç‰©è´¨(biomass)å’ŒçŸ³æ²¹ã€ç…¤ç‚­ç­‰ä¸€æ ·ï¼Œæ˜¯ä¸€ç±»é‡è¦çš„èƒ½æºç‰©è´¨ã€‚åœ¨æˆ‘å›½çš„å¹¿å¤§å†œæ‘åœ°åŒºï¼Œç”Ÿç‰©è´¨èƒ½æº(å¦‚ç§¸ç§†ã€è–ªæŸ´ä»¥åŠç‰²ç•œç²ªä¾¿ç­‰)å æ®é‡è¦åœ°ä½ï¼Œåœ¨1979å¹´å‰ï¼Œæ›´æ˜¯å åˆ°æ•´ä¸ªå†œæ‘èƒ½æºæ¶ˆè´¹æ€»é‡çš„70%ä»¥ä¸Š[^1][1]ã€‚åŒæ—¶ï¼Œç”±äºç”Ÿç‰©è´¨ç‡ƒçƒ§è¿‡ç¨‹ä¸­ï¼Œæ’æ”¾å¤§é‡æ°”æ€ä»¥åŠé¢—ç²’æ€çš„æ±¡æŸ“ç‰©ï¼Œå¯¹å¤§æ°”èƒ½è§åº¦é€ æˆå½±å“ï¼Œå±å®³äººä½“å¥åº·ï¼ŒåŒæ—¶å…·æœ‰ç€ä¸€å®šçš„æ°”å€™æ•ˆç›Šã€‚ ç¯å¢ƒé¥æ„ŸæŠ€æœ¯ä¾æ®ä¸­çº¢å¤–æ³¢æ®µå¯¹é«˜æ¸©çƒ­æºçš„æ•æ„Ÿç‰¹æ€§ï¼Œå¯å¯¹å…¨çƒé™†é¢çš„ç«ç‚¹ä¿¡æ¯è¿›è¡Œæ£€æµ‹ï¼Œè¯†åˆ«ç”Ÿç‰©è´¨å¼€æ”¾ç‡ƒçƒ§äº‹ä»¶çš„å…·ä½“ä½ç½®ã€‚ç›®å‰è¾ƒä¸ºæˆç†Ÿçš„ç«ç‚¹æ•°æ®åº“ä¸»è¦æœ‰ä¸¤ç±»ï¼šMODISä»¥åŠVIIRS 375mï¼Œä»¥å¤šç§æ•°æ®æ ¼å¼ä¸ºå…¬ä¼—æä¾›å†å²ç«ç‚¹ä¿¡æ¯ã€‚ ç‰¹å®šæ—¶æœŸç‡ƒçƒ§äº‹ä»¶æ•°æ®çš„æå–æ­¤å¤„ä»‹ç»MODISæ•°æ®é›†ä¸­åº”ç”¨æœ€ä¸ºå¹¿æ³›çš„__MCD14ML__æ•°æ®ï¼Œå¯¹åŸºç¡€æ•°æ®çš„æ‰“ç†å’Œåˆ†æåšä»¥åŸºæœ¬çš„ä»‹ç»ã€‚__MCD14ML__æ•°æ®çš„å•ä¸ªæ–‡ä»¶åŒ…å«æŸä¸ªæœˆçš„å…¨çƒç«ç‚¹ä¿¡æ¯ï¼Œå…·ä½“è·å–æ–¹æ³•å¦‚ä¸‹ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#### ä¸‹è½½ç½‘å€åŠç”¨æˆ·å ### #ftp://fuoco.geog.umd.edu/modis/C6/mcd14ml/#ç”¨æˆ·å:fire#å¯†ç : burnt #----------------------#### 1. æ–‡ä»¶çš„ä¸‹è½½ä¸‹è½½# æ­¤å¤„å‚è€ƒhttps://stackoverflow.com/questions/7006574/how-to-download-file-from-ftpä¸­çš„æ–¹æ³•è¿›è¡Œä¸‹è½½ï¼Œäº¦å¯ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ä¸‹è½½ import mimetypes,os,urllib2,urlparsedef filename_from_url(url): return os.path.basename(urlparse.urlsplit(url)[2])def download_file(url): \"\"\"Create an urllib2 request and return the request plus some useful info\"\"\" name = filename_from_url(url) r = urllib2.urlopen(urllib2.Request(url)) info = r.info() if 'Content-Disposition' in info: # If the response has Content-Disposition, we take filename from it name = info['Content-Disposition'].split('filename=')[1] if name[0] == '\"' or name[0] == \"'\": name = name[1:-1] elif r.geturl() != url: # if we were redirected, take the filename from the final url name = filename_from_url(r.geturl()) content_type = None if 'Content-Type' in info: content_type = info['Content-Type'].split(';')[0] # Try to guess missing info if not name and not content_type: name = 'unknown' elif not name: name = 'unknown' + mimetypes.guess_extension(content_type) or '' elif not content_type: content_type = mimetypes.guess_type(name)[0] return r, name, content_typeimport shutildef download_file_locally(url, dest): req, filename, content_type = download_file(url) if dest.endswith('/'): dest = os.path.join(dest, filename) with open(dest, 'wb') as f: shutil.copyfileobj(req, f) req.close() time = \"201609\" ## æ­¤å¤„ä»¥2016å¹´09æœˆä¸ºä¾‹filename = 'MCD14ML.'+time+'.006.01.txt.gz'download_file_locally('ftp://fire:burnt@fuoco.geog.umd.edu/modis/C6/mcd14ml/',filename) #### 2. æ–‡ä»¶é¢„å¤„ç†# åŸå§‹æ–‡ä»¶ä¸­å­˜åœ¨å¤šç±»åˆ†éš”ç¬¦ï¼Œéœ€è¿›è¡Œé¢„å…ˆå¤„ç†ï¼Œç»Ÿä¸€æ›¿æ¢ä¸ºâ€œ â€ï¼Œä¾¿äºåç»­è¯»å–ã€‚with open(filename, 'r') as file : filedata = file.read()# Replace the target stringfiledata = filedata.replace(' ', ' ')filedata = filedata.replace(' ', ' ')#Write the file out againwith open(filename, 'w') as file: file.write(filedata)#### 3. æ–‡ä»¶è¯»å–df = pd.read_csv(filename, sep='\\s+')df.head() ç‰¹å®šåŒºåŸŸå†…ç‡ƒçƒ§äº‹ä»¶çš„æå–åŸå§‹æ•°æ®ä¸­åŒ…å«è¯¥æ®µæ—¶æœŸç«ç‚¹ä½ç½®çš„æœ‰å…³ä¿¡æ¯ï¼Œå¯¹äºç ”ç©¶ç”Ÿç‰©è´¨ç‡ƒçƒ§å¯¹äºå±€åœ°æ±¡æŸ“äº‹ä»¶å‘ç”Ÿçš„å½±å“ï¼Œåº”å¯¹äºç‰¹å®šåœ°ç†ä½ç½®å†…çš„ç«ç‚¹ä¿¡æ¯è¿›è¡Œæå–ã€‚è¯¥å·¥ä½œåŒæ ·å¯ä»¥å€ŸåŠ©fionaåŒ…ï¼Œåˆ©ç”¨ç®€å•çš„åˆ¤æ–­è¯­å¥å®ç°ã€‚è¿™é‡Œæˆ‘ä»¬ä»¥å››å·çœä¸ºä¾‹ï¼Œå¯¹2016å¹´9æœˆå‘ç”Ÿåœ¨è¯¥çœå¢ƒå†…çš„é€æ—¥ç«ç‚¹ä¿¡æ¯è¿›è¡Œæå–ã€‚ 12345678910111213141516171819import fionafrom shapely.geometry import shape,Polygon, Point# read the shapefilesc_area = fiona.open(\"/Users/HYF/Documents/NanChong/GIS/sichuan.shp\")pol = sc_area.next()from shapely.geometry import shapegeom = shape(pol['geometry'])poly_data = pol[\"geometry\"][\"coordinates\"][0]poly = Polygon(poly_data)### zip the x coordidate and y coordidate of each hotspotpoints = zip(*np.array([df.lon.values, df.lat.values]))### to identify whether the hotspot is inside the boundary or notmask = np.array([poly.contains(Point(x, y)) for x, y in points]) df[\"MASK\"] = maskselect = df[df[\"MASK\"] == True]select['datetime'] = pd.to_datetime(select.YYYYMMDD,format = '%Y%m%d') ç®€å•çš„å›¾å½¢ç»˜åˆ¶ ç«ç‚¹åˆ†å¸ƒå›¾ä¾‹å¦‚ï¼Œæ­¤å¤„æˆ‘ä»¬ä¸“é—¨è€ƒå¯Ÿ2016å¹´9æœˆ5æ—¥å››å·çœå¢ƒå†…çš„ç«ç‚¹åˆ†å¸ƒæƒ…å†µï¼š 1234567891011121314####Only select the biomass events in 2016-09-05select[select.datetime == '2016-09-05']m = Basemap(projection='cyl', resolution='l',llcrnrlon = 97.2,llcrnrlat=26.0,urcrnrlon = 109.0,urcrnrlat=34.5)m.readshapefile('/Users/HYF/Documents/NanChong/GIS/sichuan', 'sichuan', color='b', zorder=3,linewidth=1.5)m.drawcoastlines(color = '0.15')lon, lat = m(select.lon,select.lat)plt.scatter(lon,lat, color = 'red')parallels = np.arange(26.,34,3.)m.drawparallels(parallels,labels=[False,True,True,False])meridians = np.arange(97,109.,3.)m.drawmeridians(meridians,labels=[True,False,False,True])plt.show()","link":"/2017/06/22/6.%5BCode%5DPython%E7%81%AB%E7%82%B9%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%86%E6%9E%90%E5%A4%84%E7%90%86/"},{"title":"åˆ©ç”¨Pythonè·å–åœ°ç†ä¿¡æ¯å¹¶å¯è§†åŒ–","text":"ç½‘ç»œä¸­æœ‰ä¸°å¯Œçš„åœ°ç†ä¿¡æ¯æ•°æ®èµ„æºï¼Œæ­¤å¤„ä»‹ç»æˆ‘é‡‡ç”¨Pythonå·¥å…·å®ç°åœ°å½¢åœ°è²Œã€è¡Œæ”¿åŒºåˆ’ã€åŸå¸‚äº¤é€šè·¯ç½‘ç­‰æ•°æ®è·å–åŠå¯è§†åŒ–çš„éƒ¨åˆ†å®ä¾‹ï¼Œä¾›å¤§å®¶å‚è€ƒå­¦ä¹ ã€‚ 1. ç®€æ˜“åœ°å½¢åº•å›¾ç»˜åˆ¶ 1.1 WMSæ•°æ®åº“é’ˆå¯¹è¾ƒå¤§èŒƒå›´çš„ç ”ç©¶åŒºåŸŸï¼Œæˆ‘é‡‡ç”¨WMSæ•°æ®åº“çš„åœ°å½¢åº•å›¾ï¼Œä¾å››è§’åæ ‡ä¸‹è½½ç‰¹å®šå°ºå¯¸çš„å›¾ç‰‡ï¼Œå¹¶å±•ç¤ºã€‚æ­¤å¤„ä»¥ä¸œäºšåœ°åŒºä¸ºä¾‹ï¼Œé€æ­¥å±•ç¤ºã€‚ 1234567891011121314151617181920# 1. Set the left, bottom, right, top locationsllx,lly, urx,ury = [104.211,29.150,105.523,30.150]# 2. Download the original filetarget = 'http://www2.demis.nl/wms/wms.asp?Service=WMS&amp;WMS=worldmap&amp;Version=1.1.0&amp;Request=GetMap&amp;BBox=%s,%s,%s,%s'%(llx,lly, urx,ury) +\\ '&amp;SRS=EPSG:4326&amp;Width=800&amp;Height=600&amp;Layers=Bathymetry,Countries,Topography,Ocean%20features,Hillshading,Borders,Waterbodies,Coastlines&amp;Format=image/gif'path_name = './Terrain.gif'urllib.urlretrieve(target, path_name) # 3. Plot the terrain map of East Asiafrom scipy.misc import imreadfig = plt.figure(figsize = (8,6))ax = plt.subplot()m = Basemap(projection='cyl', resolution='l',llcrnrlon = llx,llcrnrlat=lly,urcrnrlon = urx,urcrnrlat=ury)img = imread(path_name)plt.imshow(img, zorder=1,extent=[llx,urx,lly,ury ] )m.drawparallels(np.arange(-90., 120., 10.), labels=[1, 0, 0, 0],fontsize =18,linewidth= 0.02)m.drawmeridians(np.arange(45,180,10), labels=[0, 0, 0, 1],fontsize = 18,linewidth= 0.02)plt.subplots_adjust(left=0.015, bottom = 0.05, top = 0.99,right=0.99, wspace = 0.1,hspace=None) å‡ºå›¾çš„æ ·ä¾‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ 1.2 SRTM geotiffæ•°æ®å¯¹äºè¾ƒå°çš„åœ°ç†ç¯å¢ƒï¼Œç›´æ¥é‡‡ç”¨WMSæ ¼å¼å›¾ç‰‡æ¸…æ™°åº¦ä¸å¤Ÿã€‚æ­¤å¤„ä»‹ç»ä¸‹è½½å¹¶ç»˜åˆ¶åŸå§‹DEMåœ°å½¢æ–‡ä»¶çš„æ–¹æ³•ã€‚é€šè¿‡elevation packageè¿›è¡Œtifæ ¼å¼åœ°å½¢æ–‡ä»¶çš„ä¸‹è½½ 12345678910111213141516171819202122232425262728293031# ä»¥å››å·æŸå¸‚åŠå‘¨è¾¹åœ°åŒºä¸ºä¾‹## 1. Download the geotiff file!pip install elevation !eio --product SRTM3 clip -o DEM.tif --bounds 104.211 29.150 105.523 30.150 # left bottom right top locations ## 2. Read the tif file as 2-d arraypathToRaster = r'./DEM.tif'raster = gdal.Open(pathToRaster, gdal.GA_ReadOnly)dem = raster.GetRasterBand(1).ReadAsArray()dem = dem[::-1] ## 3. Generate a terrain coloarmap import matplotlib.colors as colorsdef truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100): new_cmap = colors.LinearSegmentedColormap.from_list( 'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n))) return new_cmapcmap = plt.get_cmap('terrain')terrain_cmap = truncate_colormap(cmap, 0.2, 0.7) ## 4. Plot it!llx,lly, urx,ury = [104.211,29.150,105.523,30.150]fig = plt.figure(figsize = (8,5))ax = plt.subplot()m = Basemap(projection='cyl', resolution='l',llcrnrlon = llx,llcrnrlat=lly,urcrnrlon = urx,urcrnrlat=ury) dem_plot = map.imshow(dem, extent=[llx,urx,lly,ury ], cmap =plt.get_cmap('terrain'),vmin = 0 ,vmax = 900)# terrain_cmapmap.drawparallels(np.arange(-90.0, 120., 0.5), labels=[1, 0, 0, 0],fontsize = 12,linewidth = 0.1,color = '#FFFFFF', zorder = 10)map.drawmeridians(np.arange(-180.0, 180., 0.5), labels=[0, 0, 0, 1],fontsize = 12,linewidth = 0.1,color = '#FFFFFF',zorder=10) ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š 1.3 Cartopy STAMEN å›¾å½¢ç»˜åˆ¶æ•°æ®æºstamenæ˜¯å¼€æºçš„åœ°ç†ä¿¡æ¯æœåŠ¡ç½‘ç«™ï¼Œæä¾›é“è·¯ã€åœ°å½¢ç­‰å¤šç§æ•°æ®ä¿¡æ¯ã€‚åœ¨å…¶å®˜æ–¹ç½‘é¡µé‡‡ç”¨æ‰‹åŠ¨ä¸‹æ–¹å¼ï¼Œä»…èƒ½ä»¥ç¡®å®šä¸­å¿ƒç‚¹å’Œç¼©æ”¾æ¯”ä¾‹çš„æ–¹å¼è¿›è¡Œå›¾ç‰‡ä¸‹è½½ï¼Œä¸èƒ½å®ç°ä»»æ„åŒºåŸŸå†…å›¾å½¢è·å–ã€‚æ­¤å¤„å‚ç…§Geology and Pythonåšå®¢ä¸­çš„ç›¸å…³æ•™ç¨‹ï¼Œä»¥ä¸­å›½å…¨åŸŸä¸ºä¾‹ï¼Œå±•ç¤ºåº•å›¾è·å–å’Œç»˜åˆ¶æµç¨‹ã€‚ cartopyæ˜¯ç”¨ä»¥åˆ†æå¹¶å¯è§†åŒ–åœ°ç†æ•°æ®çš„Pythonåº“ã€‚åœ¨å®‰è£…æ—¶å€™ï¼Œä¸ç³»ç»ŸåŸæœ‰çš„matplotlib.basemapå‘ç”Ÿå†²çªï¼ŒåŸå› åœ¨äºäºŒè€…æ‰€ä¾èµ–çš„GEOSåº“ä¸ä¸€è‡´ã€‚åœ¨æ­¤å¤„èŠ±è´¹äº†ä¸å°‘æ—¶é—´æ¥è§£å†³è¿™ä¸€é—®é¢˜(ç½‘ç»œä¸Šä¹Ÿæœ‰å¾ˆå¤šäººé‡åˆ°äº†ç±»ä¼¼çš„é—®é¢˜ï¼Œå¦‚æ— æ³•åŒæ—¶å®‰è£…æˆåŠŸï¼Œå®‰è£…åæ— æ³•importä»¥åŠæ‰§è¡Œcartopyæœ‰å…³æŒ‡ä»¤ï¼Œkernelä¼šè‡ªåŠ¨restart. )ã€‚ åœ¨æ­¤å¤„ï¼Œæˆ‘åˆ—å‡ºäº†è‡ªå·±çš„å®‰è£…æ­¥éª¤: 123456789101112131415161718192021222324252627282930313233343536373839404142434445## å¸è½½åŸæœ‰çš„cartopyåŠç›¸å…³åº“shapelyï¼Œä¿ç•™åŸæœ‰çš„basemapconda uninstall cartopy conda uninstall shaeply ## å‡çº§gdalåº“ brew upgrade gdal ## é‡‡ç”¨pipæ–¹å¼å®‰è£…cartopyåŠshapelypip install shapely cartopy --no-binary shapely --no-binary cartopy## ç»“æœä»æœªèƒ½æˆåŠŸimport cartopy,åŸå› åœ¨äºshapelyåº“æœªå®‰è£…æˆåŠŸã€‚ç”¨condaæ–¹å¼å†å®‰è£…conda install shapely ## å…¨éƒ¨å®‰è£…å®Œæ¯•!! d(`ï½¥âˆ€ï½¥)b## å¼€å§‹ç»˜å›¾import matplotlib.pyplot as pltimport cartopy.crs as ccrsfrom cartopy.feature import BORDERSfrom cartopy.io.img_tiles import StamenTerrainfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatterdef basemap_terrain(extent): fig = plt.figure(figsize=(8, 6)) ax = fig.add_subplot(111, projection=ccrs.PlateCarree()) ax.set_extent(extents=extent, crs=ccrs.Geodetic()) ax.coastlines(resolution='10m') BORDERS.scale = '10m' ax.add_feature(BORDERS) ax.gridlines(color='.5') ax.set_xticks(np.arange(extent[0]+2, extent[1], 15), crs=ccrs.PlateCarree()) ax.set_yticks(np.arange(extent[2]+2, extent[3], 15), crs=ccrs.PlateCarree()) ax.xaxis.set_major_formatter(LongitudeFormatter()) ax.yaxis.set_major_formatter(LatitudeFormatter()) return fig, ax# åœ°å›¾å››è‡³x1,x2,y1,y2 = 74,134, 18,51extent = x1 -1, x2+1,y1-1, y2+1 fig, ax = basemap_terrain(extent)st = StamenTerrain()ax.add_image(st, 4) 2. è¡Œæ”¿åŒºåˆ’åŠåŸå¸‚è·¯ç½‘ä¿¡æ¯æ­¤å¤„ä»¥å››å·å†…æ±Ÿå¸‚ä¸ºä¾‹ï¼Œåˆ©ç”¨osmnxåº“ï¼Œshpæ–‡ä»¶è‡ªåŠ¨ä¸‹è½½äºå·¥ä½œæ–‡ä»¶å¤¹å†…ã€‚ 1234567# è¡Œæ”¿è¾–åŒºä¸‹è½½import osmnx as oxcity = ox.gdf_from_place('Neijiang City, Sichuan, China')ox.save_gdf_shapefile(city)city = ox.project_gdf(city)fig, ax = ox.plot_shape(city, figsize=(3,3)) 1234# å†…æ±Ÿå¸‚ä¸œå…´åŒºå…¬è·¯è·¯ç½‘ä¸‹è½½Road = ox.graph_from_place('Dongxing, Neijiang, Sichuan, China', network_type='drive')Road_projected = ox.project_graph(Road)fig, ax = ox.plot_graph(Road_projected) 3. ä¸€äº›æœ‰ç”¨çš„å°æŒ‡ä»¤12345678910111213141516171819202122232425262728293031323334353637# A. ä»»æ„åœ°ç‚¹çš„æµ·æ‹”æŸ¥è¯¢!pip install geocoderimport geocoderprint geocoder.elevation ([lat,lng]).meters# B. ä¸¤ä¾‹è·å–æŸåœ°åŒºåŠå‘¨è¾¹çš„googleåœ°å›¾é™æ€å›¾åƒ## B1. å«æ˜Ÿå›¾http://maps.googleapis.com/maps/api/staticmap?center=42.2341289,118.9790778&amp;zoom=12&amp;format=png&amp;sensor=false&amp;size=2400x2000&amp;maptype=satellite&amp;style=feature:administrative|weight:0.1|invert_lightneÂºss:true|visibility:off&amp;style=feature:water|element:labels.text|visibility:off&amp;style=feature:landscape.natural|visibility:on## B2. é“è·¯åœ°å›¾http://maps.google.com/maps/api/staticmap?sensor=false&amp;size=512x512&amp;center=Brooklyn&amp;zoom=12&amp;style=feature:all|element:labels|visibility:off# C. googleåœ°å›¾ä¸‹è½½å·¥å…·http://www.chengfolio.com/google_map_customizer#satellitemap# D. åœ°å½¢å›¾åœ¨çº¿ä¸‹è½½å·¥å…·https://maps-for-free.com/ # E.æµ‹é‡ä¸¤ç‚¹é—´è·ç¦»# source: https://stackoverflow.com/questions/15736995/how-can-i-quickly-estimate-the-distance-between-two-latitude-longitude-pointsfrom math import radians, cos, sin, asin, sqrtdef haversine(lon1, lat1, lon2, lat2): \"\"\" Calculate the great circle distance between two points on the earth (specified in decimal degrees) \"\"\" # convert decimal degrees to radians lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2 c = 2 * asin(sqrt(a)) # Radius of earth in kilometers is 6371 km = 6371* c return km","link":"/2017/11/06/7.%5BCode%5D%E5%9C%B0%E7%90%86%E5%9C%B0%E5%BD%A2%E6%95%B0%E6%8D%AE%E7%9A%84%E8%8E%B7%E5%8F%96%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"title":"ä¸­å›½ç©ºæ°”è´¨é‡å†å²æ•°æ®æ•´ç†(HDF5æ ¼å¼)","text":"å…¨å›½ç©ºæ°”è´¨é‡å†å²æ•°æ®æ˜¯å¤§æ°”ç¯å¢ƒç ”ç©¶çš„é‡è¦åŸºç¡€èµ„æ–™ï¼Œè€Œæˆ‘å›½å®˜æ–¹å¹³å°ä»…æä¾›å®æ—¶ç›‘æµ‹ä¿¡æ¯ï¼Œè€Œä¸”è¿˜åªæ”¯æŒIEæµè§ˆå™¨(â•®(â•¯_â•°)â•­)ã€‚ æˆ‘æ›¾å°è¯•é‡‡ç”¨çˆ¬è™«å·¥å…·è·å–å¹¶å­˜å‚¨é€æ—¶ä¿¡æ¯ï¼Œä½†é™äºæƒé™ï¼Œæœªèƒ½åœ¨å®éªŒå®¤æœåŠ¡å™¨ä¸Šè¿ç»­é•¿æ—¶é—´è¿è¡Œã€‚äº’è”ç½‘ä¸Šç›´æ¥æä¾›æ•°æ®æˆ–APIç«¯å£çš„ç½‘ç«™æœ‰å¾ˆå¤šï¼Œå¦‚ç¯å¢ƒäº‘ï¼ŒPM25.in, é’æ‚¦å¼€æ”¾ç¯å¢ƒæ•°æ®ä¸­å¿ƒç­‰ã€‚å…¶ä¸­ï¼Œbeijingairæœ€å…·åˆ†äº«ç²¾ç¥,æä¾›äº†2013å¹´è‡³ä»Šçš„è¯¦å°½å†å²æ•°æ®ï¼Œä¸”å®Œå…¨å…è´¹ã€‚ ç”±è¡·æ„Ÿè°¢@ç‹_æ™“ç£Šçš„å‡ºè‰²å·¥ä½œå’Œæ— ç§åˆ†äº«ï¼Œæå¤§åœ°æ¨åŠ¨äº†æˆ‘å›½ç¯å¢ƒæ•°æ®çš„å…¬å¼€ä¸é€æ˜ã€‚å…¶æ•°æ®æ ¼å¼ä¸ºæ¯æ—¥ä¸€ä»½csvæ–‡ä»¶å­˜å‚¨å½“æ—¥æ‰€æœ‰ç«™ç‚¹/åŸå¸‚çš„é€æ—¶ä¿¡æ¯ã€‚åœ¨é•¿æ—¶é—´å°ºåº¦çš„æ•°æ®åˆ†ææ—¶ï¼Œéœ€é€ä¸€é˜…è¯»å„åŸå§‹æ–‡ä»¶ã€‚æ­¤å¤„ï¼Œæˆ‘è€ƒè™‘å°†å…¨å¹´æ•°æ®æ–‡ä»¶æ•´åˆä¸ºå¤šç»´åº¦æ•°æ®å­˜å‚¨æ ¼å¼(HDF5)æ–‡ä»¶ï¼Œä¾¿äºè°ƒç”¨å’Œå¤„ç†ã€‚ 1. å…¨ç«™ç‚¹æ•°æ®æ•´åˆ æˆªè‡³2018å¹´ï¼Œæˆ‘å›½367åº§åŸå¸‚å…±è®¾ç½®æœ‰1437å¤„å›½æ§ç©ºæ°”è´¨é‡ç›‘æµ‹ç«™ç‚¹ã€‚ä¸‹æ–‡ä¸ºå…¨å¹´æ•°æ®æ‰“ç†æµç¨‹ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#libraryimport pandas as pdfrom pandas import HDFStore, DataFramefrom pandas import read_hdfimport os,sys,stringimport numpy as np### CREAT A EMPTY HDF5 FILEhdf = HDFStore(\"site_2017_whole_year.h5\")### READ THE CSV FILES AND SAVE IT INTO HDF5 FORMATos.chdir(\"./site_2017/\")files = os.listdir(\"./\")files.sort()print files[0]test_file= \"./../china_sites_20170101.csv\"test_f = pd.read_csv(test_file,encoding='utf_8')site_columns = list(test_f.columns[3:])print site_columns[1]### AFTER 2017_2 PUTUO SITE IN SHANGHAI (1141A) is in.### æ³¨æ„ï¼Œ2017å¹´2æœˆåä¸Šæµ·å¸‚å¢åŠ äº†æ™®é™€ç«™ç‚¹(1141A),éœ€åœ¨site_columnsä¸­è¡¥å……ä¸Š#site_columns = site_columns.append(u'1141A')feature = ['pm25','pm10','O3','O3_8h','CO',\"NO2\",'SO2',\"aqi\"]fe_dict = {\"pm25\":1,\"aqi\":0, 'pm10':3, 'SO2':5,'NO2':7, 'O3':9,\"O3_8h\":11, \"CO\": 13}for k in range(0,len(feature),1): data_2017 = {\"date\":[],'hour':[],} for i in range(0,len(site_columns),1): data_2017[site_columns[i]] = [] for file in files[0:]: print file filename,extname = os.path.splitext(file) if (extname == \".csv\"): datafile =file f_day = pd.read_csv(datafile,encoding='utf_8') #site_columns = list(f_day.columns[3:]) for i in range(0,len(f_day),15): datetime = str(f_day[\"date\"].iloc[i]) hour = \"%02d\" % ((f_day[\"hour\"].iloc[i])) data_2017[\"date\"].append(datetime) data_2017[\"hour\"].append(hour) for t in range(0,len(site_columns),1): data_2017[site_columns[t]].append(f_day[site_columns[t]].iloc[i+fe_dict[feature[k]]]) print feature[k] data_2017 = pd.DataFrame(data_2017) hdf.put(feature[k], data_2017, format='table', encoding=\"utf-8\") 2. å…¨åŸå¸‚æ•°æ®æ•´åˆ æ–¹æ³•ç±»ä¼¼ï¼Œæ¯æ¬¡å†™å…¥367åº§åŸå¸‚æŸä¸€ç±»æ±¡æŸ“ç‰©ä¿¡æ¯ 123456789101112131415161718192021222324252627282930313233343536### CREAT A EMPTY HDF5 FILEhdf = HDFStore(\"city_2017_whole_year.h5\")### READ THE CSV FILES AND SAVE IT INTO HDF5 FORMATos.chdir(\"./city_2017/\")files = os.listdir(\"./\")files.sort()print files[0]test_file= \"china_cities_20170101.csv\"test_f = pd.read_csv(test_file,encoding='utf_8')city_columns = list(test_f.columns[3:])print city_columns[1]feature = ['pm25','pm10','O3','O3_8h','CO',\"NO2\",'SO2',\"aqi\"]fe_dict = {\"pm25\":1,\"aqi\":0, 'pm10':3, 'SO2':5,'NO2':7, 'O3':9,\"O3_8h\":11, \"CO\": 13}for k in range(0,len(feature),1): data_2017 = {\"date\":[],'hour':[],} for i in range(0,len(city_columns),1): data_2017[city_columns[i]] = [] for file in files[0:]: filename,extname = os.path.splitext(file) if (extname == \".csv\"): datafile =file f_day = pd.read_csv(datafile,encoding='utf_8') city_columns = list(f_day.columns[3:]) for i in range(0,len(f_day),15): datetime = str(f_day[\"date\"].iloc[i]) hour = \"%02d\" % ((f_day[\"hour\"].iloc[i])) data_2017[\"date\"].append(datetime) data_2017[\"hour\"].append(hour) for t in range(0,len(city_columns),1): data_2017[city_columns[t]].append(f_day[city_columns[t]].iloc[i+fe_dict[feature[k]]]) print feature[k] data_2017 = pd.DataFrame(data_2017) hdf.put(feature[k], data_2017, format='table', encoding=\"utf-8\") åœ¨æ­¤å¤„åˆ†äº«æˆ‘å·²è½¬åˆ¶çš„æ•°æ®æ–‡ä»¶ï¼Œæ¬¢è¿å¤§å®¶å­¦ä¹ ã€ä½¿ç”¨ï¼š 2017å¹´ æ‰€æœ‰ç«™ç‚¹/æ‰€æœ‰åŸå¸‚ 2016å¹´ æ‰€æœ‰ç«™ç‚¹/æ‰€æœ‰åŸå¸‚ 2015å¹´ æ‰€æœ‰ç«™ç‚¹/æ‰€æœ‰åŸå¸‚ å…¨å›½å›½æ§ç¯å¢ƒç›‘æµ‹ç‚¹åˆ—è¡¨åŠç»çº¬åº¦ 3. HDFè¯»å–åº”ç”¨ä¾‹ä¸¾ 3.1 æå–æŸä¸ªåŸå¸‚çš„ç«™ç‚¹æµ“åº¦ä¿¡æ¯12345678910111213import pandas as pdfrom pandas import ExcelWriterimport sys,csv,os#### SELECT THE SPECIES ####feature = ['pm25', 'pm10', 'O3', 'O3_8h', 'CO', 'NO2', 'SO2', 'aqi']writer = ExcelWriter('./city_2017.xlsx')for i in range(0,len(feature),1): df = pd.read_hdf(\"./site_2017_whole_year.h5\",\\ feature[i], encoding = 'utf-8') df = df[['date','hour','1744A','1745A','1746A','1747A']] df.to_excel(writer,feature[i], index = False)writer.save() 3.2 å…¨å›½PM2.5å¹´å‡æµ“åº¦åˆ†å¸ƒ123456789101112131415161718192021222324252627282930313233343536#libraryfrom mpl_toolkits.axes_grid1 import make_axes_locatableimport cartopyfrom cartopy.io.shapereader import Readerfrom cartopy.feature import ShapelyFeaturefrom mpl_toolkits.basemap import cm as base_cm#Load the data select the variabledata_city = {\"city\":[],'pm25':[],'pm10':[]}pm25 = pd.read_hdf(\"./../data/city_2017_whole_year.h5\",'pm25', encoding = 'utf-8')pm10 = pd.read_hdf(\"./../data/city_2017_whole_year.h5\",'pm10', encoding = 'utf-8')for i,t in enumerate(pm25.columns[2:]): data_city['city'].append(t) data_city['pm25'].append(pm25[t].mean()) data_city['pm10'].append(pm10[t].mean()) data_city = pd.DataFrame(data_city) ## Add the lon,lat for each city## åŠ è½½æ‰€æœ‰å›½æ§ç«™ç‚¹åç§°ï¼Œä½ç½®çš„xlsæ–‡ä»¶ï¼Œå·²ä¸Šä¼ è‡³google driveï¼Œä¸‹è½½é“¾æ¥è§ä¸Šæ–‡Df = pd.ExcelFile(\"./ç«™ç‚¹åˆ—è¡¨å«ç»çº¬åº¦-1497ä¸ª.xlsx\", )df = Df.parse(u'Sheet1')data_city['lon'] = [df[df[u'åŸå¸‚'].str[0:2]==k[0:2]][u'ç»åº¦'].mean() for k in data_city['city']]data_city['lat'] = [df[df[u'åŸå¸‚'].str[0:2]==k[:2]][u'çº¬åº¦'].mean() for k in data_city['city']]#Plotting fig = plt.figure(figsize=(6, 5))ax = plt.subplot(projection=ccrs.PlateCarree())ax.set_extent([73, 135, 17, 55])ax.coastlines(linewidth = 0.2)ax.add_feature(cartopy.feature.OCEAN)ax.add_feature(cartopy.feature.BORDERS,linewidth = 0.4)pm25_plot = ax.scatter(data_city['lon'],data_city['lat'],s = 45,zorder = 2,c = data_city['pm25'],\\ cmap=base_cm.GMT_seis_r, lw = 0,alpha = 0.9) ä¾‹å›¾","link":"/2018/04/04/8.%5BDataset%5DChina%20air%20quality%20data%20in%20HDF5%20format/"},{"title":"Linear Fitting with Matplotlib and Plotly","text":"Linear regression is a fundamental method in data analysis to understand the relationship between two variables. Here, I summarize four reusable Python functions for performing and visualizing linear fitting: Matplotlib: with and without intercept Plotly: with and without intercept All methods: Drop NaN values in x and y Plot a scatter graph Fit a linear line Annotate the equation and correlation coefficient (R) 1. Matplotlib â€” Linear Fit with Intercept123456789101112131415161718192021222324import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import linregressdef add_mpl_subplot_with_intercept(ax, df, x_col, y_col, color='blue'): x = df[x_col] y = df[y_col] valid = ~(x.isna() | y.isna()) x = x[valid] y = y[valid] ax.scatter(x, y, color=color, alpha=0.6) # Linear regression with intercept slope, intercept, r_value, _, _ = linregress(x, y) line_x = np.linspace(x.min(), x.max(), 100) line_y = slope * line_x + intercept ax.plot(line_x, line_y, 'r--') # Annotate equation and R eq_text = f\"y = {slope:.2f}x + {intercept:.2f}\\nR = {r_value:.2f}\" ax.annotate(eq_text, xy=(0.05, 0.95), xycoords='axes fraction', ha='left', va='top', fontsize=10, color='black') 2. Matplotlib â€” Linear Fit without Intercept1234567891011121314151617181920def add_mpl_subplot_no_intercept(ax, df, x_col, y_col, color='green'): x = df[x_col] y = df[y_col] valid = ~(x.isna() | y.isna()) x = x[valid] y = y[valid] ax.scatter(x, y, color=color, alpha=0.6) # Linear regression without intercept slope = np.sum(x * y) / np.sum(x ** 2) r_value = x.corr(y) line_x = np.linspace(x.min(), x.max(), 100) line_y = slope * line_x ax.plot(line_x, line_y, 'r--') eq_text = f\"y = {slope:.2f}x\\nR = {r_value:.2f}\" ax.annotate(eq_text, xy=(0.05, 0.95), xycoords='axes fraction', ha='left', va='top', fontsize=10, color='black') 3. Plotly â€” Linear Fit with Intercept1234567891011121314151617181920212223242526272829303132333435363738import plotly.graph_objects as gofrom scipy.stats import linregressdef add_plotly_subplot_with_intercept(fig, df, x_col, y_col, row, col, color='blue', show_eq=True): x = df[x_col] y = df[y_col] valid = ~(x.isna() | y.isna()) x = x[valid] y = y[valid] fig.add_trace(go.Scatter( x=x, y=y, mode='markers', marker=dict(color=color), showlegend=False ), row=row, col=col) slope, intercept, r_value, _, _ = linregress(x, y) line_x = np.linspace(x.min(), x.max(), 100) line_y = slope * line_x + intercept fig.add_trace(go.Scatter( x=line_x, y=line_y, mode='lines', line=dict(color='red', dash='dash'), showlegend=False ), row=row, col=col) if show_eq: eq_text = f\"y = {slope:.2f}x + {intercept:.2f}&lt;br&gt;R = {r_value:.2f}\" fig.add_annotation( text=eq_text, xref=f'x{col}', yref=f'y{row}', x=x.min() + 0.05 * (x.max() - x.min()), y=y.max() - 0.05 * (y.max() - y.min()), showarrow=False, font=dict(size=11, color=\"black\"), align=\"left\", row=row, col=col ) 4. Plotly â€” Linear Fit without Intercept123456789101112131415161718192021222324252627282930313233343536def add_plotly_subplot_no_intercept(fig, df, x_col, y_col, row, col, color='green', show_eq=True): x = df[x_col] y = df[y_col] valid = ~(x.isna() | y.isna()) x = x[valid] y = y[valid] fig.add_trace(go.Scatter( x=x, y=y, mode='markers', marker=dict(color=color), showlegend=False ), row=row, col=col) slope = np.sum(x * y) / np.sum(x ** 2) r_value = x.corr(y) line_x = np.linspace(x.min(), x.max(), 100) line_y = slope * line_x fig.add_trace(go.Scatter( x=line_x, y=line_y, mode='lines', line=dict(color='red', dash='dash'), showlegend=False ), row=row, col=col) if show_eq: eq_text = f\"y = {slope:.2f}x&lt;br&gt;R = {r_value:.2f}\" fig.add_annotation( text=eq_text, xref=f'x{col}', yref=f'y{row}', x=x.min() + 0.05 * (x.max() - x.min()), y=y.max() - 0.05 * (y.max() - y.min()), showarrow=False, font=dict(size=11, color=\"black\"), align=\"left\", row=row, col=col )","link":"/2024/02/10/%5BCode-library%5DLinear_fitting_plotting_code_summary/"},{"title":"GEOS-Chem Installation and Run log","text":"In this post, I document the complete process of installing and configuring the GEOS-Chem Classic model, including environmental setup, compilation, and runtime configuration. The log is based on my experience running GEOS-Chem on a Linux HPC system, and covers both the challenges I encountered and the solutions I implemented. 1. Environmental SetupGEOS-Chem requires a proper build environment. 12345export DIR=/data/user/hao_y/libexport HDF5_DIR=$DIR/hdf5export NETCDF_DIR=$DIR/netcdfexport PATH=$DIR/gcc-13.2.0/bin:$HDF5_DIR/bin:$NETCDF_DIR/bin:$PATHexport LD_LIBRARY_PATH=$DIR/gcc-13.2.0/lib64:$HDF5_DI 2. GEOS-Chem Classic initiiation2.1 Clone and Initialize123git clone --recurse-submodules https://github.com/geoschem/GCClassic.gitcd GCClassic/run./createRunDir.sh 2.2 Run Directory ConfigurationMy setting is as follows: Simulation: Full chemistry Complex SOA with semivolatile POA Meteorology: GEOS-FP Resolution: 0.25Â° Ã— 0.3125Â° Domain: Asia (manually adjusted to South Asia later) Vertical Levels: 72 (native) 2.3 CompilationGo into the run directory and install 12345cd runs/gc_025_SAsia_2018/gc_025x03125_AS_geosfp_fullchem_complexSOA/cd buildcmake ../CodeDir -DRUNDIR=..make -j 16make install After building, adjust these configuration files:geoschem_config.yml,HISTORY.rc,HEMCO_Config.rc 3. Input Data Handling3.1 Dry Run and AWS SetupTo determine required meteorological and restart data: 123456789./gcclassic --dryrun | tee log.dryrun# install the AWS service command line service tool.curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"unzip awscliv2.zip./aws/install -i ~/aws-cli -b ~/binaws s3 ls --no-sign-request s3://geos-chem/./download_data.py log.dryrun geoschem+aws 3.2 Download the data1./download_data.py log.dryrun geoschem+aws 3.3 Fixing restart file timestamp errorsSometimes AWS pulls a misaligned restart file. Manually download the correct one, for example: 1wget http://geoschemdata.wustl.edu/ExternalShare/geos-chem/10yr_benchmarks/14.0.0/GCClassic/Restarts/2018/GEOSChem.Restart.20181001_0000z.nc4 Place it into the Restarts/ directory of your run folder. 4. SLURM Run Script (run_gcclassic.slurm)Simply run by sbatch run_gcclassic.slurm to start the simulation, and there will be email to remind when the simulation was done. 1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash#SBATCH -c 256 # Request 16 cores#SBATCH -N 1 # Use 1 node#SBATCH -t 6-00:00 # Max runtime of 12 hours#SBATCH -p general # &lt;-- Replace iwith actual partition name#SBATCH --mem=400000 # Request 400 GB memory#SBATCH --job-name=gcclassic_run#SBATCH --output=gcclassic_%j.out#SBATCH --error=gcclassic_%j.err#SBATCH --mail-type=END#SBATCH --mail-user=yufang.hao@psi.ch # &lt;-- Add this if you want email alerts################################################################################## GEOS-Chem Classic run script for SLURM (MERLIN cluster)################################################################################ Load your GCC 13.2.0 environmentsource ~/.bashrc# Optionally load other environment modules if needed (HDF5, NetCDF, etc.)# Remove stack size limit to prevent segfaultsulimit -s unlimited# (Optional) log current limitsulimit -a# Set OpenMP threadsexport OMP_STACKSIZE=1Gexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK# Run GEOS-Chem Classicsrun -c $OMP_NUM_THREADS time -p ./gcclassic &gt;&gt; GC.log# Exitexit 0 5. Debugging and Errors5.1 Segmentation FaultIf encoutering:Investigated via gdb, pinpointed to cldj_fjx_sub_mod.F90, line 1457: 1DD(I,J,L,K) = -E(I,J)*C(J,L,K) Likely due to unallocated or incorrectly dimensioned array. I found increase the memory could be helpful, so in the Increased --mem=64000 (64 GB) to --mem=400000. Ensured ulimit -s unlimited applied 6. Output Control and Optimization6.1. Optmization of exectuable speed In HISTORY.rc, I disabled species output to save time and space. In geoschem_config.yml, 12autoreduce_solver: activate: true This can help reduce runtime by simplifying the chemical solver dynamically. 7. Job MonitoringSubmit: 1sbatch run_gcclassic.slurm Monitor: 12squeue -u $USERscontrol show job &lt;JOBID&gt; Stop 1scancel &lt;JOBID&gt; Check logs: 12cat gcclassic_&lt;JOBID&gt;.outcat gcclassic_&lt;JOBID&gt;.err A simple example of the results References GEOS-Chem User Guides and DocumentationGEOS-Chem Official Website: https://geos-chem.orgUser Documentation: https://geos-chem.readthedocs.io GEOS-Chem GitHub RepositorySource code and run directory setup: https://github.com/geoschem/GCClassic GEOS-Chem Input Data on AWShttps://registry.opendata.aws/geos-chem AWS CLI Installation Guidehttps://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html GEOS-Chem Error Troubleshooting Guidehttps://geos-chem.readthedocs.io/en/14.2.2/geos-chem-shared-docs/supplemental-guides/error-guide.html","link":"/2025/07/22/%5BModel%5DGEOS_Chem_Classic_South_Asia_Simulation_Tutorial/"},{"title":"Installing FLEXPART v11 with ECMWF Support","text":"This post walks through a full FLEXPART v11 installation for ECMWF meteorological data with NetCDF and Fortran support, including resolving common compilation issues. All components were built from source on a Linux HPC system with user-level access. âš™ï¸ Installation Structure12export DIR=/data/user/hao_y/libexport INSTALL_DIR=$DIR All components (GCC, HDF5, NetCDF, ecCodes, etc.) will be installed under this directory tree. ğŸ”§ Step-by-Step Build Pipeline1. GCC 13.2.0 (with Fortran Support)12345678wget https://ftp.gnu.org/gnu/gcc/gcc-13.2.0/gcc-13.2.0.tar.gztar -xzf gcc-13.2.0.tar.gzcd gcc-13.2.0./contrib/download_prerequisitesmkdir build &amp;&amp; cd build../configure --prefix=$DIR/gcc-13.2.0 --enable-languages=c,c++,fortran,go --disable-multilibmake -j$(nproc)make install Export compilers: 123456789export PATH=$DIR/gcc-13.2.0/bin:$PATHexport LD_LIBRARY_PATH=$DIR/gcc-13.2.0/lib64:$LD_LIBRARY_PATHexport CC=gccexport CXX=g++export FC=gfortranexport F77=gfortranexport CFLAGS='-gdwarf-2 -gstrict-dwarf'export FCFLAGS=-m64export FFLAGS=-m64 2. HDF5 1.14.612345678wget https://support.hdfgroup.org/releases/hdf5/v1_14/v1_14_6/downloads/hdf5-1.14.6.tar.gztar -xzf hdf5-1.14.6.tar.gzcd hdf5-1.14.6./configure --prefix=$DIR/hdf5 --enable-cxx --enable-fortran --enable-sharedmake -j$(nproc)make installh5cc -showconfig ## check whether it is installed properly 3. NetCDF (C + Fortran)3.1 NetCDF-C1234567891011wget https://github.com/Unidata/netcdf-c/archive/refs/tags/v4.9.3.tar.gz -O netcdf-c-4.9.3.tar.gztar -xzf netcdf-c-4.9.3.tar.gzcd netcdf-c-4.9.3./configure --prefix=$DIR/netcdf \\ CPPFLAGS=\"-I$DIR/hdf5/include\" \\ LDFLAGS=\"-L$DIR/hdf5/lib\" \\ --enable-netcdf-4 --disable-dapmake -j$(nproc)make installnc-config --all ## check whether it is installed properly 3.2 NetCDF-Fortran123456wget https://github.com/Unidata/netcdf-fortran/archive/refs/tags/v4.6.1.tar.gz -O netcdf-fortran-4.6.1.tar.gztar -xzf netcdf-fortran-4.6.1.tar.gzcd netcdf-fortran-4.6.1./configure --prefix=$DIR/netcdfmake -j$(nproc)make install 4. OpenMPI (Optional for MPI mode)12345678wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.6.tar.gztar -xzf openmpi-4.1.6.tar.gzcd openmpi-4.1.6./configure --prefix=$DIR/libs/openmpimake -j$(nproc)make installexport PATH=$DIR/libs/openmpi/bin:$PATHexport LD_LIBRARY_PATH=$DIR/libs/openmpi/lib:$LD_LIBRARY_PATH 5. AEC Library (Required by ecCodes)123456wget https://gitlab.dkrz.de/k202009/libaec/-/archive/v1.0.6/libaec-v1.0.6.tar.gztar -xzf libaec-v1.0.6.tar.gzcd libaec-v1.0.6 &amp;&amp; mkdir build &amp;&amp; cd buildcmake .. -DCMAKE_INSTALL_PREFIX=$DIR/libaecmake -j$(nproc)make install 6. ecCodes (with Fortran and NetCDF)1234567wget https://confluence.ecmwf.int/download/attachments/45757960/eccodes-2.33.0-Source.tar.gztar -xzf libaec-v1.0.6.tar.gz# Unpack and buildcd eccodes-2.33.0-source &amp;&amp; mkdir build &amp;&amp; cd buildcmake .. -DCMAKE_INSTALL_PREFIX=$DIR/eccodes -DCMAKE_PREFIX_PATH=\"$DIR/netcdf;$DIR/hdf5;$DIR/libaec\" -DENABLE_NETCDF=ON -DENABLE_FORTRAN=ON -DCMAKE_BUILD_TYPE=Releasemake -j$(nproc)make install ğŸ“ƒ Environment Exports SummarySet these variables in your .bashrc or shell: 1234567891011121314151617181920212223242526272829303132export PATH=$DIR/gcc-13.2.0/bin:$PATHexport LD_LIBRARY_PATH=$DIR/gcc-13.2.0/lib64:$LD_LIBRARY_PATHexport CC=gccexport CXX=g++export FC=gfortranexport F77=gfortranexport FFLAGS=-m64# HDF5export HDF5_DIR=$DIR/hdf5export CPATH=$HDF5_DIR/include:$CPATHexport LIBRARY_PATH=$HDF5_DIR/lib:$LIBRARY_PATHexport LD_LIBRARY_PATH=$HDF5_DIR/lib:$LD_LIBRARY_PATH# NetCDFexport NETCDF_DIR=$DIR/netcdfexport CPATH=$NETCDF_DIR/include:$CPATHexport LIBRARY_PATH=$NETCDF_DIR/lib:$LIBRARY_PATHexport LD_LIBRARY_PATH=$NETCDF_DIR/lib:$LD_LIBRARY_PATHexport PKG_CONFIG_PATH=$NETCDF_DIR/lib/pkgconfig:$PKG_CONFIG_PATH# AECexport CPATH=$DIR/libaec/include:$CPATHexport LIBRARY_PATH=$DIR/libaec/lib:$LIBRARY_PATHexport LD_LIBRARY_PATH=$DIR/libaec/lib:$LD_LIBRARY_PATH# ecCodesexport CPATH=$DIR/eccodes/include:$CPATHexport LIBRARY_PATH=$DIR/eccodes/lib:$LIBRARY_PATHexport LD_LIBRARY_PATH=$DIR/eccodes/lib:$LD_LIBRARY_PATHexport PATH=$DIR/eccodes/bin:$PATHexport PKG_CONFIG_PATH=$DIR/eccodes/lib/pkgconfig:$PKG_CONFIG_PATH ğŸŒ FLEXPART v11 Compilation (ECMWF Mode)12345wget https://gitlab.phaidra.org/flexpart/flexpart/-/archive/v11/flexpart-v11.tar.gztar -xzf flexpart-v11.tar.gzcd flexpart-v11# the place to change in the ./src/makefile_gfortran is the F90 = /data/user/hao_y/lib/gcc-13.2.0/bin/gfortran make -f makefile_gfortran -j4 eta=no ncf=yes eta=no: Use hybrid ECMWF coordinate system ncf=yes: Enable NetCDF support âŒ Common Errors and Fixes Error Cause Solution cannot find -leccodes_f90 ecCodes built without Fortran Add -DENABLE_FORTRAN=ON AEC library not found libaec missing Install libaec and add to CMAKE_PREFIX_PATH GLIBCXX_3.4.30 not found Old libstdc++ in system Build newer GCC (&gt;=13) and use its lib path gfortran: command not found GCC built without Fortran Rebuild with --enable-languages=c,c++,fortran ğŸ“… Final Notes Think of ./configure as the blueprint, make as construction, and make install as moving into your house and wiring the electricity.You now have a clean, custom-built FLEXPART environment with all dependencies resolved, ready to simulate ECMWF-based transport at scale. when copy the setting from another PC into the release folder, you need to change the permission of the folder by typing chmod -R 755 ./options and chmod 644./options/RELEASES or any other files. For multi-core processing, you can use the export OMP_NUM_THREADS=4 to set 4 cores (can set for more) and then with ./FLEXPART command to run FLEXPART with multiple processes. Sometimes, we will find error of â€œProgram received signal SIGSEGV: Segmentation fault - invalid memory reference.â€ By typing ulimit -s unlimted, we can solve this problem. Reference FLEXPART GitLab repositoryhttps://gitlab.phaidra.org/flexpart/flexpart(Main source code and Makefiles for FLEXPART v10/v11) FLEXPART Installation Guide (official wiki)https://www.flexpart.eu/wiki/FpInstallation(Overview of FLEXPART compilation across versions and systems) ECMWF ecCodes documentationhttps://confluence.ecmwf.int/display/ECC/ecCodes+Home(Build instructions, environment variables, and sample readers)","link":"/2025/06/20/%5BModel%5DFLEXPART.V11_ECMWF_supported_installation/"},{"title":"Useful python scripts for daily uses","text":"Welcome to my collection of Python scripts and code snippets for everyday use. This document contains a variety of scripts and code snippets that can be used to automate repetitive tasks, process data, and perform various other operations. Whether you are a beginner or an experienced Python programmer, this collection of scripts and code snippets will provide you with useful tools and shortcuts for your everyday work. In this document, you will find code snippets and scripts for tasks such as: Data processing File manipulation Web scraping Automating tasks And more! This document is organized into sections for easy navigation, so you can quickly find the script or code snippet that you need. Many of them were collected from stackoverflow. On 2022.03.31, I re-organize all the scripts into Python 3.x version and post again.On 2023.02.12, I edited the whole content with help from ChatGPT 1.Data processing 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368#1. Find the duplicated values in one columnfrom collections import Counter print ([item for item, count in Counter(df[\"Sample\"].values).items() if count &gt;1] )#2. Build new dataframe based on the user-defined column names### N: lengthdef new_df(N, LIST): df = pd.DataFrame(np.nan, index=np.arange(0,N,1), columns=['A']) for i,t in enumerate(LIST): df[LIST[i]] = np.nan df = df.drop(['A'],1) #3. Change the integer to N digits ## method 1# Method 1df.MONTH.apply(\"{0:0=2d}\".format)# Method 2\"%02d\" &amp; (string)#4. Replace the values in specific columnspd.options.mode.chained_assignment = Nonedf.loc[condition function, 'column'] = xxx #5. Find the index of the nearest value to a pre-defined inputdef find_nearest(array,value): idx = (np.abs(array-value)).argmin() return array[idx] #6. Test a value if it is NaN: def isNaN(num): return num != num#7. Test dataframe if it is emptydf.empty==True#8. Zip two pandas columns into 2-d arrayfrom itertools import product c = list(product(df.lon.values, df.lat.values)) #9. Manipulate the string## Add an elementlist.insert(0, 'A')## Delete an element by indexlist.pop(2)## Delete an element by its valuelist.remove('A')#10. Assign value to specific location of a dataframe## Method 1 - the fastestdf.set_value('row index', 'column name', value)## Method 2 - moderate speeddf['column name']['row index'] = value## Method 3 - the slowestdf.at('column name', 'row index') = value#11. Group the dataframe with user-defined range settingbins = [0,50, 60, 70, 80, 90,100]group_names = ['&lt;50', '50-60', '60-70', '70-80','80-90','&gt;90']df['categories'] = pd.cut(df['RH'], bins, labels=group_names)## If we want to plot them group-name wisedfig = plt.figure()for i in range(0,len(categories.unique())-1,1): ddf = df[df['categories'] == categories.unique()[i]] plt.scatter(ddf['PM2.5'], ddf['VISIBILITY'], alpha = 0.45, label ='RH: '+ roup_names[i]) popt, pcov = curve_fit(func, ddf[\"PM2.5\"], ddf[\"VISIBILITY\"]) xdata = np.arange(0, 300, 4) plt.plot(xdata, func(xdata, *popt), zorder =3)plt.legend(ncol = 2) plt.show() #12. Transform the list to stringimport astx = u'[ \"A\",\"B\",\"C\",\"D\"]'x = ast.literal_eval(x)print x # Output: ABCD#13. Subset of pandas dataframe by datetime rangemask = (met['Date'] &lt; '2016-03-01')|(met['Date'] &gt;= '2016-12-01')met_sub = met.locmask#14. Sort one column by values of another column (ranking species by their concentrations)SE_COL = ['Cr', 'Co', 'Ni', 'Cu', 'Zn', 'As', 'Se', 'Mo', 'Cd', 'Pb', 'V', 'Mg', 'Ca', 'K', 'Ti', 'Mn', 'Fe', 'Ba', 'Sr']SE_COL = [SE_COL[t] for t in np.argsort(np.array([vals.dropna().mean() for col, vals in pm25_ef.items()]))]#15. Drop np.nan and then find the 10th percentilev = df['value'].notna()ra_ = df.loc[v, 'value'].valuesse_index = ra_.argsort().argsort()[:int(0.1len(df))]sorted(ra_)[int(0.1len(ra_))]#16. Rename the columns of dataframedf.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)#17. Delete specific columnscolumns = ['Col1', 'Col2', ...]df.drop(columns=columns, inplace=True, axis = 1)#18. Add new columnsdf[new_column_name] = pd.Series(data, index=df.index)#19. Delete the rows with np.nan## Method 1df = df[~np.isfinite('col1')]## Method 2df.dropna(subset=['col1'], inplace=True)#20. Sort the values from the biggestvar_exp = sorted(eigen_vals, reverse=True)#21. Saving a CSV file containing Chinese characters##To save a CSV file containing Chinese characters, you need to make sure that the file is encoded in UTF-8 format.import sysreload(sys)sys.setdefaultencoding('utf-8') df.to_csv('file.csv', encoding='utf-8-sig', index=False)#22. Adding a day to a datetime object## To add one day to a datetime object, you can use the pd.to_timedelta() function. For example:date = pd.to_datetime('2017-11-01')new_date = date + pd.to_timedelta(1, 'D')#23. Getting the keys and values of a dictionarykeys = [key for key, value in dict_.items()]values = [value for key, value in dict_.items()]#24. Binning data with natural breakfrom pysal.esda.mapclassify import Natural_Breaks as nbbreaks = nb(df.value,k=5)class_ = pd.DataFrame({'class': breaks.yb}, index=df[df['value'].notnull()].index)df = df.join(class_)df.class.fillna(-1, inplace=True)#25. Printing a pandas DataFrame in Markdown formatfrom tabulate import tabulateprint(tabulate(response_df, tablefmt=\"pipe\", headers=\"keys\", showindex=False))#26. saving the 2-d array in txt and read itnp.savetxt(\"mask_PM_0.1x0.1.txt\",mask)mask = np.loadtxt('./mask_PM_0.1x0.1.txt')#27. np.array dropna for the NaN of ratios t = np.array([(a/b) for a, b in zip(dyn_w[spec].values,dyn_s[spec].values)])t = t[~np.isnan(t)]# 28. How to select the data points within specific area### é•¿ä¸‰è§’yrd_area = fiona.open(\"./data/shp/YRD/yrd_b.shp\")pol = yrd_area.next()from shapely.geometry import shapegeom = shape(pol['geometry'])poly_data = pol[\"geometry\"][\"coordinates\"][0][0]#[0:12]poly = Polygon(poly_data)yrd_mask = np.array([poly.contains(Point(x,y)) for x,y in zip(data_city['lon'],data_city['lat'],)])data_yrd = data_city[yrd_mask]data_yrd = data_yrd.reset_index()# 29. å­˜å‚¨ä¸­æ–‡xlsæŠ¥é”™é—®é¢˜å¤„ç†df = pd.read_csv('./xxx.txt', delimiter = \"\\t\",encoding='utf-8')with pd.ExcelWriter('xxx.xls',options={'encoding':'utf-8'}) as writer: # df.to_excel(writer, sheet_name=u'sheet1')# 30. split the list by specfic string (e.g., quotes)[s for s in l.split('\"') if s.strip() != ''][1]# 31. replace specific values by condition df.loc[df['COLA']!='XXX','COLB']=yyy# 32. Select subset of netcdf file by specific attributesimport netCDF4 as ncval_ = ['HGT_M', 'XLONG_C','XLAT_C']with netCDF4.Dataset(\"geo_em.d01.nc\") as src, netCDF4.Dataset(\"out.nc\", \"w\") as dst: # copy global attributes all at once via dictionary dst.setncatts(src.__dict__) # copy dimensions for name, dimension in src.dimensions.items(): dst.createDimension( name, (len(dimension) if not dimension.isunlimited() else None)) # copy all file data except for the excluded for name, variable in src.variables.items(): if name in val_: x = dst.createVariable(name, variable.datatype, variable.dimensions) dst[name][:] = src[name][:] # copy variable attributes all at once via dictionary dst[name].setncatts(src[name].__dict__)# 33. combining the listscode_ = []for pr in pc_code: code_ = list(itertools.chain.from_iterable([code_,[t['code'] for t in pr['children']]])) # 34. 2-Dæ•°æ®æ’å€¼#å‘æ–°ç½‘æ ¼è¿›è¡Œæ’å€¼from scipy import interpolatehfunc = interpolate.interp2d(lon_dust_sub,lat_dust_sub,PM_dust)new_dust = np.zeros(len(lat_anthr_sub)*len(lon_anthr_sub))t = 0 for i in range(0,len(lat_anthr_sub),1): for j in range(0,len(lon_anthr_sub),1): new_dust[t] = hfunc(lon_anthr_sub[j],lat_anthr_sub[i]) t+=1dust_int = new_dust.reshape(len(lat_anthr_sub),len(lon_anthr_sub))# 35. æ’åºè·å¾—å…¶åºåˆ—indexsorted(range(len(s)), key=lambda k: s[k])# 39. Sortæ—¶æ³¨æ„æ’åºsorter = source_df_contr_cr.source = df_contr_cr.source.astype(\"category\")df_contr_cr.source.cat.set_categories(sorter, inplace=True)df_contr_cr = df_contr_cr.sort_values([\"source\"]) df['categories'].value_counts().reindex(group_names)# 39. åˆ†é’Ÿæ•°æ®è½¬åŒ–ä¸ºå°æ—¶æ•°æ®df2_re = df2.groupby(np.arange(len(df2))//60.0).mean()# 40. æœ‰æ•ˆä½æ•°è¾“å‡ºto_clipboard(float_format = '%.4f')# 41.è‡ªç„¶åŸºé‡‘æŠ¥å‘Šä¸‹è½½import osfor i in range(0,100,1): os.system(\"wget http://output.nsfc.gov.cn/report/41/41205122_%i.png\" %i) import osimport natsortfrom fpdf import FPDFfiles = os.listdir('./')files = natsort.natsorted(files, reverse = False)files = files[0:]pdf = FPDF()for image in files[0:]: if image[-3:]=='png': pdf.add_page() pdf.image(image,0,0,210,297) pdf.output(\"../hh.pdf\", \"F\") # 43. ç¼ºå¤±å€¼ç›‘æµ‹pd.isnull()dataframe[xxx].values.isnull()# 44. If Else in list comprehension[x for x in list(df.columns[2:]) if x not in ['é‡åº†','å±±è¥¿','è¥¿è—']]# 45. Two variables in list comprehension (double iteration)# 46. assign two value to pandas columns with if condition.contr_df['type'] = contr_df['sample'].apply(lambda x:10 if x[0:7] ==\"IITK-20\" else \"x\")# 46. txt file reading with multiple delimiter and skiplinesdf = pd.read_csv('./data/210717_Lucknow_Day1.txt',delimiter='\\t',skiprows=[i for i in range(0,10)])# 47. add multiple lists as the columns of new dataframedpara = pd.DataFrame(list(zip(t_series,rn_series,minErr1ion)),columns=['t_series','rn_series','minErr1ion'])# 48. replace all the quotation mark in the columnspecies_name = [str(c.rstrip().split(\"\\t\")[1].replace('\"', '')) for c in species_info1]# 49. Kruskal-Wallis testfrom scipy import statsstats.kruskal(dd['Glass vial'],dd['Plastical vial'],dd['Dilution water'])# 50. list substractionlist(set( list(dafr_no3.columns)) - set(spike_cols)-set(['time'])# 51. Assign 2-d matrix into pandas dataframe with column name dafr_org = pd.DataFrame(org_, columns=df_org['Parameter'].attrs['ExactMassText'].astype(str))# 52. divide string based on chemical formula import rec = 'C2H3O'splitted = list(re.split(\"([A-Z][a-z]*)\",c))keyss = list(filter(lambda a: a[0].isupper() if a else False, splitted))values = list(filter(lambda a: a[0].isdigit() if a else False, splitted))spec_dict = dict(zip(keyss,values)) # 53. check string type in columnsna_em_power.applymap(type)==str# 54. Sorting files by specific strings witin the filenamedef last_4chars(x): return(x[-25:])sorted(imgs, key = last_4chars) # 55. split string by more than one delimitersimport rere.split('; |, ', string_to_split)# 56. Pandas rename multiple columnsdf_area.rename(columns=dict(zip(df_area.columns[:], mz_list)),inplace=True)# 57. Pandas check whether the whole column is np.nanfor c in mz_list[0:]: if df_area[c].isnull().all(): print(\"All values in the column 'B' are NaN\")# 58. Rename part of column names by conditiondef renaming_fun(a): if \"_unc_new_toc_scaled\" in a: return a.split('_')[0] +\"_unc\" # or None return aauto_df_fa.columns = [renaming_fun(col) for col in auto_df_fa.columns]# 58. Append 2-d array into 3-d array### https://stackoverflow.com/questions/43363641/building-a-3d-array-from-a-number-of-2d-arrays-with-numpyOC_emissions_daily = np.empty((1,720, 1440), int)daily_em_array = month_OC_emissions * daily_fractionOC_emissions_daily = np.vstack([OC_emissions_daily, daily_em_array[None]])# 59. processing pandas multiple columns df[df.columns[2:]] = df[df.columns[2:]].multiply(spi_conc/RIE/c, axis=\"index\")# 60. Lambda function### Apply function NumPy.square() to square the values of two rows 'A'and'Bdf2 = df.apply(lambda x: np.square(x) if x.name in ['A','B'] else x)# 61. Saving n-d array and reload via pickle methodimport numpy as npimport pickle# 62. create a large numpy arraydata = np.random.rand(1000, 1000, 100)# 63. save the numpy array to a file using picklewith open('data.pkl', 'wb') as f: pickle.dump(data, f)# 64. load the numpy array from the filewith open('data.pkl', 'rb') as f: loaded_data = pickle.load(f)# 65. check if the original and loaded numpy arrays are the sameprint(np.array_equal(data, loaded_data))# 66. Daily averagedf.set_index('datetime',inplace=True)df = df.rename(columns=new_column_names)daily_ave = df.resample('D').mean()# 67. Create a Pandas Excel writer using 'xlsxwriter' enginewith pd.ExcelWriter('my_excel_file.xlsx', engine='xlsxwriter') as writer: # Write dataframes df1 and df2 to two separate sheets in the same Excel file df1.to_excel(writer, sheet_name='Sheet1') df2.to_excel(writer, sheet_name='Sheet2') # 68. Simple way to classify chemical formuladef classify_chemicals(formula): if pd.isnull(formula): return np.nan else: elements = ['C', 'H', 'O', 'N', 'Na'\\] components = re.findall('\\[A-Z\\]\\[a-z\\]*', formula) # Split formula into components category = ''.join(\\[element for element in elements if element in components]) return category ds['Ion-rule1-class'] = ds['ion-rule1'].apply(classify_chemicals) 2. Data statistics 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#1. Curve fitting (y = ax + b)from scipy import statsimport numpy as npslope, intercept, r_value, p_value, std_err = stats.linregress(x,y)##1.1 remove the nan value first!mask = ~np.isnan(x_) &amp; ~np.isnan(y_)slope, intercept, r_value, p_value, std_err = stats.linregress(x_[mask], y_[mask])#2. Fitting by user-defined function from scipy.optimize import curve_fit def func(x, a,b): return ax**2+bx#+0.55def get_r(f,x,y,popt): residuals = y- f(x, *popt) ss_res = np.sum(residuals**2) ss_tot = np.sum((y-np.mean(y))**2) r_squared = 1 - (ss_res / ss_tot) return r_squaredpopt, pcov = curve_fit(func, df['x'], df['y']) print \"## f(x) = ax*2 +bx ##\"print \"## a##\"print poptprint \"## r-square ##\"print get_r(func,df['x'], df['y'], popt )#3. pivot table df.groupby(['site']).get_group(df.site.unique()[i])['value'].mean()#4. Summarize the frequencyclu_d['Cluster'].value_counts().to_frame()clu_d['Cluster'].value_counts().index.tolist() #5. one-way ANOVA test# å¦‚ä½•ç»Ÿè®¡ seasonal anova differencespec_ = ['crustal','trace','OM',\"EC\",'SO42-','NO3-','Cl-','NH4+','n_K',]val_annual,val_sp,val_su,val_au,val_wi = [],[],[],[],[]for t in sp_pie: val_sp=df[df.season=='Spring'][t].dropna().values val_su=df[df.season=='Summer'][t].dropna().values val_au=df[df.season=='Autumn'][t].dropna().values val_wi=df[df.season=='Winter'][t].dropna().values print t print stats.f_oneway(val_sp,val_su, val_au,val_wi)# pearson correlation mask = ~np.isnan(df['Ma_re'])&amp;~np.isnan(df['PM25'])stats.pearsonr(df['Ma_re'][mask],df['PM25'][mask])# 6. D-N averagedsample_id = df_op[df_op['Sample ID'].str[-1] !='C']['Sample ID'].values#IITK-2018-02-12-N #IITK-2018-01-21-Nsample_id = [c for c in sample_id if c not in ['IITK-2018-02-12-N' ,'IITK-2018-01-21-N',]]df_op_c = df_op[~(df_op['Sample ID'].isin(sample_id))]df_op_c = df_op_c[df_op_c['Sample ID']!='IITK-2018-11-01-C']df_op_dn = pd.DataFrame()df_op_dn['Sample ID'] = [c[:-2]+'-C' for c in sample_id]df_op_dn['Date'] = [c[5:-2] for c in sample_id]for co in df_op.columns[2:]: val_ = [] for c in sample_id: val_.append(df_op[df_op['Sample ID'].str[:-2] == c[:-2]][co].mean()) df_op_dn[co] = val_ df_op_new = pd.concat([df_op_dn,df_op_c]).reset_index(drop=True) ## OM:OCfrom pyvalem.formula import Formuladef cal_omoc(ion): f = Formula(ion) atom_dict = f.atom_stoich n_c = atom_dict['C'] omoc_ratio = f.mass/(n_c*12.0) return omoc_ratiopro_pr['OM:OC'] = pro_pr['Ion'].apply(lambda ion: cal_omoc(ion)) 3. Data visualization 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537#1. Temperature symbolax.set_ylabel(u'Temperature (\\N{DEGREE SIGN}C)', fontweight='bold')#2. Match the length of colorbar with the figurefrom mpl_toolkits.axes_grid1 import make_axes_locatabledivider = make_axes_locatable(ax1)cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)cbar = plt.colorbar(cf, cax=cax) cbarlabel = \"xxxx\"cbar.ax.set_xlabel(cbarlabel,size = 8,labelpad=-35)#3. User-defined colorbar location and ticklabel edittingcbaxes = fig.add_axes([0.83, 0.125, 0.1, 0.01]) cbar = plt.colorbar(ss,cax=cbaxes,orientation='horizontal')loc_ = np.array([5,10,15,20,25])cbar.set_ticks(loc_)cbar.set_ticklabels(loc_)## two methods for ticklabel fontsize(1) cbar.ax.tick_params(labelsize=8)(2) cbar.ax.set_xticklabels(cbar.ax.get_xticklabels(), fontsize=8)cbar.ax.set_xlabel(r'$\\mathregular{OC/EC}$',fontsize = 8,labelpad = -28)#4. Chinese settingfrom matplotlib.font_manager import FontProperties mpl.rcParams['font.sans-serif'] = ['Microsoft YaHei']# å¾®è½¯é›…é»‘çš„ä¸­è‹±æ–‡æ··æ’æ•ˆæœè¾ƒå¥½plt.rcParams['axes.unicode_minus'] = False #5. User-defined axes settingdef stylize_axes(ax): ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.xaxis.set_tick_params(top='off', direction='out', width=1) ax.yaxis.set_tick_params(right='off', direction='out', width=1)#6. Plotting the first-order regression line## (x,y) is the original datasetsfrom scipy import statsslope, intercept, r_value, p_value, slope_std_error = stats.linregress(x, y)xx = np.arange(x.min(),x,max(),0.01)yy = slope*xx+interceptclabel = # make the font itatilc styleplt.plot(xx,yy,color ='k', lw =1.2, linestyle = \"-\", label = clabel)#7. Setting the two subplots shared the same aspect ratio## Noted that when one subplot using basemap or cartopy (geo-based coordinate), and the other subplot is in basic matplotlib style, there aspect ratio would not be the same. def get_aspect(ax): xlim = ax.get_xlim() ylim = ax.get_ylim() aspect_ratio = abs((ylim[0]-ylim[1]) / (xlim[0]-xlim[1])) return aspect_ratiofig = plt.figure(figsize=(12,8))ax1=plt.subplot(121, projection=ccrs.PlateCarree())ax2=plt.subplot(122)ax2.set_aspect(get_aspect(ax1) / get_aspect(ax2))#8. Fake legend markers#å‚è€ƒ https://jakevdp.github.io/PythonDataScienceHandbook/04.06-customizing-legends.htmlfor area in [100, 300, 500]: plt.scatter([], [], c='k', alpha=0.3, s=area, label=str(area) + ' km$^2$')plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='City Area')#9. Remove the plotting items and retain the label#10. Remove the tick lines while retain the ticklabelsax.tick_params(axis=u'both', which=u'both',length=0)#11. plotting two subplots with different sizefig = plt.figure(figsize=(7,4))##definitions for the axesleft, width = 0.07, 0.6bottom, height = 0.1, .8bottom_h = left_h = left+width+0.05rect_cones = [left, bottom, width, height]rect_box = [left_h, bottom, 0.5, height]ax1 = plt.axes(rect_cones,projection=ccrs.PlateCarree())ax2 = plt.axes(rect_box)ax2.set_aspect(get_aspect(ax1) / get_aspect(ax2))#12. fake circle legenddef plot_legend(title): s = [1,3,5,7,9] c = plt.cm.binary(np.arange(5)/5.0) labels =['A',\"B\",'C',\"D\",\"E\"] cir1 = plt.scatter([1], [2], c=c[0], alpha=0.8, s=s[0],label=r'&lt;0.5') cir2 = plt.scatter([1], [2], c=c[1], alpha=0.8, s=s[1],label=labels[1]) cir3 = plt.scatter([1], [2], c=c[2], alpha=0.8, s=s[2],label=labels[2]) cir4 = plt.scatter([1], [2], c=c[3], alpha=0.8, s=s[3],label=labels[3]) cir5 = plt.scatter([1], [2], c=c[4], alpha=0.8, s=s[4],label=labels[4]) leg = plt.legend([cir1,cir2,cir3,cir4,cir5],labels,scatterpoints = 1, \\ frameon=False, labelspacing=0.9, ncol =2, fontsize=8,title=title , loc = [0.05,0.1]) #13. seperated colors from colormapc_list = plt.cm.rainbow(np.arange(6)/6.0)# 14. set twin axis with different coloraxu = ax.twinx()axu.spines[\"right\"].set_position((\"axes\", 1.25))so_,=plt.plot(xxx)axu.yaxis.label.set_color(so_.get_color())axu.spines['right'].set_color(so_.get_color())axu.spines[\"right\"].set_edgecolor(so_.get_color())axu.tick_params(axis='y', colors=so_.get_color())# 15. add a rectanglefrom matplotlib.patches import Rectangleimport matplotlib.patches as mpatchesrec = mpatches.Rectangle((position[i]- width/2.0,bot_[i]),width,hei_[i],linewidth=1,edgecolor='b',facecolor='none',zorder=12) ax.add_patch(rec)# 16. output the color by pre-defined alphadef make_rgb_transparent(rgb, bg_rgb, alpha): return [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)]from matplotlib import colorsimport matplotlib.pyplot as pltalpha = 0.5kwargs = dict(edgecolors='none', s=3900, marker='s')for i, color in enumerate(['red', 'blue', 'green']): rgb = colors.colorConverter.to_rgb(color) rgb_new = make_rgb_transparent(rgb, (1, 1, 1), alpha) print(color, rgb, rgb_new) plt.scatter([i], [0], color=color, **kwargs) plt.scatter([i], [1], color=color, alpha=alpha, **kwargs) plt.scatter([i], [2], color=rgb_new, **kwargs)# 17. terrain mapdef truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100): new_cmap = colors.LinearSegmentedColormap.from_list( 'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n))) return new_cmapcmap = plt.get_cmap('terrain')terrain_cmap = truncate_colormap(cmap, 0.15, 0.9) # 18. ä¸ç­‰è·subplotsimport matplotlib.gridspec as gridspecfig = plt.figure(figsize=(9,6))gs = gridspec.GridSpec(46,1)row_xx = 23# pm25_pm10ax1 = plt.subplot(gs[0:row_xx, 0])# 19. two axis one legend# ask matplotlib for the plotted objects and their labelslines, labels = ax.get_legend_handles_labels()lines2, labels2 = ax2.get_legend_handles_labels()ax2.legend(lines + lines2, labels + labels2, loc=0)# 20. Cartopy longitude## æ–¹æ³•1ax.set_xticks([0, 60, 120, 180, 240, 300, 360], crs=ccrs.PlateCarree())dax.set_yticks([-90, -60, -30, 0, 30, 60, 90], crs=ccrs.PlateCarree())lon_formatter = LongitudeFormatter(zero_direction_label=True)lat_formatter = LatitudeFormatter()ax.xaxis.set_major_formatter(lon_formatter)ax.yaxis.set_major_formatter(lat_formatter)## æ–¹æ³•2ax.set_xticks(np.arange(extent[0]+2, extent[1]+2, 15), crs=ccrs.PlateCarree())ax.set_yticks(np.arange(extent[2]+3, extent[3], 15), crs=ccrs.PlateCarree())ax.set_xticklabels([r'$\\mathrm{75^o E}$',r'$\\mathrm{90^o E}$',r'$\\mathrm{105^o E}$',\\ r'$\\mathrm{120^o E}$',r'$\\mathrm{135^o E}$',])ax.set_yticklabels([r'$\\mathrm{20^o N}$',r'$\\mathrm{35^o N}$',r'$\\mathrm{50^o N}$'])# 21 'GeoAxesSubplot' object has no attribute '_hold'from matplotlib.axes import Axesfrom cartopy.mpl.geoaxes import GeoAxesGeoAxes._pcolormesh_patched = Axes.pcolormesh# 22.ä¸­è‹±æ–‡æ··æ’# # -*- coding: utf-8 -*-import matplotlib.pyplot as pltfrom matplotlib.font_manager import FontPropertiessong_ti = FontProperties(fname=r'/Library/Fonts/Songti.ttc', size=20)times_new_roman = FontProperties(fname=r'/Library/Fonts/Arial Black.ttf', size=15)ax = plt.gca()ax.set_title(u'èƒ½é‡éšæ—¶é—´çš„å˜åŒ–', fontproperties=song_ti)ax.set_xlabel('Time (s)', fontproperties=times_new_roman)ax.set_ylabel('Energy (J)', fontproperties=times_new_roman)plt.show()# 23. no-legend(label='_nolegend_')# 25. plot with grey backgroundax.set_facecolor(\"#F5F5F5\")# 26. Midpoint-normalizedimport matplotlib.colors as colors#https://stackoverflow.com/questions/25500541/matplotlib-bwr-colormap-always-centered-on-zeroclass MidpointNormalize(colors.Normalize): def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False): self.midpoint = midpoint colors.Normalize.__init__(self, vmin, vmax, clip) def __call__(self, value, clip=None): # I'm ignoring masked values and all kinds of edge cases to make a # simple example... x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1] return np.ma.masked_array(np.interp(value, x, y))# 27. æœˆä»½colorbarcbaxes = fig.add_axes([0.29, 0.27, 0.18, 0.02]) cbar = plt.colorbar(ss,cax=cbaxes, orientation='horizontal')n = 6st_po = []for i in range(0,n,1): st_po.append(np.array_split(pd.to_datetime(sorted(date_point)),n)[i].values[0])cb_ticks = [float(c) for c in st_po]cbar.ax.set_xticklabels(pd.to_datetime(cb_ticks).strftime('%b')) #B cbar.ax.tick_params(labelsize=8.5) # 28. nclcmapsimport nclcmapsnclcmaps.cmaps(\"precip2_17lev\")# 29. åˆ›å»ºæ–°çš„colormap cm = LinearSegmentedColormap.from_list('test',plt.cm.BuPu(np.arange(6)/6.0)[1:], N=5) # 30. æœ€å¥½çš„terrain maphttp://chris35wills.github.io/discrete_colourbar/# 31. å¯¹æ•°åæ ‡ç³»ç»Ÿæ˜¾ç¤ºå®é™…æ•°å€¼from matplotlib.ticker import StrMethodFormatter, NullFormatterax.yaxis.set_major_formatter(StrMethodFormatter('{x:.3f}'))ax.yaxis.set_minor_formatter(NullFormatter()) # 32. loop figure for subplotfig, axs = plt.subplots(2,4, figsize=(15, 6), facecolor='w', edgecolor='k')#subplot_kw={'projection': ccrs.PlateCarree()}fig.subplots_adjust(hspace = .5, wspace=.001)axs = axs.ravel()for i in range(8): axs[i].contourf(np.random.rand(10,10),5,cmap=plt.cm.Oranges) axs[i].set_title(str(250+i))# patch_box from matplotlib.collections import PatchCollectionfrom matplotlib.patches import Rectangledef PATCH_BOX(ax,pos,wid): for x in pos[::2]: width = wid/2.0 facecolor = \"#F0F0F0\" rect = Rectangle((x-width, 0.00), width*4.0,1000000,facecolor=facecolor,linewidth = 0)# clip_on=False, ax.add_patch(rect) # 33. cartopy extent æŠ¥é”™é—®é¢˜https://github.com/SciTools/cartopy/issues/837pip uninstall shapely &amp;&amp; pip install --no-binary :all: shapely# 34. shapelyçš„å‘é‡åŒ–æ€»æ˜¯æŠ¥é”™# https://github.com/Toblerity/Shapely/issues/810pip uninstall shapely &amp; pip install shapely --no-binary shapely==1.7a2 # 35. scatterå›¾çš„å¤–ç¯linewidths=1# 36. discrete color from continuous colormapfrom matplotlib import cmcs=cm.Set2(np.arange(4)/4.)# 36. how to plot circle on unequal axes ax.plot(0.3,-0.1 , 'ro', fillstyle='full', markersize=5, transform=ax.transAxes,clip_on =False)# 37. Plotting multi-polygons with pre-defined color lists.# 38. Plotting scatter plimport matplotlib.dates as mdates# Method# In method 1, the color for each data point is determined by the date value in date_point. The date values are converted to numerical values using mdates.date2num() before passing to ax.scatter(). The colorbar ticks and labels are set using mdates.AutoDateLocator() and mdates.ConciseDateFormatter(). 1fig1 = plt.figure(figsize=(5, 5))ax1 = fig1.add_subplot()date_point = df['Date']ss1 = ax1.scatter(df['WSOC'], df['OC'], c=mdates.date2num(date_point), cmap=plt.cm.coolwarm, s=100)cbaxes1 = fig1.add_axes([0.29, 0.27, 0.48, 0.02]) cb1 = plt.colorbar(ss1, cax=cbaxes1, orientation='horizontal')loc1 = mdates.AutoDateLocator(maxticks=12)cb1.ax.xaxis.set_major_locator(loc1)cb1.ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(loc1))# Method 2# Method 2 uses the datetime values directly as color values, which means the colorbar ticks will match the dates exactly. It manually divides the range of dates into 5 segments and creates tick labels with the abbreviated month names.fig2 = plt.figure(figsize=(5, 5))ax2 = fig2.add_subplot()date_point = df['Date']ss2 = ax2.scatter(df['WSOC'], df['OC'], c=date_point, cmap=plt.cm.coolwarm, s=100)cbaxes2 = fig2.add_axes([0.29, 0.27, 0.48, 0.02]) cbar2 = plt.colorbar(ss2, cax=cbaxes2, orientation='horizontal')n = 5st_po = []for i in range(0, n, 1): st_po.append(np.array_split(pd.to_datetime(sorted(date_point)), n)[i].values[0])cb_ticks = [float(c) for c in st_po]cbar2.ax.set_xticklabels(pd.to_datetime(cb_ticks).strftime('%b')) #B cbar2.ax.tick_params(labelsize=8.5)#39. plotting color palette based on user-defined color listfrom matplotlib.colors import ListedColormapimport matplotlib as mplfig = plt.subplots(figsize=(10,2))ax = plt.subplot(111)color_list = ['#cccccc', '#da6701','#00994d','#00cccc', '#0000ff','#000099','#c9ace6','#9039e6','#000000']cmap_diy = ListedColormap(color_list, color_list)col_map = cmap_diynew_val = []ticks = np.linspace(0.0,1, len(color_list)+1)for i in range(0,len(ticks)-1,1): new_val.append((ticks[i]+ticks[i+1])/2.0)cbar = mpl.colorbar.ColorbarBase(ax, cmap=col_map, orientation = 'horizontal', ticks =new_val, alpha = 0.75)cbar.ax.set_xticklabels(color_list, fontsize =12, rotation = 30) ttl = plt.title('EESI Color Palette 1',fontweight=\"bold\",fontsize =18,)ttl.set_position([.5, 1.15])plt.tight_layout()#40. Boxplotdef box_plot(ax, data, pos, color): bp = ax.boxplot(data, sym='', whis=[5, 95], widths=(len(data) + 4) / (2 * len(data)) * 0.7, positions=pos, boxprops=dict(facecolor=color, edgecolor=color), medianprops=dict(color='k'), whiskerprops=dict(color='k'), capprops=dict(color='k'), patch_artist=True) for box in bp['boxes']: box.set(facecolor=color) return bp#41. Linear fitting in short# Remove NaN values from the dataimport statsmask = ~np.isnan(x) &amp; ~np.isnan(y)x_clean = x[mask]y_clean = y[mask]# Perform linear regression and calculate the slope and interceptslope, intercept, r_value, p_value, std_err = stats.linregress(x_clean, y_clean)# Create a scatter plot with error bars using matplotlibfig, ax = plt.subplots()ax.scatter(x_clean, y_clean, s=50, edgecolors='black', alpha=0.8)# Add the linear fit lineax.plot(x_clean, slope*x_clean + intercept, color='red', linestyle='-', label='Linear Fit')# Annotate the fitted equationax.text(0.05, 0.95, 'y = {:.2f}x + {:.2f}'.format(slope, intercept), transform=ax.transAxes, fontsize=14,horizontalalignment='left',\\ verticalalignment='top', bbox=dict(facecolor='white',edgecolor='white', alpha=0.8))#42. Color-coded with datesfig = plt.figure(figsize=(12,4))c_time = mdates.date2num(auto_df_dl['Date'])ss = plt.scatter(pd.to_datetime(auto_df_dl['sample_starting_time']), auto_df_dl['130.15903'], c=c_time,cmap = plt.cm.Spectral_r)cbaxes = fig.add_axes([0.61, 0.75, 0.25,0.02]) cb = plt.colorbar(ss, cax=cbaxes, orientation='horizontal')loc = mdates.AutoDateLocator(maxticks=12)cb.ax.xaxis.set_major_locator(loc)cb.ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(loc))# 43. Add tooltips to the scatter plotslabels = [\"%.2f\" % (c) + tof_ion[i] for i, c in enumerate(tof_mz.values)]tooltip = mpld3.plugins.PointLabelTooltip(ax.collections, labels=labels)mpld3.plugins.connect(fig, tooltip)# 44. Horizontial and vertical color barcbaxes = fig.add_axes([0.775, 0.2, 0.11, 0.015]) ticks = [50,100, 250,500] cbar = plt.colorbar(ss_,ticks =ticks, cax=cbaxes,orientation='horizontal', )cbar.ax.set_xticks(ticks) # set the tick locationscbar.ax.set_xticklabels(tick_labels, fontsize=7) # set the tick labelscbar.ax.set_xlabel('FRP (MW)',fontsize = 8, labelpad = -25)cbar.ax.tick_params(color='#FFFFFF', direction='in')# 45. Dendragramdef plot_matrix_dendrogram_cn(corr_,va): D = corr_ ax1 = fig.add_axes([0.9,0.15,0.11,0.7*7/6.0]) Y = sch.linkage(D, method='ward') #method='centroid')ward set_link_color_palette(line_color) # Temporarily override the default line width: with plt.rc_context({'lines.linewidth': 1.25}): Z1 = sch.dendrogram(Y, orientation='right',above_threshold_color='grey', color_threshold=va, ) ax1.set_xticks([]) ax1.set_yticks([]) ax1.axis('off') axmatrix = fig.add_axes([0.05,0.15,0.70,0.7*7/6.0]) idx1 = Z1['leaves'] idx2 = Z1['leaves'] D = D[idx1,:] D = D[:,idx1] mask = np.tri(D.shape[1], k=-1) A = np.ma.array(D, mask=mask) cmap = plt.cm.YlOrRd# cmap = cm.get_cmap('YlGnBu') #YlOrRd cmap.set_over('#C0C1C0') im = axmatrix.matshow(A, aspect='auto', origin='lower', cmap=cmap,vmax = 0.999, )#Spectral_r# im = axmatrix.pcolormesh(A, cmap=cmap,vmax = 0.999, edgecolors ='grey',lw = 0.0)#Spectral_r # axmatrix.set_xticks(range(len(ele_name))-0.5) axmatrix.set_xticks(np.arange(0,len(ele_name),1) - 0.5) site_X = [ele_name[i] for i in idx1] axmatrix.set_xticklabels(site_X, minor=False, va=\"baseline\") axmatrix.xaxis.set_label_position('bottom') axmatrix.xaxis.tick_bottom() pylab.xticks(fontsize=8,) axmatrix.tick_params(axis='x', pad=60,length=0,rotation = 45) axmatrix.tick_params(axis='y', pad=2,length=0) ind_array = np.arange(0,len(ele_name),1) x, y = np.meshgrid(ind_array, ind_array) for i in range(0,len(ele_name),1): for j in range(0,len(ele_name),1): if math.isnan(A[j,i]) == 0: axmatrix.text(i, j, str(round(A[j,i], 2)), color = 'black',va='center', ha='center', fontsize = 12) axcolor = fig.add_axes([0.05,0.7,0.2,0.015]) cbar = pylab.colorbar(im, cax=axcolor,orientation='horizontal' ) cbar.set_label(r'$\\mathregular{{Pearson^{\\prime}}s\\ r}$', labelpad = -35, fontsize = 10) loc_ = np.array([0.2,0.5,0.8]) cbar.set_ticks(loc_) ttt = [0.2,0.5,0.8] cbar.set_ticklabels(ttt,) cbar.ax.tick_params(color=\"w\", direction='in',labelsize=10)# length axmatrix.set_yticks(range(len(ele_name))) site_Y = [ ele_name[i] for i in idx1] axmatrix.set_yticklabels(site_Y, minor=False, fontsize =8) axmatrix.yaxis.set_label_position('right') axmatrix.yaxis.tick_right() pylab.yticks( fontsize=8) axmatrix.spines['left'].set_visible(False) axmatrix .spines['top'].set_visible(False) def fitting_line(x,y): mask = ~np.isnan(x) &amp; ~np.isnan(y) x_clean = x[mask] y_clean = y[mask] slope, intercept, r_value, p_value, std_err = stats.linregress(x_clean, y_clean) annotation = f\"y = {slope:.2f}x + {intercept:.2f}\\n\\n\" + f\"r = {r_value:.2f}\" ax.annotate(annotation, xy=(0.05, 0.7), xycoords='axes fraction', fontsize = 10) # the version without slope # x_clean = x_clean[:,np.newaxis] # a, _, _, _ = np.linalg.lstsq(x_clean, y_clean) # print (a,) # annotation = f\"y = {a[0]:.2f}x\" # ax.annotate(annotation, xy=(0.15, 0.7), xycoords='axes fraction', fontsize = 10)## 46. Venn plot for 2 elementslist_a,list_b = owb_poa,owb_soaions_intersection = list_a[list_a['Ion'].isin(list_b['Ion'])]['Ion'].unique()ions_only_a = list_a[~list_a['Ion'].isin(list_b['Ion'])]['Ion'].unique()ions_only_b = list_b[~list_b['Ion'].isin(list_a['Ion'])]['Ion'].unique()from matplotlib_venn import venn2# Determine the countsnum_ions_in_a_not_b = len(ions_only_a)num_ions_in_b_not_a = len(ions_only_b)num_ions_intersection = len(ions_intersection)# Plot Venn Diagramplt.figure(figsize=(6, 5))venn2(subsets=(num_ions_in_a_not_b,num_ions_in_b_not_a, num_ions_intersection), set_labels=('Open wood burning POA','Open wood burning SOA'), set_colors=('#5477b1', '#f5a34f'))# plt.title(\"Orbitrap peak list\")plt.savefig('./Figures/20240423/Orbitrap_fitted_ions_OWB_POA+SOA.png', dpi = 400)plt.tight_layout()plt.show()## 47. Venn plot for 3 elementsdef only_in_target(tar,ref1,ref2): list1= (tar[tar['Ion'].isin(ref1['Ion'])]['Ion'].unique()) list2= (tar[tar['Ion'].isin(ref2['Ion'])]['Ion'].unique()) list_tot = np.unique(np.append(list1,list2)) return tar[~tar['Ion'].isin(list_tot)]['Ion'].unique()# Calculate intersectionsions_intersection_ab = list_a[list_a['Ion'].isin(list_b['Ion'])]['Ion'].unique()ions_intersection_ac = list_a[list_a['Ion'].isin(list_c['Ion'])]['Ion'].unique()ions_intersection_bc = list_b[list_b['Ion'].isin(list_c['Ion'])]['Ion'].unique()# Intersection of all three listsions_intersection_abc = list_a[list_a['Ion'].isin(ions_intersection_bc)]['Ion'].unique()# Calculate unique ionsions_only_a = only_in_target(list_a,list_b,list_c)ions_only_b = only_in_target(list_b,list_a,list_c)ions_only_c = only_in_target(list_c,list_a,list_b)# Unique in a, b and c but not in the intersection of all threeions_unique_a = np.setdiff1d(ions_only_a, ions_intersection_abc)ions_unique_b = np.setdiff1d(ions_only_b, ions_intersection_abc)ions_unique_c = np.setdiff1d(ions_only_c, ions_intersection_abc)# Calculate lengthsnum_ions_in_a_not_bc = len(ions_unique_a)num_ions_in_b_not_ac = len(ions_unique_b)num_ions_in_c_not_ab = len(ions_unique_c)num_ions_intersection_ab = len(ions_intersection_ab) - len(ions_intersection_abc)num_ions_intersection_ac = len(ions_intersection_ac) - len(ions_intersection_abc)num_ions_intersection_bc = len(ions_intersection_bc) - len(ions_intersection_abc)num_ions_intersection_abc = len(ions_intersection_abc)from matplotlib_venn import venn3plt.figure(figsize=(10, 6))v = venn3(subsets=(num_ions_in_a_not_bc, num_ions_in_b_not_ac, num_ions_intersection_ab, num_ions_in_c_not_ab, num_ions_intersection_ac, num_ions_intersection_bc, num_ions_intersection_abc), set_colors=('#a6cee3', '#d83f3f', '#405f3a'), set_labels=('All samples Merged (2371 ions)', ' Coal burning SOA (616 ions)', ' Open burning SOA (858 ions)'))# change the edgecolor to blackfor area in v.patches: if area: area.set_edgecolor('k')plt.tight_layout()plt.savefig(\"./Figures/20240423/Combined_coal_POA_owb_SOA_All.png\", dpi = 400)plt.show() 4. Jupyter and Python setting 123456789101112131415161718192021222324252627282930313233343536#1. Full width displaying all the timefrom IPython.core.display import display, HTMLdisplay(HTML(\"&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;\"))#2. Import script.py filesimport os,sysscriptpath = \"/Users/HYF/Downloads/stackoverflow_tag_cloud-master/\"##Add the directory containing your module to the Python path (wants absolute paths)sys.path.append(os.path.abspath(scriptpath))## Do the importimport stackoverflow_users_taginfofrom stackoverflow_users_taginfo import taginfoinfo = taginfo(link = 4918632, num_tags = 400)WordCloud().generate_from_frequencies(info).to_image().save('TagCloud.pdf')#3. Check the installing pathimport timetime.file#4. Get the pathname of the filescript_dir = os.path.dirname(os.path.realpath(filename)) + os.sep #5. Change the size of the uploaded figure&lt;img src=\"https://i.stack.imgur.com/kK1LC.png\" width=\"100\"&gt;#6. Inquire the information of computer, Python and its package using watermark packagepip install watermark%load_ext watermark ## Compiler, system, and CPU%watermark ## numpy version%watermark -p numpy# 7. å–æ¶ˆWarningsæ˜¾ç¤ºimport warningswarnings.filterwarnings('ignore') &nbsp;5. Some tex characters 1\\leq &lt; 6. Spatial analysis 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# 1. intersection of two linesimport fionafrom shapely.geometry import shapefrom shapely.geometry import LineStringline1 = fiona.open(fname)# fname is the path to a specific polyline-format shapefile.line1 = shape(line1.next()['geometry']) start_point, end_point = (111,22),(33,33)line2 = LineString([start_point, end_point])print line1.intersection(line2)print line1.intersection(line2).is_empty# 2. Read .tifimport warningsimport rasteriowarnings.filterwarnings('ignore')import skimage.transform as st pathToRaster =r'/Users/HYF/Downloads/2014/per2014/ä¸­å›½å¹´å‡é™æ°´.tif'xs = np.array([100.5, 12.0])ys = np.array([2.5, 42.0])# src.boundswith rasterio.open(pathToRaster) as src: arr = src.read() arr[arr&lt;0] = np.nan arr = arr[0,:,:] rows, cols = rasterio.transform.rowcol(src.transform, xs, ys) # 3. mask point seriesmask_= []for i in range(0,len(df_fire_cn),1): xy_point = gpd.geoseries.Point(df_fire_cn.lon.iloc[i],df_fire_cn.lat.iloc[i]) res = geom.contains(xy_point) mask_.append(res)df_fire_cn = df_fire_cn[mask_] # 4. Classify scatter point based on polylinesimport warningswarnings.filterwarnings('ignore')from shapely.geometry import shapefrom shapely.geometry import LineString# loading the boundary layerimport fionafname = '/Users/HYF/Dropbox/data/geo/ä¸­å›½åœ°å›¾/ä¾›æš–åˆ†ç•Œ/N-S_boundary.shp'line1 = fiona.open(fname)line1 = shape(line1.next()['geometry']) # set a end point which is the southernmost for all stations. end_point = (dy[dy['lat']==dy['lat'].min()]['lon'].values[0],dy[dy['lat']==dy['lat'].min()]['lat'].values[0])# loop all monitoring stations for classificationdy['NS']= np.nanfor i in range(0,len(dy),1): start_point = (dy['lon'].iloc[i],dy['lat'].iloc[i]) line2 = LineString([start_point, end_point]) if line1.intersection(line2).is_empty: dy[\"NS\"].iloc[i]='S' else: dy[\"NS\"].iloc[i]='N' # 5. transform shp file to geojsonimport fionaimport json# Open the shapefile using fionawith fiona.open(\"shapefile.shp\") as src: # Create a new GeoJSON file with open(\"shapefile.geojson\", \"w\") as output: # Write the GeoJSON representation of the shapefile to the file output.write(json.dumps(list(src)))# referencing https://gist.github.com/pelson/9785576import fionaimport shapely.vectorizedfrom shapely.geometry import shapefname = r'xxx/china.shp'cn_area = fiona.open(fname)pol = cn_area.next()geom = shape(pol['geometry'])## four corner x0,x1 = geom.bounds[0],geom.bounds[2]y0,y1 = geom.bounds[1],geom.bounds[3]## creating the mask arrayx = np.linspace(x0,x1,30)y = np.linspace(y0,y1,20)xx,yy = np.meshgrid(x,y)import timestart = time.time()mask_ = shapely.vectorized.contains(geom, xx,yy)print \"Process time: \" + str(time.time() - start)## å‡ºç°é”™è¯¯: 'Polygon' object is not iterable shapeFeature([c.geometry]) # when nothing showed up in Plotly import plotly.io as piopio.renderers.default = \"notebook\" # or \"jupyterlab\" 7. Other functions 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# 1.Crop PDF file# https://pypi.org/project/pdfcropper/#filespdf-crop-margins -v -s -u /Users/HYF/Dropbox/thesis//5_å…¨å›½æ—¶ç©ºåˆ†æ/output/Sec3/As_ç©ºé—´å·®å¼‚å›¾.pdf# 2. When the self-installed files were unable to be importedpython setup.py buildpython setup.py install## Then, install the package and egg into&gt;/Users/HYF/anaconda3/lib/python3.7/site-packages#3. Enabling code folding in Jupyter labsetting/notebook{ \"codeCellConfig\": { \"codeFolding\": true }}#4. How to search for content in any notebook on local PC by keywordsimport globpattern = './../../../../**/*.ipynb'query = 'EESI_inforamtion_example'for filepath in glob.iglob(pattern, recursive=True): with open(filepath) as file: s = file.read() if (s.find(query) &gt; -1): print(filepath) # 5. pandas function for reading markdown table# Paste in your table and assign it to a variablemd_table_string = \"\"\"| Risk Factor | Estimated Annual Number of Deaths ||---|---|| Air pollution (outdoor &amp; indoor) | 1,670,000 || High blood pressure | 1,470,000 || High blood sugar | 1,120,000 || Smoking | 1,010,000 || Outdoor particulate matter pollution | 979,682 || Indoor air pollution | 630,093 || Obesity | 606,890 || Unsafe water source | 508,290 || Low birth weight | 579,108 || Alcohol use | 343,695 || Unsafe sanitation | 258,257 || Diet low in whole grains | 256,232 || Diet low in fruits | 240,098 || Secondhand smoke | 174,797 || Diet high in sodium | 173,939 || No access to handwashing facility | 164,172 || Diet low in nuts and seeds | 134,007 || Diet low in vegetables | 118,682 || Low bone mineral density | 118,283 || Low physical activity | 96,012 || Drug use | 72,903 || Child stunting | 17,973 || Iron deficiency | 10,098 || Non-exclusive breastfeeding | 1,501 |\"\"\"from io import StringIOdeath_factor = pd.read_csv( StringIO(md_table_string.replace(' ', '')), # Get rid of whitespaces sep='|', index_col=1).dropna( axis=1, how='all').iloc[1:]## mount the external drivesudo umount /mnt/nsudo mount -t drvfs -o metadata,uid=1000,gid=1000 N: /mnt/n## unzip all files within the folder#!/bin/bash \\# Navigate to the directory containing the zip files cd /path/to/your/directory # Loop through all files with .z* extensions and unzip them for file in 248650.z*; do echo \"Unzipping $file...\" unzip \"$file\" -d ./unzip/ \\# Unzip to a subdirectory called 'unzipped_files' done echo \"All files have been unzipped.\"","link":"/2018/07/22/9.%5BCode%5DUseful%20python%20scripts%20for%20daily%20use/"},{"title":"WRF post processing 2: Xarray tricks","text":"Xarray provides pandas-level convenience for working with ultidimensional data. Xarray has two fundamental data structure: a DataArray, which holds a single multi-dimensional variable and its coordinates;a Dataset, which holds multiple variables that potentially share the same coordinates. Moreover, a DataArray has four attributes: values: a numpy.ndarray holding the arrayâ€™s values (çŸ©é˜µæ•°å€¼ï¼Œä¾‹å¦‚åœ°è¡¨æ¸©åº¦å…·ä½“æ•°å€¼) dims: dimension names for each axis (e.g., (â€˜xâ€™, â€˜yâ€™, â€˜zâ€™)) (ç»´åº¦åç§°ï¼Œå¦‚ç»åº¦ã€çº¬åº¦ã€å‚ç›´åˆ†å±‚ã€æ—¶é—´ç­‰) coords: a dict-like container of arrays (coordinates) that label each * point (e.g., 1-dimensional arrays of numbers, datetime objects or strings) (å„ç»´åº¦åæ ‡ä½“ç³»ï¼Œå¦‚æ—¶é—´åºåˆ—) attrs: an OrderedDict to hold arbitrary metadata (attributes) (å¯¹å„å±æ€§çš„æè¿°) 1. Subset and Diurnal profiles As an illustration, we firstly extract the original file generated from WRF-Chem by specific time range and spatial coverage. Then, the daily averages for those selected grids are calculated and saved as another netCDF file. 123456789101112131415161718192021222324import xarray as xrfilename = \"./../wrfout_allBVOCs_d01_2018-06-01_00:00:00\"df = xr.open_dataset(filename)# 1. add dimsdf = df.assign_coords(south_north=('south_north',df.XLAT[0,:,0].values))df = df.assign_coords(west_east=('west_east',df.XLONG[0,0,:].values))df = df.assign_coords(Time=('Time',df.XTIME.values))# 2. selcting the grids with lon in the range of 100-120ï¼Œlat in the range of 40-50 in the bottom vertical level. The simulation period was chosen in the range of 2018-06-01 to 2018-06-05da = df.sel(bottom_top = slice(0,1), south_north = slice(40,50),west_east = slice(100,120), Time = slice('2018-06-01T00:00:00.000000000',\\ '2018-06-08T00:00:00.000000000'))# 3. selecting one/multi variable(s)var = 'PM2_5_DRY' var_list = ['asoa1j','asoa1i','asoa2j','asoa2i']# 3.1 Summarizing multiple variablesda['SUM'] = 0for c in var_list: da['SUM'] = da['SUM']+da[c] # 4. Calculating the diurnal averagesdm = da[\"SUM\"].groupby('Time.hour').mean()dm.to_netcdf(\"diurnal.nc\") 2. Rolling windowIn the second example, we use resample trick to group the original data, and use rolling trick to calculate the daily maximum of 8-hour ozone concentrations. 123da['8h_o3_averege'] = da['o3'].rolling(Time=8, center=False).mean() # center = False-&gt;time=end point of the rolling window.dt = da['8h_o3_averege'].resample(Time ='1D').max()dt.to_netcdf(\"./8h_o3_max.nc\")","link":"/2020/11/01/WRF-technique-xarray-tricks/"},{"title":"[WRF-Chemå­¦ä¹ ç¬”è®°â‘ ]æ’æ”¾é¢„å¤„ç†æ¨¡å—çš„å®‰è£…ä¸ä½¿ç”¨","text":"æ°”è±¡åœºå’ŒåŒ–å­¦ç‰©è´¨æ’æ”¾æ˜¯å¤§æ°”åŒ–å­¦æ¨¡å¼çš„ä¸¤é¡¹æœ€é‡è¦çš„å‰é©±èµ„æ–™ã€‚åœ¨å®éªŒä¹‹ä½™ï¼Œæˆ‘å°†ç€åŠ›å­¦ä¹ WRF-Chemçš„æœ‰å…³å†…å®¹ï¼Œå¹¶è®°å½•ã€å‘å¸ƒã€‚ æŒ‰ç…§WRF-Chem Tutorialä¸­çš„è¯´æ˜ï¼Œåœ¨Learning to Run WRF-Chemä¹‹å‰ï¼Œæˆ‘å…ˆå­¦ä¹ How to Generate Emission for WRF-Chem æœ¬æ–‡è®°å½•Exercise 1çš„å®ç°è¿‡ç¨‹ï¼Œå¯ä¾›æœ‰å…´è¶£çš„åŒå­¦å‚è€ƒã€‚ è¿è¡ŒWRFå’ŒWRF-Chemçš„ä¸€ä¸ªé‡è¦åŒºåˆ«: æ’æ”¾æºæ¸…å•çš„è¾“å…¥ã€‚Exercise 1__å°†ä¼šä»‹ç»å»ºç«‹WRF-Chemå¯è¯»å–çš„æºæ¸…å•æ–‡ä»¶çš„æœ€åŸºæœ¬æ–¹æ³•(è½¬ç½®ç°æœ‰å…¨çƒæ¸…å•)ï¼Œå…¶æ ¸å¿ƒå·¥å…·ä¸º__PREP_CHEM_SOURCESã€‚ å®‰è£…è¿‡ç¨‹ HDF5å®‰è£…è¯¥ç¨‹åºçš„é¡ºåˆ©è¿è¡Œéœ€è¦ä¾èµ–HDF5, NetCDF, zlibç­‰æ ‡å‡†åº“ã€‚ä¹‹å‰åœ¨å®‰è£…WRF-Chemæ—¶å·²ç»å®‰è£…äº†NetCDFå’Œzlibï¼Œè¿™é‡Œåªåˆ—å‡ºHDF5å’Œszipçš„å®‰è£…æ­¥éª¤ã€‚ 12345678910111213#1. szipå®‰è£…wget http://www.hdfgroup.org/ftp/lib-external/szip/2.1/src/szip-2.1.tar.gztar -xvzf szip-2.1.tar.gzcd szip-2.1./configure --prefix=/The_path/you/want/to/install/make &amp; make check &amp; make install#2. HDF5å®‰è£…wget http://www.hdfgroup.org/ftp/HDF5/current/bin/linux-centos7-x86_64-gcc485/hdf5-1.8.17-linux-centos7-x86_64-gcc485-shared.tar.gz#è§£å‹å¹¶è¿›å…¥æ–‡ä»¶å¤¹./configure --prefix=/disk2/hyf/lib/hdf5 --enable-fortran \\ --with-szlib=/szip/install/path\\ --with-zlib=/zlib/install/path ## æ­¤å¤„æ³¨æ„ä¸åæ–‡prep_chem_sourceçš„ç¼–è¯‘å™¨åº”å¯¹åº”ä¸Šï¼Œå¦åˆ™libä¸­çš„æ–‡ä»¶æ— æ³•ç¼–è¯‘æˆåŠŸmake &amp; make check &amp; make install æ’æ”¾é¢„å¤„ç†ç¨‹åºå®‰è£…ä¸‹è½½å¹¶è§£å‹åï¼Œè¿›å…¥/bin/buildæ–‡ä»¶å¤¹ï¼Œä¿®æ”¹ä¸ç¼–è¯‘å™¨å¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼Œå¦‚æˆ‘å°†é‡‡ç”¨gfortranç¼–è¯‘å™¨ç¼–è¯‘ï¼Œåˆ™vim include.mk.gfortran, ä¿®æ”¹å…¶ä¸­çš„NETCDFä¸HDF5åº“å¯¹åº”ä½ç½®ï¼ŒæŒ‰ä»¥ä¸‹å‘½ä»¤è¿›è¡Œç¼–è¯‘: 1`make OPT=gfortran CHEM=RADM_WRF_FIM` å…¶ä¸­OPT=å¯¹åº”ç¼–è¯‘å™¨ç±»åˆ«(i.e, intel, pgi,gfortran, â€¦)ï¼ŒCHEM=å¯¹åº”ç‰¹å®šåŒ–å­¦æœºç†ï¼Œæ­¤å¤„é€‰æ‹©__RADM__æœºç†ã€‚è¿™å°†å†³å®šWRF-Chemè¯»å–æºæ¸…å•æ–‡ä»¶çš„åŒ–å­¦ç‰©è´¨åˆ†ç±»æ ¼å¼ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒæŒ‰ä¸Šè¿°æ“ä½œï¼Œå°†åœ¨ç¼–è¯‘__edgar_emission.f90__è¿‡ç¨‹å‡ºé”™ï¼ŒWRFè®ºå›ä¸­ä¹Ÿæœ‰ç”¨æˆ·æäº¤äº†åŒæ ·çš„æŠ¥é”™ï¼Œå¹¸æœ‰å¤§ç¥å›å¤å¹¶è§£å†³ï¼Œè§£å†³æ–¹æ³•æ˜¯è°ƒæ•´æŸäº›è¡Œçš„å­—ç¬¦ä¸²çš„é•¿åº¦ï¼Œå¯å‚è€ƒè¯¥ç½‘é¡µè¿›è¡Œä¿®æ”¹ã€‚ ä¿®æ”¹å®Œæˆåï¼Œå†æ¬¡ç¼–è¯‘ï¼Œè‹¥å‡ºç°Finished building === ../prep_chem_sources_RADM_WRF_FIM_.exeï¼Œè¯´æ˜ç¼–è¯‘æˆåŠŸ!ãƒ½(âœ¿ï¾Ÿâ–½ï¾Ÿ)ãƒ PS: å®‰è£…å®Œæˆåï¼Œç¼–è¾‘~/.bashrc 12345# æ·»åŠ ä»¥ä¸‹å‡ è¡Œexport HDF5=\"$DIR/hdf5\" # $DIRä¸ºæ‰€æœ‰åº“å­˜æ”¾ä½ç½®export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HDF5/libexport NETCDF=\"$DIR/netcdf\"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NETCDF/lib æ¡ˆä¾‹ æ’æ”¾é¢„å¤„ç†ç¨‹åºå®‰è£…æŒ‰ç…§Exercise 1ä¸­çš„inputæ–‡ä»¶ï¼Œæ›¿æ¢prep_chem_src\\binæ–‡ä»¶å¤¹ä¸‹åŸæœ‰çš„inputæ–‡ä»¶ï¼Œå¹¶æŒ‰ç…§è‡ªå·±çš„global_emissionæ–‡ä»¶å¤¹ä¿®æ”¹è¯»å–åŸå§‹æ’æ”¾æ•°æ®çš„è·¯å¾„ã€‚ è¿è¡Œ$./prep_chem_sources_RADM_WRF_FIM_.exe è¿è¡ŒæˆåŠŸåï¼Œä¼šå‡ºç°äº”ä¸ªè¾“å‡ºæ–‡ä»¶ï¼Œå…¶ä¸­æœ‰ä¸‰ä¸ªåç¼€åä¸º.binçš„æ–‡ä»¶ï¼Œåˆ†åˆ«ä¸º: 123WRF-TUTORIAL-T-2010-07-14-000000-g1-ab.bin ## äººä¸ºæºæ’æ”¾æ¸…å•WRF-TUTORIAL-T-2010-07-14-000000-g1-bb.bin ## ç”Ÿç‰©è´¨ç‡ƒçƒ§æ¸…å•WRF-TUTORIAL-T-2010-07-14-000000-g1-gocartBG.bin ## GOCARTçš„å…¨çƒèƒŒæ™¯åŒ–å­¦åœºèµ„æ–™ å°†prep_chem_src\\binæ–‡ä»¶å¤¹ä¸­ä¸‰ä¸ª.binåç¼€çš„è¾“å‡ºæ–‡ä»¶ç§»åŠ¨åˆ°WRF-Chemæ–‡ä»¶å¤¹ä¸­, æ­¤å¤„æˆ‘åœ¨WRFV3\\test\\æ–‡ä»¶å¤¹å†…æ–°å»ºem_ex1æ–‡ä»¶å¤¹ï¼Œå†…å®¹å¤åˆ¶åŒç›®å½•ä¸‹çš„em_realã€‚ è¿è¡Œ./convert_emiss.exe æŒ‰ç…§è®ºå›ä¸Šå¤§å®¶çš„è§‚ç‚¹ï¼Œç›®å‰WRF-Chem 3.4ç‰ˆæœ¬ä»¥ä¸Šä¼šæŠ¥é”™ï¼Œå‚è€ƒ1å‚è€ƒ2ã€‚ è‹¥è¿è¡ŒæˆåŠŸï¼Œåˆ™ä¼šç”Ÿæˆä¸‰ä¸ªæ’æ”¾æºæ–‡ä»¶ï¼Œå…¶ä¸­ (1) __wrfchemi_d01__æ–‡ä»¶å¯¹åº”è®¾è®¡æ¨¡æ‹ŸåŒºåŸŸå†…å„ç±»æ±¡æŸ“ç‰©çš„æ’æ”¾æ¸…å•ç»“æœ,åŒ…æ‹¬CO, NH3, PM10, PM2.5åŠå„ç±»æœ‰æœºç‰©(æŒ‰ç…§RACMæœºç†åˆ†ç±»)ã€‚(2) __wrffirechemi_d01__æ–‡ä»¶å¯¹åº”ç”Ÿç‰©è´¨ç‡ƒçƒ§æ’æ”¾æƒ…å†µ.(3)__wrfchemi_gocart_bg_d01__æ–‡ä»¶å¯¹åº”äºŒç”²åŸºç¡«(DMS), NO3, OH, H2O2çš„èƒŒæ™¯æµ“åº¦åˆ†å¸ƒã€‚DMSå•ä½ä¸ºnM/L,å…¶ä»–ç‰©ç§å•ä½ä¸ºä½“ç§¯æ··åˆæ¯”(volume mixing ratio)ã€‚ ä¾æ®ä¸Šè¿°çš„æ­¥éª¤ åŸå§‹å…¨çƒæ¸…å• â†’ æ¨¡æ‹ŸåŒºåŸŸæ’æ”¾äºŒè¿›åˆ¶æ–‡ä»¶ â†’ WRF-Chemå¯è¯»å–çš„ncæ ¼å¼æ’æ”¾æºæ–‡ä»¶å®Œæˆäº†æ’æ”¾æºèµ„æ–™çš„åˆ¶ä½œ åˆå§‹åœºä¸è¾¹ç•Œåœºæ–‡ä»¶çš„ç”Ÿæˆæ­¤å¤„é“¾æ¥åŒæ¨¡æ‹ŸåŒºåŸŸã€æ—¶é—´çš„met_em.xxxæ–‡ä»¶(ç”±WPSæ¨¡å‹å®Œæˆ)é“¾æ¥ï¼Œè¿è¡Œ./real.exe, ç”Ÿæˆåˆå§‹åœºå’Œè¾¹ç•Œåœºæ–‡ä»¶,å†…éƒ¨åŒ…æ‹¬äº†åŒ–å­¦ç‰©è´¨å’Œæ°”è±¡èµ„æ–™ä¿¡æ¯ã€‚ è¾“å‡ºæ–‡ä»¶ç”Ÿæˆè¿è¡Œ./wrf.exeï¼Œç”Ÿæˆwrfoutæ–‡ä»¶ã€‚ä¸‹å›¾ä¸ºPM2.5çš„åˆ†å¸ƒæƒ…å½¢ï¼Œå¯ä»¥çœ‹å‡ºå¦‚é˜¿å°”å‘æ–¯å±±ã€å®‰çº³é™€åˆ©äºšé«˜åŸç­‰é«˜æµ·æ‹”åœ°åŒºæµ“åº¦ç›¸å¯¹ä½ã€‚ é™„å›¾: å‚è€ƒæ›¼å½»æ–¯ç‰¹å¤§å­¦æ•™ç¨‹ï¼Œç»˜åˆ¶WRF-Chemè¿è¡Œæµç¨‹å¦‚ä¸‹: æ­¤å¤„éœ€è¦è¿è¡Œ./real.exeä¸¤æ¬¡ï¼Œç›®å‰æˆ‘è¿˜æ²¡æœ‰å¼„æ˜ç™½ã€‚ å‚è€ƒèµ„æ–™ convert_emiss.exe","link":"/2016/09/04/4.%5BModel%5D%5BWRF-Chem%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%91%A0%5D%E6%8E%92%E6%94%BE%E9%A2%84%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"title":"ğŸµéé¶å‘è´¨è°±æ–¹æ³•æ¢ç´¢å„ç±»èŒ¶é¥®çš„åˆ†å­ç»„æˆ","text":"æˆ‘ä¸€ç›´æƒ³å¯¹èŒ¶å¶ä¸­å¤æ‚çš„åŒ–å­¦æˆåˆ†å……æ»¡å…´è¶£ã€‚å®éªŒå®¤æœ‰åŒäº‹éœ€è¦æµ‹é‡ç”Ÿç‰©æ°”æº¶èƒ¶æ ·å“ï¼ˆå¦‚å­¢å­ã€çœŸèŒç­‰ï¼‰ï¼Œå€ŸåŠ©ååŠ©å…¶è°ƒè¯•ç»´æŠ¤è´¨è°±ä»ªå™¨çš„æœºä¼šï¼Œå¼€å±•äº†ä¸€é¡¹è¯¾å¤–æ¢ç´¢ï¼šä½¿ç”¨é«˜åˆ†è¾¨ç‡è´¨è°±æ–¹æ³•ï¼Œæ¢ç´¢ä¸åŒç±»å‹èŒ¶å¶åœ¨åˆ†å­å±‚é¢æ˜¯å¦å…·æœ‰å¯è¾¨è¯†çš„ç‰¹å¾ã€‚ æœ¬æ¬¡å°è¯•ä½¿ç”¨éé¶å‘ç›´æ¥æ³¨å…¥åˆ†æï¼Œç»“åˆOrbitrapä¸TOFä¸¤å°è´¨è°±ï¼Œè¯„ä¼°æˆ‘æ„å»ºçš„åˆ†ææµç¨‹åœ¨å®é™…å¤æ‚æ ·å“ä¸­çš„è¡¨ç°ã€‚ 1. å®éªŒè®¾è®¡æˆ‘æ€»å…±è®¾è®¡äº†ä¸¤ç»„æµ‹é‡ä»»åŠ¡ï¼š ä»»åŠ¡ Aï¼šä¸åŒå›½å®¶ä¸äº§åŒºçš„èŒ¶å¶å¯¹æ¯”åŒ…æ‹¬ä¸­å›½ç»¿èŒ¶ã€æ—¥æœ¬ç»¿èŒ¶ã€å°åº¦çº¢èŒ¶ã€æ–¯é‡Œå…°å¡çº¢èŒ¶ç­‰ï¼Œå¢æ—ºè¾¾çº¢èŒ¶ç­‰ï¼Œæ¢ç©¶ä¸åŒç±»å‹/äº§åœ°èŒ¶å¶èƒ½å¦é€šè¿‡è´¨è°±åˆ†æå‘ˆç°å·®å¼‚ã€‚ ä»»åŠ¡ Bï¼šè¥¿æ¹–é¾™äº•çš„â€œæ¡£æ¬¡åˆ†è¾¨â€æŒ‘æˆ˜å¯¹æ¯”æ­å·æ ¸å¿ƒäº§åŒºä¸éæ ¸å¿ƒäº§åŒºé¾™äº•ï¼ˆä»·æ ¼ä¸åŒï¼‰ä»¥åŠå…¶ä»–åœ°åŒºçš„â€æŒ‚ç‰Œâ€é¾™äº•çš„åˆ†å­ç»„æˆå·®å¼‚ã€‚è¯¥éƒ¨åˆ†å®éªŒå·²å®Œæˆï¼Œå°šæœªåˆ†æå…¶æ•°æ®ï¼Œæœªæ¥ä¼šä¸“æ–‡ä»‹ç»ã€‚ èŒ¶å¶æ ·å“ç›¸æ¯”å¤§æ°”æ±¡æŸ“ç‰©è‡ªç„¶é‡‡é›†æ›´æœ‰æ„æ€ã€‚æˆ‘ä¸ªäººä»è€å®¶å¸¦æ¥å‡ ç§å®‰å¾½äº§çš„ç»¿èŒ¶ï¼Œä¸ºäº†æ‰©å¤§æ ·å“é‡ï¼Œå‘åŒäº‹ä»¬å€ŸèŒ¶ï¼Œå¹¶å‰å¾€Lenzburgçš„èŒ¶å¶å•†åº—Itâ€™s tea timeè´­ä¹°çº¢èŒ¶ã€‚æœ€ç»ˆæˆ‘æ”¶é›†åˆ°çš„èŒ¶å¶æ ·å“åŒ…æ‹¬ï¼š ä¸­å›½ç»¿èŒ¶ï¼šå®‰å¾½æ± å·ç»¿èŒ¶ã€å››å·é›…å®‰ç»¿èŒ¶ æ—¥æœ¬ç»¿èŒ¶ï¼šä¸¤æ¬¾äº§åœ°ä¸åŒçš„ç…èŒ¶ æ–¯é‡Œå…°å¡ç»¿èŒ¶çº¢èŒ¶ç³»åˆ—ï¼š ä¸­å›½ç¥é—¨çº¢èŒ¶ å°åº¦å¤§å‰å²­çº¢èŒ¶ï¼ˆMargaretâ€™s Hopeï¼‰ è¶Šå—çº¢èŒ¶ï¼ˆGolden Tippyï¼‰ æ–¯é‡Œå…°å¡çº¢èŒ¶ï¼ˆNuwara Eliyaï¼‰ å¢æ—ºè¾¾çº¢èŒ¶ï¼ˆRukeriï¼‰ åœ¨å›½å†…æœ‹å‹çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘æ”¶åˆ°äº†å¤šä¸ªä»·ä½ï¼ˆä» 200 å…ƒè‡³ 1000 å…ƒä¸ç­‰ï¼‰çš„è¥¿æ¹–é¾™äº•æ ·å“ï¼Œå¹¶ç”±ä¸€ä½å›å›½æ¢äº²çš„åŒäº‹å¸®å¿™æºå¸¦è‡³ç‘å£«ï¼Œå…·è§ä¸‹å›¾ï¼š é€šè¿‡é¢„å®éªŒï¼Œå°½å¯èƒ½æ¨¡æ‹ŸçœŸå®é¥®èŒ¶çŠ¶æ€â€”â€”å³å°†èŒ¶å¶æµ¸æ³¡äºçƒ­æ°´ä¸­æå–æˆåˆ†ï¼Œå¹¶ç›´æ¥è¿›è¡Œè´¨è°±æµ‹é‡ã€‚ 2. æ•°æ®å¤„ç†RDKit Python library can reproduce the structurs based on the smiles code. 3. ç»“æœåˆ†æ m/z Formula ä¸­æ–‡åç§° English Name Note 140.06853 C5H11NO2-Naâº ç¼¬æ°¨é…¸ Valine æ°¨åŸºé…¸ 158.08109 C7H11NO3-Hâº ç”˜æ°¨é…¸ Tiglylglycine æ°¨åŸºé…¸è¡ç”Ÿç‰© 175.10800 C7H14N2O3-Hâº èŒ¶æ°¨é…¸ Theanine èŒ¶å¶ä¸­é‡è¦æ°¨åŸºé…¸æˆåˆ†ä¹‹ä¸€ 197.09180 C9H12N2O3-Hâº 3-æ°¨åŸº-L-é…ªæ°¨é…¸ 3-Amino-L-tyrosine é…ªæ°¨é…¸è¡ç”Ÿç‰© 215.05261 C7H12O6-Naâº å¥å®é…¸ Quinic acid æ¤ç‰©ä»£è°¢äº§ç‰© 217.06955 C8H10N4O2-Naâº å’–å•¡å›  Caffeine èŒ¶å¶ä¸­çš„å…¸å‹ç”Ÿç‰©ç¢±æˆåˆ† 219.07156 C7H13N2O3-Naâº L-èŒ¶æ°¨é…¸ Theanine åˆç§°è°·æ°¨é…°ä¹™èƒºï¼Œæ˜¯èŒ¶é²œå‘³çš„ä¸»è¦æ¥æºï¼Œå æ¸¸ç¦»æ°¨åŸºé…¸50%ä»¥ä¸Š 313.06851 C15H14O6-Naâº å„¿èŒ¶ç´  Catechin å¤šé…šç±»æŠ—æ°§åŒ–æˆåˆ† 335.09497 C11H20O10-Naâº æ¥éª¨æœ¨äºŒç³– Sambubiose å¤šç³–ç±» 365.10501 C12H22O11-Naâº è”—ç³– Sucrose å¸¸è§åŒç³– 203.05258 C6H12O6-Naâº è‘¡è„ç³– Glucose å•ç³– 329.09976 C16H18O6-Naâº å‡éº»ç´  Cimifugin ä¸­è‰è¯æ´»æ€§æˆåˆ†ï¼Œå…·æœ‰æŠ—ç‚æ´»æ€§","link":"/2024/05/29/39.%5BMS%5DUntargeted_Analysis_on_Tea/"},{"title":"From Formula to Function: Inferring Compound Class from MS1 + MS2","text":"In this post, I document the full formula assignment pipeline using MFAssignR, tailored for Orbitrap-MS data post-peak list processing. The steps include quality control filtering, noise estimation via Kendrick Mass Defect (KMD) plots, isotopic filtering, recalibrant inspection, and final formula assignment. Tool 1. MetFraghttps://msbi.ipb-halle.de/MetFrag/ Tool 2. CFM-IDhttps://msbi.ipb-halle.de/MetFrag/ Tool 3. SIRIUShttps://msbi.ipb-halle.de/MetFrag/ Simple code for results visualizationhttps://msbi.ipb-halle.de/MetFrag/ MS2 Analyzerhttps://chemrxiv.org/engage/chemrxiv/article-details/6492507524989702c2b082fc https://fiehnlab.ucdavis.edu/projects/ms2analyzer Polaris drug discoveryhttps://polarishub.io/competitions/asap-discovery/antiviral-drug-discovery-2025 To do list. Orbitrack tool make 4-5 tools as the blog webpage å¼€å‘ç¬”è®° Drawing structure as a blog page MS2ç»“æ„åˆ†æ","link":"/2025/04/01/41.%5BMS%5DFrom_Formula_to%20Function_Inferring_Compound_Class_from_MS1+MS2/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Air quality","slug":"Air-quality","link":"/tags/Air-quality/"},{"name":"Model","slug":"Model","link":"/tags/Model/"},{"name":"Visualization","slug":"Visualization","link":"/tags/Visualization/"},{"name":"GIS","slug":"GIS","link":"/tags/GIS/"},{"name":"WRF","slug":"WRF","link":"/tags/WRF/"},{"name":"Knowledge Base","slug":"Knowledge-Base","link":"/tags/Knowledge-Base/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"Data processing","slug":"Data-processing","link":"/tags/Data-processing/"},{"name":"Visulaization","slug":"Visulaization","link":"/tags/Visulaization/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"Package","slug":"Package","link":"/tags/Package/"},{"name":"Dataset","slug":"Dataset","link":"/tags/Dataset/"},{"name":"Windows","slug":"Windows","link":"/tags/Windows/"},{"name":"Atmosphere","slug":"Atmosphere","link":"/tags/Atmosphere/"},{"name":"Geospatial","slug":"Geospatial","link":"/tags/Geospatial/"},{"name":"Orbitrap","slug":"Orbitrap","link":"/tags/Orbitrap/"},{"name":"Mass Spectrometry","slug":"Mass-Spectrometry","link":"/tags/Mass-Spectrometry/"},{"name":"äººæ–‡é˜…è¯»","slug":"äººæ–‡é˜…è¯»","link":"/tags/%E4%BA%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"Chemistry","slug":"Chemistry","link":"/tags/Chemistry/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"Plotly","slug":"Plotly","link":"/tags/Plotly/"},{"name":"Food","slug":"Food","link":"/tags/Food/"}],"categories":[{"name":"Research","slug":"Research","link":"/categories/Research/"},{"name":"Tech","slug":"Tech","link":"/categories/Tech/"},{"name":"Life","slug":"Life","link":"/categories/Life/"}]}